{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "from utils.Rnn_model import Rnn_model\n",
    "# import kwargs dicts\n",
    "from utils.kwargs_file import kwargs_neural_data_init, kwargs_rnn\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "# from tensorflow.python.framework import ops  # for custom actiavation function definition\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label suggestion data\n",
      "The are 3565 observations\n",
      "Sampling from allowed 96 labels\n",
      "96 labels in the validation set, with\n",
      "2111 potential observation to draw from.\n",
      "486 observations sampled for validation\n",
      "1625 observations for training\n",
      "The ratio of validation to *training* is about 0.299\n"
     ]
    }
   ],
   "source": [
    "x_feed_train, y_feed_train, x_feed_val, y_feed_val,\\\n",
    "    char_int, char_int_inv, label_int, label_int_inv, \\\n",
    "    statistics_dict =\\\n",
    "    data_load_preprocess(**kwargs_neural_data_init)\n",
    "    \n",
    "\n",
    "#### BEFORE (similarity of at least 0.85): ####\n",
    "# The are 2919 observations\n",
    "# Sampling from allowed 82 labels\n",
    "# 82 labels in the validation set, with\n",
    "# 1587 potential observation to draw from.\n",
    "# 365 observations sampled for validation\n",
    "# 1222 observations for training\n",
    "# The ratio of validation to *training* is about 0.299\n",
    "\n",
    "#### NOW (similarity of at least 0.8): ####\n",
    "# The are 3565 observations\n",
    "# Sampling from allowed 96 labels\n",
    "# 96 labels in the validation set, with\n",
    "# 2111 potential observation to draw from.\n",
    "# 486 observations sampled for validation\n",
    "# 1625 observations for training\n",
    "# The ratio of validation to *training* is about 0.299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory was deleted.\n"
     ]
    }
   ],
   "source": [
    "if kwargs_rnn.save_step == np.inf and \\\n",
    "    kwargs_rnn.to_save: \n",
    "    kwargs_rnn.save_step = kwargs_rnn.epochs\n",
    "    \n",
    "kwargs_rnn = nice_dict({**kwargs_rnn, \n",
    "                        **statistics_dict})\n",
    "\n",
    "kwargs_rnn.epochs = 120\n",
    "\n",
    "if kwargs_rnn.del_log: remove_dir_content(kwargs_rnn.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enable passing some keyword arguments from command line.\n",
    "This does not affect the Jupyter notebook.\n",
    "\"\"\"\n",
    "# try:\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--logdir', action='store', dest='logdir',\n",
    "                    help='Specify the log directory path', \n",
    "                    type=str,\n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--rnn_type', action='store', dest='rnn_type',\n",
    "                    help='Specify RNN type (\"GRU\" or \"LSTM\")', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--bidir', action='store', dest='bidir',\n",
    "                    help='Specify data feed direction '+\\\n",
    "                        '(False for forward-feed or True for bidirectional)', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--use_suggestions', action='store', dest='use_suggestions',\n",
    "                    help='Should the algorithm use label suggestions '+\\\n",
    "                        '(False would mean using only the given labeled data)', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "'use_suggestions'\n",
    "\n",
    "#     results = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "if args.logdir is not None and isinstance(args.logdir, str):\n",
    "    kwargs_rnn.log_dir = str(args.logdir)\n",
    "if args.rnn_type is not None and isinstance(args.rnn_type, str):\n",
    "    kwargs_rnn.rnn_type = str(args.rnn_type)\n",
    "if args.bidir is not None and isinstance(args.bidir, str):\n",
    "    kwargs_rnn.bidirection = args.bidir in ['True', 'T']\n",
    "if args.use_suggestions is not None and isinstance(args.use_suggestions, str):\n",
    "    kwargs_rnn.use_suggestions = args.use_suggestions in ['True', 'T']\n",
    "\n",
    "# except:\n",
    "#     pass\n",
    "# print(kwargs_simple_lstm.log_dir, type(kwargs_simple_lstm.log_dir))\n",
    "# print(kwargs_simple_lstm.rnn_type, type(kwargs_simple_lstm.rnn_type))\n",
    "# print(kwargs_simple_lstm.bidirection, type(kwargs_simple_lstm.bidirection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "look_at_some_examples = False\n",
    "\"\"\"\n",
    "Collect examples from training and validation sets,\n",
    "group by label and print two examples for each \n",
    "(e.g. for each label, print 2 training and 2 validation examples).\n",
    "This was done due to a suspicion raised by similar evaulation metrics on the training and test.\n",
    "\"\"\"\n",
    "if look_at_some_examples:\n",
    "    label_to_text_val = {}  # collect validation examples\n",
    "    for obs,label in zip(x_feed_val, y_feed_val):\n",
    "        label_to_text_val.setdefault(label,[]).append(obs)\n",
    "\n",
    "    label_to_text_train = {}  # collect training examples\n",
    "    for obs,label in zip(x_feed_train, y_feed_train):\n",
    "        label_to_text_train.setdefault(label,[]).append(obs)\n",
    "\n",
    "    unique_keys = list(label_to_text_train.keys())\n",
    "    unique_keys.sort()\n",
    "\n",
    "    label_to_text_merge = {}  # collect both\n",
    "    for key in unique_keys:\n",
    "        label_to_text_merge[key] = {'training': label_to_text_train[key], \n",
    "                                    'validation': label_to_text_val[key]}\n",
    "\n",
    "    for key in unique_keys:\n",
    "        cur_dict = label_to_text_merge[key]\n",
    "        print('Key:{}, training:'.format(key))\n",
    "        print(''.join([char for char in cur_dict['training'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['training'][1] if char != '<pad-char>']))\n",
    "        print('validation:')\n",
    "        print(''.join([char for char in cur_dict['validation'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['validation'][1] if char != '<pad-char>']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "# training data\n",
    "X_train, _, Y_train = index_transorm_xy(x=x_feed_train, \n",
    "                                        y=y_feed_train, \n",
    "                                        char_int=char_int, \n",
    "                                        label_int=label_int, \n",
    "                                        **kwargs_rnn)\n",
    "\n",
    "# validation data\n",
    "X_val, _, Y_val = index_transorm_xy(x=x_feed_val, \n",
    "                                    y=y_feed_val, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    **kwargs_rnn)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_rnn.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep_first_k_chars(input=X_train, k=2, \n",
    "#                        model='neural', \n",
    "#                        char_int=char_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train model GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=1.15,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=32,l2_weight_reg=1.0E-03,target_rep_weight=0.3/\n",
      "0.252 of observations in the top is 5\n",
      "Epoch number 10, training accuracy is 0.10708 and test accuracy is 0.11728, \n",
      "training cost is 4.25295 and test cost is 4.19220 and \n",
      "0.268 of observations in the top is 5\n",
      "Epoch number 20, training accuracy is 0.10092 and test accuracy is 0.10494, \n",
      "training cost is 4.14840 and test cost is 4.10002 and \n",
      "0.306 of observations in the top is 5\n",
      "Epoch number 30, training accuracy is 0.10892 and test accuracy is 0.11523, \n",
      "training cost is 3.92011 and test cost is 3.85613 and \n",
      "0.357 of observations in the top is 5\n",
      "Epoch number 40, training accuracy is 0.12308 and test accuracy is 0.12346, \n",
      "training cost is 3.69239 and test cost is 3.64486 and \n",
      "0.432 of observations in the top is 5\n",
      "Epoch number 50, training accuracy is 0.18031 and test accuracy is 0.17078, \n",
      "training cost is 3.44982 and test cost is 3.40317 and \n",
      "0.465 of observations in the top is 5\n",
      "Epoch number 60, training accuracy is 0.17846 and test accuracy is 0.18930, \n",
      "training cost is 3.32302 and test cost is 3.26910 and \n",
      "0.513 of observations in the top is 5\n",
      "Epoch number 70, training accuracy is 0.21046 and test accuracy is 0.21605, \n",
      "training cost is 3.16002 and test cost is 3.11599 and \n",
      "0.586 of observations in the top is 5\n",
      "Epoch number 80, training accuracy is 0.27077 and test accuracy is 0.27778, \n",
      "training cost is 2.95798 and test cost is 2.92683 and \n",
      "0.663 of observations in the top is 5\n",
      "Epoch number 90, training accuracy is 0.32923 and test accuracy is 0.33333, \n",
      "training cost is 2.71984 and test cost is 2.70127 and \n",
      "0.739 of observations in the top is 5\n",
      "Epoch number 100, training accuracy is 0.39138 and test accuracy is 0.38272, \n",
      "training cost is 2.48239 and test cost is 2.48400 and \n",
      "0.828 of observations in the top is 5\n",
      "Epoch number 110, training accuracy is 0.46277 and test accuracy is 0.45267, \n",
      "training cost is 2.23301 and test cost is 2.29680 and \n",
      "0.866 of observations in the top is 5\n",
      "Epoch number 120, training accuracy is 0.51815 and test accuracy is 0.51235, \n",
      "training cost is 2.01892 and test cost is 2.13146 and \n",
      "Training the model is done! (GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=1.15,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=32,l2_weight_reg=1.0E-03,target_rep_weight=0.3/)\n"
     ]
    }
   ],
   "source": [
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "hparam_str = make_hparam_string(**kwargs_rnn)\n",
    "\n",
    "rnn_model = Rnn_model(hparam_str=hparam_str, \n",
    "                      embed_vis_path=embed_vis_path, \n",
    "                      feed_dict_train=kwargs_feed_dict_train, \n",
    "                      feed_dict_test=kwargs_feed_dict_test, \n",
    "#                       **{**kwargs_rnn, \n",
    "#                       **{'epochs': 100}}\n",
    "                **kwargs_rnn\n",
    "                      )\n",
    "\n",
    "rnn_model.train()\n",
    "rnn_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load weights from file\n",
    "\n",
    "# hparam_str = make_hparam_string(**kwargs_rnn)\n",
    "\n",
    "rnn_new = Rnn_model(\n",
    "#                   hparam_str=hparam_str, \n",
    "#                 embed_vis_path=embed_vis_path, \n",
    "#                 feed_dict_train=kwargs_feed_dict_train, \n",
    "                feed_dict_test=kwargs_feed_dict_test, \n",
    "                **{**kwargs_rnn, \n",
    "                   **{'epochs': 100}}\n",
    "#                 **kwargs_rnn\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_new.restore(cp_path=os.path.join(kwargs_rnn.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_new.update_test_dict(kwargs_feed_dict_train)\n",
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "# lstm.p_delta_scale.eval(session=lstm.sess)\n",
    "# lstm.p_delta_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.sess.run(lstm.outputs, feed_dict=lstm.feed_dict_train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "rnn_models = {}\n",
    "for keep_prob in [0.7]:\n",
    "#         for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "    for hidden_state_size in [64, 128]:\n",
    "        for l2_weight_reg in list(np.logspace(-3, -4, 2)):\n",
    "            for (activation_function,  # all interaction / permutations for noisy activation\n",
    "                 learn_p_delta_scale, \n",
    "                 noise_act_alpha, \n",
    "                 noise_act_half_normal) in [\n",
    "                [tf.tanh, False, 1.15, False]] +\\\n",
    "                [[*j] for j in\\\n",
    "                list(itertools.product(['noisy_tanh'] ,\n",
    "                               [True, False], \n",
    "                               [1.15, 0.9], \n",
    "                               [True, False]))]:\n",
    "                    \n",
    "                # collect new hyperparameters as args\n",
    "                current_kw_rnn = {\n",
    "                    **kwargs_rnn, \n",
    "                    **{'keep_prob': keep_prob, \n",
    "                       'hidden_state_size': hidden_state_size, \n",
    "                       'l2_weight_reg': l2_weight_reg, \n",
    "                       'activation_function': activation_function,\n",
    "                       'learn_p_delta_scale': learn_p_delta_scale,\n",
    "                       'noise_act_alpha': noise_act_alpha,\n",
    "                       'noise_act_half_normal': noise_act_half_normal\n",
    "    #                        'bidirection': bidirection, \n",
    "    #                        'target_rep': target_rep\n",
    "                      }}\n",
    "                hparam_str = make_hparam_string(**current_kw_rnn)\n",
    "                print(hparam_str)\n",
    "                var = 'rnn_{}'.format(hparam_str)\n",
    "                rnn_models[var] = rnn_model(feed_dict_train=kwargs_feed_dict_train, \n",
    "                                              feed_dict_test=kwargs_feed_dict_test, \n",
    "                                              hparam_str=hparam_str, \n",
    "                                              embed_vis_path=embed_vis_path, \n",
    "                                              **current_kw_rnn)\n",
    "                rnn_models[var].train()\n",
    "                rnn_models[var].close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting data directly from a tensorboard log dir\n",
    "from tensorflow.python.summary import event_multiplexer\n",
    "# specify path (for parent log dir)\n",
    "log_parent_dir = './logdir_exper_4_3/'\n",
    "ea = event_multiplexer.EventMultiplexer().AddRunsFromDirectory(log_parent_dir)\n",
    "ea.Reload()  # load\n",
    "\n",
    "child_dir = next(os.walk(log_parent_dir))[1]\n",
    "print(ea.Scalars(child_dir[0], 'accuracy/accuracy_test'))  # specify run, scalar_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/39921607/tensorflow-how-to-make-a-custom-activation-function-with-only-python\n",
    "\n",
    "# def custom_tanh(x):  # spiky\n",
    "#     return np.tanh(x)\n",
    "# #     return x ** 2\n",
    "# np_custom_tanh = np.vectorize(custom_tanh)  # np_spiky = np.vectorize(spiky)\n",
    "\n",
    "\n",
    "# def d_custom_tanh(x):  # d_spiky\n",
    "#     return 1 - np.tanh(x) ** 2\n",
    "# #     return 2 * x\n",
    "# np_d_custom_tanh = np.vectorize(d_custom_tanh)  # np_d_spiky = np.vectorize(d_spiky)\n",
    "\n",
    "\n",
    "\n",
    "# def tf_d_custom_tanh(x,name=None, stateful=False):  # tf_d_spiky\n",
    "#     \"\"\"\n",
    "#     Converting a Numpy function to a Tensorflow function.\n",
    "#     tf.py_func acts on lists of tensors and returns a list of tensors.\n",
    "#     stateful, if the same input might produce a different outputs (stochastic).\n",
    "#     \"\"\"\n",
    "#     with ops.name_scope(name, \n",
    "#                         default_name='d_custom_act', \n",
    "#                         values=[x]) as name:\n",
    "#         result = tf.py_func(lambda x: np_d_custom_tanh(x).astype(np.float32),\n",
    "#                         [x],\n",
    "#                         [tf.float32],\n",
    "#                         name=name,\n",
    "#                         stateful=stateful)\n",
    "#         return result[0]\n",
    "\n",
    "\n",
    "# def py_func(func, input, type_out, stateful=True, name=None, grad=None):\n",
    "#     \"\"\"\n",
    "#     Modify the tf.py_func function to make it define the gradient at the same time\n",
    "#     \"\"\"\n",
    "#     # Need to generate a unique name to avoid duplicates:\n",
    "#     rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "\n",
    "#     tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "#     g = tf.get_default_graph()\n",
    "#     with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "#         return tf.py_func(func, input, type_out, stateful=stateful, name=name)\n",
    "\n",
    "\n",
    "# def custom_tanh_grad(op, grad):  # spikygrad\n",
    "#     \"\"\"\n",
    "#     py_func requires a function of a particular form, \n",
    "#     taking an operation and a 'gradient' and returning the computed gradient.\n",
    "#     \"\"\"\n",
    "#     x = op.inputs[0]\n",
    "#     n_gr = tf_d_custom_tanh(x)\n",
    "# #     print('op is:', op)\n",
    "#     print('grad is:', grad)\n",
    "#     print('x is:', x)\n",
    "#     print('n_gr is:', n_gr)\n",
    "#     return grad * n_gr\n",
    "# #     return [g * tf_d_custom_tanh(inp) for g, inp in zip(grad ,x)]\n",
    "\n",
    "\n",
    "# def tf_custom_tanh(x, name=None):  # tf_spiky\n",
    "\n",
    "#     with ops.name_scope(name, \n",
    "#                         default_name='custom_act', \n",
    "#                         values=[x]) as name:\n",
    "#         result = py_func(lambda x: np_custom_tanh(x).astype(np.float32),\n",
    "#                         [x],\n",
    "#                         [tf.float32],\n",
    "#                         name=name,\n",
    "#                         grad=custom_tanh_grad)  # <-- here's the call to the gradient\n",
    "#         return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # based Noisy Activation Functions paper\n",
    "# # https://arxiv.org/abs/1603.00391\n",
    "# # https://github.com/caglar/noisy_units/blob/master/codes/tf/nunits.py\n",
    "\n",
    "# def lin_sigmoid(x):\n",
    "#     \"\"\"\n",
    "#     First-order Taylor expansion around zero of the sigmoid function\n",
    "#     \"\"\"\n",
    "#     return 0.25 * x + 0.5\n",
    "\n",
    "\n",
    "# def hard_sigmoid(x):\n",
    "#     \"\"\"\n",
    "#     Hard saturating sigmoid function, with clipping applied\n",
    "#     \"\"\"\n",
    "#     return tf.minimum(tf.maximum(lin_sigmoid(x), 0.0), 1.0)\n",
    "\n",
    "\n",
    "# def noise_hard_tanh_sat(x, use_noise, stddev=0.25):\n",
    "#     \"\"\"\n",
    "#     Noisy Hard Tanh Units at Saturation: NANIS as proposed in the paper\n",
    "#     https://arxiv.org/abs/1603.00391\n",
    "#     Arguments:\n",
    "#         x: input tensor variable.\n",
    "#         use_noise: bool, whether to add noise or not (useful for test time)\n",
    "#         c: float, standard deviation of the noise\n",
    "#     \"\"\"\n",
    "#     threshold = 1.001  # point where the unit is saturated, in abs terms\n",
    "#     def noise_func() :return tf.random_normal(tf.shape(x), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "#     def zero_func (): return tf.zeros(tf.shape(x), dtype=tf.float32, name=None)\n",
    "#     noise = tf.cond(use_noise,noise_func,zero_func)  # add noise or zeroes\n",
    "    \n",
    "#     test = tf.cast(tf.greater(tf.abs(x) , threshold), tf.float32)\n",
    "#     res = test * hard_tanh(x + stddev * noise) + (1.0 - test) * hard_tanh(x)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
