{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "from utils.Rnn_model import Rnn_model\n",
    "# import kwargs dicts\n",
    "from utils.kwargs_file import kwargs_neural_data_init, kwargs_rnn\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "# from tensorflow.python.framework import ops  # for custom actiavation function definition\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enable passing some keyword arguments from command line.\n",
    "This does not affect the Jupyter notebook.\n",
    "\"\"\"\n",
    "# try:\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--logdir', action='store', dest='logdir',\n",
    "                    help='Specify the log directory path', \n",
    "                    type=str,\n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--use_suggestions', action='store', dest='use_suggestions',\n",
    "                    help='Should the algorithm use label suggestions '+\\\n",
    "                        '(False would mean using only the given labeled data)', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--rnn_type', action='store', dest='rnn_type',\n",
    "                    help='Specify RNN type (\"GRU\" or \"LSTM\")', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--bidir', action='store', dest='bidir',\n",
    "                    help='Specify data feed direction '+\\\n",
    "                        '(False for forward-feed or True for bidirectional)', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "\n",
    "# read-in regularization constants\n",
    "parser.add_argument('--l2_weight_reg', action='store', dest='l2_weight_reg',\n",
    "                    help='Specify the L2 reuglarization constant', \n",
    "                    type=float, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--target_rep', action='store', dest='target_rep',\n",
    "                    help='Specify if target replication should be used (bool)', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--target_rep_weight', action='store', dest='target_rep_weight',\n",
    "                    help='Specify the weight of the target replication component in the loss function', \n",
    "                    type=float, \n",
    "                    default=None)\n",
    "\n",
    "\n",
    "# read-in activation functions and noisy activations\n",
    "parser.add_argument('--activation_function', action='store', dest='activation_function',\n",
    "                    help='Specify the desired activation function (\"tf.tanh\" or \"noisy_tanh\")', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--learn_p_delta_scale', action='store', dest='learn_p_delta_scale',\n",
    "                    help='Specify whether \"p_delta\" scalar should be learned (bool) '+\\\n",
    "                        '(part of the noisy activation procedure)', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--noise_act_alpha', action='store', dest='noise_act_alpha',\n",
    "                    help='Specify the alpha hyperparameter ' +\\\n",
    "                        '(part of the noisy activation procedure)', \n",
    "                    type=float, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--noise_act_half_normal', action='store', dest='noise_act_half_normal',\n",
    "                    help='Specify if half-normal noise should be used (bool, else normal noise will be used) '+\\\n",
    "                        '(part of the noisy activation procedure)', \n",
    "                    type=str, \n",
    "                    default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "if args.logdir is not None:\n",
    "    kwargs_rnn.log_dir = str(args.logdir)\n",
    "if args.use_suggestions is not None:\n",
    "    kwargs_rnn.use_suggestions = args.use_suggestions in ['True', 'T']\n",
    "if args.rnn_type is not None:\n",
    "    kwargs_rnn.rnn_type = str(args.rnn_type)\n",
    "if args.bidir is not None:\n",
    "    kwargs_rnn.bidirection = args.bidir in ['True', 'T']\n",
    "\n",
    "# set regularization constants\n",
    "if args.l2_weight_reg is not None:\n",
    "    kwargs_rnn.l2_weight_reg = args.l2_weight_reg\n",
    "if args.target_rep is not None:\n",
    "    kwargs_rnn.target_rep = args.target_rep in ['True', 'T']\n",
    "if args.target_rep_weight is not None:\n",
    "    kwargs_rnn.target_rep_weight = args.target_rep_weight\n",
    "\n",
    "# set activation functions and noisy activations\n",
    "if args.activation_function is not None:\n",
    "    kwargs_rnn.activation_function = str(args.activation_function)\n",
    "if args.learn_p_delta_scale is not None:\n",
    "    kwargs_rnn.learn_p_delta_scale = args.learn_p_delta_scale in ['True', 'T']\n",
    "if args.noise_act_alpha is not None:\n",
    "    kwargs_rnn.noise_act_alpha = args.noise_act_alpha\n",
    "if args.noise_act_half_normal is not None:\n",
    "    kwargs_rnn.noise_act_half_normal = args.noise_act_half_normal in ['True', 'T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [1E-1, 1E-2, 1E-3]\n",
    "\n",
    "dynamic_learn_rates = [True, False]\n",
    "\n",
    "# rnn_types = ['GRU', 'LSTM']\n",
    "rnn_types = [kwargs_rnn.rnn_type]\n",
    "\n",
    "one_hots = [True, False]\n",
    "\n",
    "char_embeddings = [4]\n",
    "\n",
    "hidden_state_sizes = [8, 64]\n",
    "\n",
    "# keep_probs = [0.5, 0.7, 1.0]\n",
    "keep_probs = [1.0]\n",
    "\n",
    "rnn_settings = [learning_rates, \n",
    "                dynamic_learn_rates, \n",
    "                rnn_types, \n",
    "                one_hots, \n",
    "                char_embeddings, \n",
    "                hidden_state_sizes, \n",
    "                keep_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_feed_train, y_feed_train, x_feed_val, y_feed_val,\\\n",
    "    char_int, char_int_inv, label_int, label_int_inv, \\\n",
    "    statistics_dict =\\\n",
    "    data_load_preprocess(**kwargs_neural_data_init)\n",
    "    \n",
    "\n",
    "#### BEFORE (similarity of at least 0.85): ####\n",
    "# The are 2919 observations\n",
    "# Sampling from allowed 82 labels\n",
    "# 82 labels in the validation set, with\n",
    "# 1587 potential observation to draw from.\n",
    "# 365 observations sampled for validation\n",
    "# 1222 observations for training\n",
    "# The ratio of validation to *training* is about 0.299\n",
    "\n",
    "#### NOW (similarity of at least 0.8): ####\n",
    "# The are 3565 observations\n",
    "# Sampling from allowed 96 labels\n",
    "# 96 labels in the validation set, with\n",
    "# 2111 potential observation to draw from.\n",
    "# 486 observations sampled for validation\n",
    "# 1625 observations for training\n",
    "# The ratio of validation to *training* is about 0.299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if kwargs_rnn.save_step == np.inf and \\\n",
    "    kwargs_rnn.to_save: \n",
    "    kwargs_rnn.save_step = kwargs_rnn.epochs\n",
    "    \n",
    "kwargs_rnn = nice_dict({**kwargs_rnn, \n",
    "                        **statistics_dict})\n",
    "\n",
    "if kwargs_rnn.del_log: remove_dir_content(kwargs_rnn.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "look_at_some_examples = False\n",
    "\"\"\"\n",
    "Collect examples from training and validation sets,\n",
    "group by label and print two examples for each \n",
    "(e.g. for each label, print 2 training and 2 validation examples).\n",
    "This was done due to a suspicion raised by similar evaulation metrics on the training and test.\n",
    "\"\"\"\n",
    "if look_at_some_examples:\n",
    "    label_to_text_val = {}  # collect validation examples\n",
    "    for obs,label in zip(x_feed_val, y_feed_val):\n",
    "        label_to_text_val.setdefault(label,[]).append(obs)\n",
    "\n",
    "    label_to_text_train = {}  # collect training examples\n",
    "    for obs,label in zip(x_feed_train, y_feed_train):\n",
    "        label_to_text_train.setdefault(label,[]).append(obs)\n",
    "\n",
    "    unique_keys = list(label_to_text_train.keys())\n",
    "    unique_keys.sort()\n",
    "\n",
    "    label_to_text_merge = {}  # collect both\n",
    "    for key in unique_keys:\n",
    "        label_to_text_merge[key] = {'training': label_to_text_train[key], \n",
    "                                    'validation': label_to_text_val[key]}\n",
    "\n",
    "    for key in unique_keys:\n",
    "        cur_dict = label_to_text_merge[key]\n",
    "        print('Key:{}, training:'.format(key))\n",
    "        print(''.join([char for char in cur_dict['training'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['training'][1] if char != '<pad-char>']))\n",
    "        print('validation:')\n",
    "        print(''.join([char for char in cur_dict['validation'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['validation'][1] if char != '<pad-char>']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "# training data\n",
    "X_train, _, Y_train = index_transorm_xy(x=x_feed_train, \n",
    "                                        y=y_feed_train, \n",
    "                                        char_int=char_int, \n",
    "                                        label_int=label_int, \n",
    "                                        **kwargs_rnn)\n",
    "\n",
    "# validation data\n",
    "X_val, _, Y_val = index_transorm_xy(x=x_feed_val, \n",
    "                                    y=y_feed_val, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    **kwargs_rnn)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_rnn.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep_first_k_chars(input=X_train, k=2, \n",
    "#                        model='neural', \n",
    "#                        char_int=char_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "hparam_str = make_hparam_string(**kwargs_rnn)\n",
    "\n",
    "rnn_model = Rnn_model(hparam_str=hparam_str, \n",
    "                      embed_vis_path=embed_vis_path, \n",
    "                      feed_dict_train=kwargs_feed_dict_train, \n",
    "                      feed_dict_test=kwargs_feed_dict_test, \n",
    "#                       **{**kwargs_rnn, \n",
    "#                       **{'epochs': 100}}\n",
    "                **kwargs_rnn\n",
    "                      )\n",
    "\n",
    "rnn_model.train()\n",
    "rnn_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load weights from file\n",
    "\n",
    "# hparam_str = make_hparam_string(**kwargs_rnn)\n",
    "\n",
    "rnn_new = Rnn_model(\n",
    "#                   hparam_str=hparam_str, \n",
    "#                 embed_vis_path=embed_vis_path, \n",
    "#                 feed_dict_train=kwargs_feed_dict_train, \n",
    "                feed_dict_test=kwargs_feed_dict_test, \n",
    "                **{**kwargs_rnn, \n",
    "                   **{'epochs': 100}}\n",
    "#                 **kwargs_rnn\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_new.restore(cp_path=os.path.join(kwargs_rnn.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_new.update_test_dict(kwargs_feed_dict_train)\n",
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "# lstm.p_delta_scale.eval(session=lstm.sess)\n",
    "# lstm.p_delta_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.sess.run(lstm.outputs, feed_dict=lstm.feed_dict_train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "# create a dict to instantiate models\n",
    "rnn_models = {}\n",
    "for learn_rate, \\\n",
    "    dynamic_learn_rate, \\\n",
    "    rnn_type, \\\n",
    "    one_hot, \\\n",
    "    char_embed, \\\n",
    "    hidden_state_size, \\\n",
    "    keep_prob \\\n",
    "    in product(*rnn_settings):\n",
    "        # collect new hyperparameters as args\n",
    "        current_kw_rnn = \\\n",
    "            {**kwargs_rnn,\n",
    "             **dict(learn_rate=learn_rate, \n",
    "                    dynamic_learn_rate=dynamic_learn_rate, \n",
    "                    rnn_type=rnn_type, \n",
    "                    one_hot=one_hot, \n",
    "                    char_embed=char_embed, \n",
    "                    hidden_state_size=hidden_state_size, \n",
    "                    keep_prob=keep_prob)}\n",
    "        # make hyper-parameters string\n",
    "        hparam_str = make_hparam_string(**current_kw_rnn)\n",
    "        print(hparam_str)\n",
    "        \n",
    "        var = 'rnn_{}'.format(hparam_str)\n",
    "        rnn_models[var] = Rnn_model(feed_dict_train=kwargs_feed_dict_train, \n",
    "                                    feed_dict_test=kwargs_feed_dict_test, \n",
    "                                    hparam_str=hparam_str, \n",
    "                                    embed_vis_path=embed_vis_path, \n",
    "                                    **current_kw_rnn)\n",
    "        rnn_models[var].train()\n",
    "        rnn_models[var].close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "rnn_models = {}\n",
    "for keep_prob in [0.7]:\n",
    "#         for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "    for hidden_state_size in [64, 128]:\n",
    "        for l2_weight_reg in list(np.logspace(-3, -4, 2)):\n",
    "            for (activation_function,  # all interaction / permutations for noisy activation\n",
    "                 learn_p_delta_scale, \n",
    "                 noise_act_alpha, \n",
    "                 noise_act_half_normal) in [\n",
    "                [tf.tanh, False, 1.15, False]] +\\\n",
    "                [[*j] for j in\\\n",
    "                list(itertools.product(['noisy_tanh'] ,\n",
    "                               [True, False], \n",
    "                               [1.15, 0.9], \n",
    "                               [True, False]))]:\n",
    "                    \n",
    "                # collect new hyperparameters as args\n",
    "                current_kw_rnn = {\n",
    "                    **kwargs_rnn, \n",
    "                    **{'keep_prob': keep_prob, \n",
    "                       'hidden_state_size': hidden_state_size, \n",
    "                       'l2_weight_reg': l2_weight_reg, \n",
    "                       'activation_function': activation_function,\n",
    "                       'learn_p_delta_scale': learn_p_delta_scale,\n",
    "                       'noise_act_alpha': noise_act_alpha,\n",
    "                       'noise_act_half_normal': noise_act_half_normal\n",
    "    #                        'bidirection': bidirection, \n",
    "    #                        'target_rep': target_rep\n",
    "                      }}\n",
    "                hparam_str = make_hparam_string(**current_kw_rnn)\n",
    "                print(hparam_str)\n",
    "                var = 'rnn_{}'.format(hparam_str)\n",
    "                rnn_models[var] = rnn_model(feed_dict_train=kwargs_feed_dict_train, \n",
    "                                              feed_dict_test=kwargs_feed_dict_test, \n",
    "                                              hparam_str=hparam_str, \n",
    "                                              embed_vis_path=embed_vis_path, \n",
    "                                              **current_kw_rnn)\n",
    "                rnn_models[var].train()\n",
    "                rnn_models[var].close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/39921607/tensorflow-how-to-make-a-custom-activation-function-with-only-python\n",
    "\n",
    "# def custom_tanh(x):  # spiky\n",
    "#     return np.tanh(x)\n",
    "# #     return x ** 2\n",
    "# np_custom_tanh = np.vectorize(custom_tanh)  # np_spiky = np.vectorize(spiky)\n",
    "\n",
    "\n",
    "# def d_custom_tanh(x):  # d_spiky\n",
    "#     return 1 - np.tanh(x) ** 2\n",
    "# #     return 2 * x\n",
    "# np_d_custom_tanh = np.vectorize(d_custom_tanh)  # np_d_spiky = np.vectorize(d_spiky)\n",
    "\n",
    "\n",
    "\n",
    "# def tf_d_custom_tanh(x,name=None, stateful=False):  # tf_d_spiky\n",
    "#     \"\"\"\n",
    "#     Converting a Numpy function to a Tensorflow function.\n",
    "#     tf.py_func acts on lists of tensors and returns a list of tensors.\n",
    "#     stateful, if the same input might produce a different outputs (stochastic).\n",
    "#     \"\"\"\n",
    "#     with ops.name_scope(name, \n",
    "#                         default_name='d_custom_act', \n",
    "#                         values=[x]) as name:\n",
    "#         result = tf.py_func(lambda x: np_d_custom_tanh(x).astype(np.float32),\n",
    "#                         [x],\n",
    "#                         [tf.float32],\n",
    "#                         name=name,\n",
    "#                         stateful=stateful)\n",
    "#         return result[0]\n",
    "\n",
    "\n",
    "# def py_func(func, input, type_out, stateful=True, name=None, grad=None):\n",
    "#     \"\"\"\n",
    "#     Modify the tf.py_func function to make it define the gradient at the same time\n",
    "#     \"\"\"\n",
    "#     # Need to generate a unique name to avoid duplicates:\n",
    "#     rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "\n",
    "#     tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "#     g = tf.get_default_graph()\n",
    "#     with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "#         return tf.py_func(func, input, type_out, stateful=stateful, name=name)\n",
    "\n",
    "\n",
    "# def custom_tanh_grad(op, grad):  # spikygrad\n",
    "#     \"\"\"\n",
    "#     py_func requires a function of a particular form, \n",
    "#     taking an operation and a 'gradient' and returning the computed gradient.\n",
    "#     \"\"\"\n",
    "#     x = op.inputs[0]\n",
    "#     n_gr = tf_d_custom_tanh(x)\n",
    "# #     print('op is:', op)\n",
    "#     print('grad is:', grad)\n",
    "#     print('x is:', x)\n",
    "#     print('n_gr is:', n_gr)\n",
    "#     return grad * n_gr\n",
    "# #     return [g * tf_d_custom_tanh(inp) for g, inp in zip(grad ,x)]\n",
    "\n",
    "\n",
    "# def tf_custom_tanh(x, name=None):  # tf_spiky\n",
    "\n",
    "#     with ops.name_scope(name, \n",
    "#                         default_name='custom_act', \n",
    "#                         values=[x]) as name:\n",
    "#         result = py_func(lambda x: np_custom_tanh(x).astype(np.float32),\n",
    "#                         [x],\n",
    "#                         [tf.float32],\n",
    "#                         name=name,\n",
    "#                         grad=custom_tanh_grad)  # <-- here's the call to the gradient\n",
    "#         return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # based Noisy Activation Functions paper\n",
    "# # https://arxiv.org/abs/1603.00391\n",
    "# # https://github.com/caglar/noisy_units/blob/master/codes/tf/nunits.py\n",
    "\n",
    "# def lin_sigmoid(x):\n",
    "#     \"\"\"\n",
    "#     First-order Taylor expansion around zero of the sigmoid function\n",
    "#     \"\"\"\n",
    "#     return 0.25 * x + 0.5\n",
    "\n",
    "\n",
    "# def hard_sigmoid(x):\n",
    "#     \"\"\"\n",
    "#     Hard saturating sigmoid function, with clipping applied\n",
    "#     \"\"\"\n",
    "#     return tf.minimum(tf.maximum(lin_sigmoid(x), 0.0), 1.0)\n",
    "\n",
    "\n",
    "# def noise_hard_tanh_sat(x, use_noise, stddev=0.25):\n",
    "#     \"\"\"\n",
    "#     Noisy Hard Tanh Units at Saturation: NANIS as proposed in the paper\n",
    "#     https://arxiv.org/abs/1603.00391\n",
    "#     Arguments:\n",
    "#         x: input tensor variable.\n",
    "#         use_noise: bool, whether to add noise or not (useful for test time)\n",
    "#         c: float, standard deviation of the noise\n",
    "#     \"\"\"\n",
    "#     threshold = 1.001  # point where the unit is saturated, in abs terms\n",
    "#     def noise_func() :return tf.random_normal(tf.shape(x), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "#     def zero_func (): return tf.zeros(tf.shape(x), dtype=tf.float32, name=None)\n",
    "#     noise = tf.cond(use_noise,noise_func,zero_func)  # add noise or zeroes\n",
    "    \n",
    "#     test = tf.cast(tf.greater(tf.abs(x) , threshold), tf.float32)\n",
    "#     res = test * hard_tanh(x + stddev * noise) + (1.0 - test) * hard_tanh(x)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
