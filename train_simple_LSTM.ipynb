{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(seed())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "\n",
    "# from collections import Counter\n",
    "# from math import ceil\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize data from main (original) CSV file\n",
    "x, y, n, main_data = init_data()\n",
    "freq = [i for i in main_data['CNT'][:n]]  # frequencies, turned into a list\n",
    "# initialize data from suggestions CSV file\n",
    "x_suggest, y_suggest, freq_suggest = init_data_suggest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory was deleted.\n"
     ]
    }
   ],
   "source": [
    "kwargs_simple_lstm = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'char_filter': 100, \n",
    "    'scale_func': unscale,  # log_scale, \n",
    "    'to_permute': True, \n",
    "    'seed': seed(), \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1E-1,  # 1E-4\n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': False,\n",
    "    'hidden_state_size': 8, \n",
    "    'keep_prob': 0.7, \n",
    "    'epochs': 500,\n",
    "    'summary_step': 5, \n",
    "    'save_step': 10\n",
    "})\n",
    "\n",
    "if kwargs_simple_lstm.del_log: remove_dir_content(kwargs_simple_lstm.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter characters according to 'char_filter',\n",
    "# makes all sequences the same (max) length and pads with 'unknown' character\n",
    "x_char_filtered_pad, statistics_dict = \\\n",
    "    text_filter_pad_to_index(text=x, y=y, **kwargs_simple_lstm)\n",
    "# update main dict with newly calculated figures\n",
    "kwargs_simple_lstm = nice_dict({**kwargs_simple_lstm, **statistics_dict})\n",
    "\n",
    "# create look-up dictionaries (and inverse) for an index representation\n",
    "char_int, char_int_inv, label_int, label_int_inv = \\\n",
    "    lookup_dicts_chars_labels(**kwargs_simple_lstm)\n",
    "\n",
    "# transform x_suggest in a similar manner\n",
    "# taking into consideration the given character set\n",
    "x_suggest_char_filtered_pad, statistics_dict = \\\n",
    "    text_filter_pad_to_index(text=x_suggest, y=y_suggest, **kwargs_simple_lstm)\n",
    "\n",
    "# check that there are no \"new\" statistics popping out\n",
    "assert_no_stats_change(new_dict=statistics_dict, \n",
    "                       kwargs=kwargs_simple_lstm)\n",
    "\n",
    "# merge original and suggested data\n",
    "x_merge, y_merge, freq_merge = \\\n",
    "    x_char_filtered_pad + x_suggest_char_filtered_pad, \\\n",
    "    y + y_suggest, \\\n",
    "    freq + freq_suggest\n",
    "# y_merge = y + y_suggest\n",
    "# freq_merge = freq + freq_suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of potential labels to draw from: 82\n",
      "number of potential observation to draw from: 1587\n",
      "365 observations sampled for validation\n",
      "2919 total number of observations\n",
      "2554 observations for training\n",
      "The ratio of validation to total observations is about 0.125\n"
     ]
    }
   ],
   "source": [
    "# split to training and validation sets\n",
    "x_val, x_train, y_val, y_train, freq_val, freq_train, valid_index = \\\n",
    "    train_validation_split(x=x_merge, y=y_merge, freq=freq_merge, \n",
    "                           label_count_thresh=10, \n",
    "                           valid_ratio=0.25)\n",
    "n_train, n_test = len(y_train), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scale data (proportional to frequency)\n",
    "# training data\n",
    "x_train_scaled, y_train_scaled, kwargs_simple_lstm['n_train'] = \\\n",
    "    scale_permute_data(x=x_train, \n",
    "                       y=y_train, \n",
    "                       freq=freq_train, \n",
    "                       scale_func=kwargs_simple_lstm.scale_func, \n",
    "                       to_permute=kwargs_simple_lstm.to_permute)\n",
    "\n",
    "# validation data\n",
    "x_val_scaled, y_val_scaled, kwargs_simple_lstm['n_test'] = \\\n",
    "    scale_permute_data(x=x_val, \n",
    "                       y=y_val, \n",
    "                       freq=freq_val, \n",
    "                       scale_func=kwargs_simple_lstm.scale_func, \n",
    "                       to_permute=kwargs_simple_lstm.to_permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aliasing, so that will run smoothly from here\n",
    "x_feed_train, y_feed_train, x_feed_val, y_feed_val = \\\n",
    "    x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "# training data\n",
    "X_train, _, Y_train = index_transorm_xy(x=x_feed_train, \n",
    "                                        y=y_feed_train, \n",
    "                                        char_int=char_int, \n",
    "                                        label_int=label_int, \n",
    "                                        **kwargs_simple_lstm)\n",
    "\n",
    "# validation data\n",
    "X_val, _, Y_val = index_transorm_xy(x=x_feed_val, \n",
    "                                    y=y_feed_val, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    **kwargs_simple_lstm)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_simple_lstm.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Lstm_model(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 *args, \n",
    "                 hparam_str, \n",
    "                 n_train, \n",
    "                 n_test, \n",
    "                 seq_len, \n",
    "                 n_class, \n",
    "                 n_char, \n",
    "                 char_embed_dim, \n",
    "                 one_hot, \n",
    "                 hidden_state_size, \n",
    "                 keep_prob, \n",
    "                 learn_rate, \n",
    "                 epochs, \n",
    "                 log_dir, \n",
    "                 embed_vis_path, \n",
    "                 summary_step, \n",
    "                 save_step, \n",
    "                 seed, \n",
    "                 feed_dict_train, \n",
    "                 feed_dict_test, \n",
    "                 **kwargs):\n",
    "#         self.feed_dict_train = {}\n",
    "#         self.feed_dict_val = {}\n",
    "        self.hparam_str = hparam_str\n",
    "        self.n_train = n_train\n",
    "        self.n_test = n_test\n",
    "        self.seq_len = seq_len \n",
    "        self.n_class = n_class \n",
    "        self.n_char = n_char\n",
    "        self.char_embed_dim = char_embed_dim\n",
    "        self.one_hot = one_hot\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.learn_rate = learn_rate\n",
    "        self.epochs = epochs\n",
    "        self.log_dir = log_dir\n",
    "        self.embed_vis_path = embed_vis_path\n",
    "        self.summary_step = summary_step \n",
    "        self.save_step = save_step\n",
    "        self.seed = seed\n",
    "        # placeholders\n",
    "        self.embedding_matrix = None\n",
    "                \n",
    "        # g = tf.Graph()\n",
    "        # with g.as_default():\n",
    "        #     tf.set_random_seed(1)\n",
    "        \n",
    "#         self.g = tf.Graph()\n",
    "#         self.g.seed = self.seed\n",
    "    #         with self.g.as_default():\n",
    "#         tf.set_random_seed(self.seed)\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "\n",
    "        # Setup placeholders, and reshape the data\n",
    "        self.x_ = tf.placeholder(tf.int32, [None, self.seq_len], \n",
    "                            name='Examples')\n",
    "        self.y_ = tf.placeholder(tf.int32, [None, self.n_class], \n",
    "                            name='Lables')\n",
    "        \n",
    "        self.feed_dict_train = {self.x_: feed_dict_train['x'], \n",
    "                                self.y_: feed_dict_train['y']}\n",
    "        \n",
    "        self.feed_dict_test = {self.x_: feed_dict_test['x'], \n",
    "                              self.y_: feed_dict_test['y']}    \n",
    "\n",
    "        self.embedding_matrix = self.embed_matrix()\n",
    "\n",
    "        self.outputs = self.lstm_unit(input=self.x_)\n",
    "\n",
    "        self.logits = self.logit(input=self.outputs, \n",
    "                            size_in=self.hidden_state_size, \n",
    "                            size_out=self.n_class)\n",
    "\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=self.y_))\n",
    "            tf.summary.scalar('cross_entropy_train', self.cost, collections=['train'])\n",
    "            tf.summary.scalar('cross_entropy_test', self.cost, collections=['test'])\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_step = tf.train.AdamOptimizer(\n",
    "                self.learn_rate).minimize(self.cost)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "#             name = 'accuracy'\n",
    "#             name_var = tf.Variable('accuracy', dtype=tf.string)\n",
    "#             name = tf.cond(self.train_data, \n",
    "#                            lambda: self.add_train(name_var), \n",
    "#                            lambda: self.add_valid(name_var))\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('accuracy_train', self.accuracy, collections=['train'])\n",
    "            tf.summary.scalar('accuracy_test', self.accuracy, collections=['test'])\n",
    "#             tf.summary.scalar('accuracy_validation', self.accuracy)\n",
    "\n",
    "        # embedding vis\n",
    "        self.embedding_vis = tf.Variable(tf.zeros(self.embedding_matrix.get_shape().as_list()), \n",
    "                                    trainable=False, \n",
    "                                    name='embedding_vis')\n",
    "#         tf.nn.embedding_lookup(embeddings, input)\n",
    "        self.assignment = self.embedding_vis.assign(self.embedding_matrix)\n",
    "\n",
    "        # summaries per collection and saver object\n",
    "#         self.summ = tf.summary.merge_all()\n",
    "        \n",
    "        self.summ_train = tf.summary.merge_all('train')\n",
    "        self.summ_test = tf.summary.merge_all('test')\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # init vars and setup writer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + self.hparam_str)\n",
    "        self.writer.add_graph(self.sess.graph)\n",
    "        \n",
    "        \n",
    "        # NOT SURE!\n",
    "#         self.writer_val = tf.summary.FileWriter(self.log_dir + self.hparam_str + 'validation/')\n",
    "#         with tf.name_scope('accuracy'):\n",
    "#             tf.summary.scalar('accuracy_validation', self.accuracy, collections=['test'])\n",
    "#         with tf.name_scope('cross_entropy'):\n",
    "#             tf.summary.scalar('cross_entropy_validation', self.cost, collections=['test'])\n",
    "#         self.summ_val = tf.summary.merge([self.accuracy, self.cost])\n",
    "#         self.summ_val = tf.summary.merge(['accuracy_validation', 'cross_entropy_validation'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Add embedding tensorboard visualization. Need tensorflow version\n",
    "        self.config = projector.ProjectorConfig()\n",
    "        self.embed = self.config.embeddings.add()\n",
    "        self.embed.tensor_name = self.embedding_vis.name\n",
    "        self.embed.metadata_path = os.path.join(self.embed_vis_path)\n",
    "        projector.visualize_embeddings(self.writer, self.config)\n",
    "        \n",
    "        # embedding vis\n",
    "        \n",
    "    def embed_matrix(self, stddev=0.1, name='embeddings'):\n",
    "        # index_size would be the size of the character set\n",
    "        with tf.name_scope(name):\n",
    "            if not self.one_hot:\n",
    "                embedding_matrix = tf.get_variable(\n",
    "                    'embedding_matrix', \n",
    "                    initializer=tf.truncated_normal([self.n_char, self.char_embed_dim], \n",
    "                                                    stddev=stddev, \n",
    "                                                    seed=self.seed), \n",
    "                    trainable=True)\n",
    "            else:\n",
    "                # creating a one-hot for each character corresponds to the identity matrix\n",
    "                embedding_matrix = tf.constant(value=np.identity(self.n_char), \n",
    "                                               name='embedding_matrix', \n",
    "                                               dtype=tf.float32)\n",
    "                self.char_embed_dim = self.n_char\n",
    "\n",
    "            tf.summary.histogram('embedding_matrix', embedding_matrix, collections=['train'])\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            return self.embedding_matrix\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, \n",
    "                  input, \n",
    "                  name='LSTM'):\n",
    "        with tf.name_scope(name):\n",
    "#             print(input.shape)  # (?, 80)\n",
    "            input = tf.nn.embedding_lookup(self.embedding_matrix, input)\n",
    "#             print(input.shape)  # (?, 80, 4)\n",
    "            \n",
    "            # reshaping\n",
    "            # Permuting batch_size and n_steps\n",
    "            input = tf.transpose(input, [1, 0, 2])\n",
    "            # Reshaping to (n_steps*batch_size, n_input)\n",
    "            input = tf.reshape(input, [-1, self.char_embed_dim])\n",
    "            # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "            rnn_inputs = tf.split(input, self.seq_len, 0)\n",
    "\n",
    "            cell = rnn.BasicLSTMCell(num_units=self.hidden_state_size)\n",
    "            keep_prob = tf.constant(self.keep_prob)\n",
    "            cell = rnn.DropoutWrapper(cell, \n",
    "                                      output_keep_prob=keep_prob, \n",
    "                                      seed=self.seed)\n",
    "\n",
    "            outputs, states = rnn.static_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "            outputs = outputs[-1]\n",
    "    #         outputs = tf.constant(value=outputs, \n",
    "    #                               name='outputs')\n",
    "            tf.summary.histogram('outputs', outputs, collections=['train'])\n",
    "            return outputs\n",
    "\n",
    "\n",
    "    def logit(self, \n",
    "              input, \n",
    "              size_in, \n",
    "              size_out, \n",
    "              stddev=0.1, \n",
    "              name='logit'):\n",
    "\n",
    "        with tf.name_scope(name):\n",
    "            w = tf.Variable(tf.truncated_normal([size_in, size_out], \n",
    "                                                stddev=stddev, \n",
    "                                                seed=self.seed), \n",
    "                           name='W')\n",
    "            b = tf.Variable(tf.constant(0.1, \n",
    "                                        shape=[size_out]), \n",
    "                            name='B')\n",
    "            logits = tf.matmul(input, w) + b\n",
    "            tf.summary.histogram('weights', w, collections=['train'])\n",
    "            tf.summary.histogram('biases', b, collections=['train'])\n",
    "            tf.summary.histogram('logits', logits, collections=['train'])\n",
    "            return logits\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        print('Starting to train model {:s}'.format(self.hparam_str))\n",
    "        \n",
    "#         self.sess.graph.seed = self.seed\n",
    "        for i in range(self.epochs + 1):\n",
    "            if (i+1) % self.summary_step == 0:\n",
    "                # minimizing cost (while also tracking accuracy, for summary)\n",
    "                # train\n",
    "                [train_accuracy, train_cost, s] = \\\n",
    "                    self.sess.run([self.accuracy, \n",
    "                                   self.cost, \n",
    "                                   self.summ_train],\n",
    "                                  feed_dict=self.feed_dict_train)\n",
    "                self.writer.add_summary(s, i)\n",
    "                # test\n",
    "                [test_accuracy, test_cost, s] = \\\n",
    "                    self.sess.run([self.accuracy, \n",
    "                                   self.cost, \n",
    "                                   self.summ_test],\n",
    "                                  feed_dict=self.feed_dict_test)\n",
    "                self.writer.add_summary(s, i)\n",
    "                \n",
    "                print('Epoch number {}, '.format(i+1) +\n",
    "                      'accuracy is {:.5f} and '.format(train_accuracy) + \n",
    "                      'cost is {:.5f}'.format(train_cost))\n",
    "                print('Epoch number {}, '.format(i+1) +\n",
    "                      'test accuracy is {:.5f} and '.format(test_accuracy) + \n",
    "                      'test cost is {:.5f}'.format(test_cost))\n",
    "                \n",
    "            if (i+1) % self.save_step == 0:\n",
    "                print('Saving step {}'.format(i+1))\n",
    "                self.sess.run(self.assignment, feed_dict=self.feed_dict_train)\n",
    "                self.saver.save(self.sess, os.path.join(self.log_dir, \n",
    "                                                        self.hparam_str, \n",
    "                                                        'model.ckpt'), i)\n",
    "            self.sess.run(self.train_step, feed_dict=self.feed_dict_train)\n",
    "\n",
    "#         self.sess.close()\n",
    "        print('Training the model is done! ({:s})'.format(self.hparam_str))\n",
    "    \n",
    "    \n",
    "    def restore(self, cp_path, feed_dict = None):\n",
    "        \n",
    "        print('Loading variables from {:s}'.format(cp_path))\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(cp_path)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\"no checkpoint found\")\n",
    "\n",
    "#         if feed_dict:\n",
    "#             self.feed(feed_dict=feed_dict)\n",
    "        print('Loading successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "hparam_str = make_hparam_string(**kwargs_simple_lstm)\n",
    "\n",
    "lstm = Lstm_model(hparam_str=hparam_str, \n",
    "                  embed_vis_path=embed_vis_path, \n",
    "                  feed_dict_train=kwargs_feed_dict_train, \n",
    "                  feed_dict_test=kwargs_feed_dict_test, \n",
    "                  **{**kwargs_simple_lstm, \n",
    "                     **{'epochs': 500}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train model learn_rate=1.0E-01,one_hot=F,keep_prob=0.7,char_embed_dim=4,hidden_state_size=8/\n",
      "Epoch number 5, accuracy is 0.02232 and cost is 5.89566\n",
      "Epoch number 5, test accuracy is 0.05479 and test cost is 5.00173\n",
      "Epoch number 10, accuracy is 0.04190 and cost is 5.83024\n",
      "Epoch number 10, test accuracy is 0.08767 and test cost is 5.01003\n",
      "Saving step 10\n",
      "Epoch number 15, accuracy is 0.03641 and cost is 5.81552\n",
      "Epoch number 15, test accuracy is 0.10137 and test cost is 5.01079\n",
      "Epoch number 20, accuracy is 0.04464 and cost is 5.78443\n",
      "Epoch number 20, test accuracy is 0.08767 and test cost is 4.83411\n",
      "Saving step 20\n",
      "Epoch number 25, accuracy is 0.04581 and cost is 5.71820\n",
      "Epoch number 25, test accuracy is 0.10137 and test cost is 4.83510\n",
      "Epoch number 30, accuracy is 0.04464 and cost is 5.64253\n",
      "Epoch number 30, test accuracy is 0.10411 and test cost is 4.77894\n",
      "Saving step 30\n",
      "Epoch number 35, accuracy is 0.04777 and cost is 5.55580\n",
      "Epoch number 35, test accuracy is 0.09589 and test cost is 4.73669\n",
      "Epoch number 40, accuracy is 0.05129 and cost is 5.49752\n",
      "Epoch number 40, test accuracy is 0.10685 and test cost is 4.74246\n",
      "Saving step 40\n",
      "Epoch number 45, accuracy is 0.04307 and cost is 5.45864\n",
      "Epoch number 45, test accuracy is 0.09863 and test cost is 4.69496\n",
      "Epoch number 50, accuracy is 0.04816 and cost is 5.40142\n",
      "Epoch number 50, test accuracy is 0.08767 and test cost is 4.63593\n",
      "Saving step 50\n",
      "Epoch number 55, accuracy is 0.04777 and cost is 5.38335\n",
      "Epoch number 55, test accuracy is 0.11507 and test cost is 4.59584\n",
      "Epoch number 60, accuracy is 0.05051 and cost is 5.32738\n",
      "Epoch number 60, test accuracy is 0.07945 and test cost is 4.57287\n",
      "Saving step 60\n",
      "Epoch number 65, accuracy is 0.05795 and cost is 5.29308\n",
      "Epoch number 65, test accuracy is 0.11507 and test cost is 4.52335\n",
      "Epoch number 70, accuracy is 0.05638 and cost is 5.23264\n",
      "Epoch number 70, test accuracy is 0.10685 and test cost is 4.52853\n",
      "Saving step 70\n",
      "Epoch number 75, accuracy is 0.06226 and cost is 5.18011\n",
      "Epoch number 75, test accuracy is 0.12603 and test cost is 4.38321\n",
      "Epoch number 80, accuracy is 0.05873 and cost is 5.14493\n",
      "Epoch number 80, test accuracy is 0.13151 and test cost is 4.38698\n",
      "Saving step 80\n",
      "Epoch number 85, accuracy is 0.06069 and cost is 5.09961\n",
      "Epoch number 85, test accuracy is 0.13425 and test cost is 4.32602\n",
      "Epoch number 90, accuracy is 0.06695 and cost is 5.10860\n",
      "Epoch number 90, test accuracy is 0.15342 and test cost is 4.31007\n",
      "Saving step 90\n",
      "Epoch number 95, accuracy is 0.07165 and cost is 5.04017\n",
      "Epoch number 95, test accuracy is 0.15068 and test cost is 4.24524\n",
      "Epoch number 100, accuracy is 0.07753 and cost is 4.98086\n",
      "Epoch number 100, test accuracy is 0.16712 and test cost is 4.16354\n",
      "Saving step 100\n",
      "Epoch number 105, accuracy is 0.08653 and cost is 4.92203\n",
      "Epoch number 105, test accuracy is 0.16986 and test cost is 4.17418\n",
      "Epoch number 110, accuracy is 0.08379 and cost is 4.90225\n",
      "Epoch number 110, test accuracy is 0.15890 and test cost is 4.14125\n",
      "Saving step 110\n",
      "Epoch number 115, accuracy is 0.07987 and cost is 4.84175\n",
      "Epoch number 115, test accuracy is 0.18082 and test cost is 4.03960\n",
      "Epoch number 120, accuracy is 0.08144 and cost is 4.80433\n",
      "Epoch number 120, test accuracy is 0.15616 and test cost is 4.07288\n",
      "Saving step 120\n",
      "Epoch number 125, accuracy is 0.08027 and cost is 4.81047\n",
      "Epoch number 125, test accuracy is 0.16164 and test cost is 4.12059\n",
      "Epoch number 130, accuracy is 0.08340 and cost is 4.76114\n",
      "Epoch number 130, test accuracy is 0.18082 and test cost is 4.08036\n",
      "Saving step 130\n",
      "Epoch number 135, accuracy is 0.07165 and cost is 4.72930\n",
      "Epoch number 135, test accuracy is 0.14247 and test cost is 4.06020\n",
      "Epoch number 140, accuracy is 0.08027 and cost is 4.67555\n",
      "Epoch number 140, test accuracy is 0.12877 and test cost is 4.06575\n",
      "Saving step 140\n",
      "Epoch number 145, accuracy is 0.09123 and cost is 4.63794\n",
      "Epoch number 145, test accuracy is 0.18082 and test cost is 4.04176\n",
      "Epoch number 150, accuracy is 0.08888 and cost is 4.61827\n",
      "Epoch number 150, test accuracy is 0.20000 and test cost is 3.89479\n",
      "Saving step 150\n",
      "Epoch number 155, accuracy is 0.07870 and cost is 4.65893\n",
      "Epoch number 155, test accuracy is 0.13151 and test cost is 4.12606\n",
      "Epoch number 160, accuracy is 0.07870 and cost is 4.55975\n",
      "Epoch number 160, test accuracy is 0.16438 and test cost is 3.95935\n",
      "Saving step 160\n",
      "Epoch number 165, accuracy is 0.08379 and cost is 4.52543\n",
      "Epoch number 165, test accuracy is 0.16164 and test cost is 3.98735\n",
      "Epoch number 170, accuracy is 0.08536 and cost is 4.51887\n",
      "Epoch number 170, test accuracy is 0.15890 and test cost is 3.95792\n",
      "Saving step 170\n",
      "Epoch number 175, accuracy is 0.08771 and cost is 4.45079\n",
      "Epoch number 175, test accuracy is 0.18904 and test cost is 3.83832\n",
      "Epoch number 180, accuracy is 0.09240 and cost is 4.44770\n",
      "Epoch number 180, test accuracy is 0.17808 and test cost is 3.81837\n",
      "Saving step 180\n",
      "Epoch number 185, accuracy is 0.08262 and cost is 4.45733\n",
      "Epoch number 185, test accuracy is 0.16438 and test cost is 3.79661\n",
      "Epoch number 190, accuracy is 0.08771 and cost is 4.41677\n",
      "Epoch number 190, test accuracy is 0.17808 and test cost is 3.89415\n",
      "Saving step 190\n",
      "Epoch number 195, accuracy is 0.09749 and cost is 4.39261\n",
      "Epoch number 195, test accuracy is 0.19726 and test cost is 3.72223\n",
      "Epoch number 200, accuracy is 0.10023 and cost is 4.32061\n",
      "Epoch number 200, test accuracy is 0.18904 and test cost is 3.84678\n",
      "Saving step 200\n",
      "Epoch number 205, accuracy is 0.10493 and cost is 4.35888\n",
      "Epoch number 205, test accuracy is 0.17808 and test cost is 3.91733\n",
      "Epoch number 210, accuracy is 0.10689 and cost is 4.29458\n",
      "Epoch number 210, test accuracy is 0.19726 and test cost is 3.78731\n",
      "Saving step 210\n",
      "Epoch number 215, accuracy is 0.10807 and cost is 4.25639\n",
      "Epoch number 215, test accuracy is 0.21096 and test cost is 3.74383\n",
      "Epoch number 220, accuracy is 0.09710 and cost is 4.43786\n",
      "Epoch number 220, test accuracy is 0.14521 and test cost is 4.05162\n",
      "Saving step 220\n",
      "Epoch number 225, accuracy is 0.10767 and cost is 4.30639\n",
      "Epoch number 225, test accuracy is 0.23014 and test cost is 3.72865\n",
      "Epoch number 230, accuracy is 0.10807 and cost is 4.20896\n",
      "Epoch number 230, test accuracy is 0.20548 and test cost is 3.73149\n",
      "Saving step 230\n",
      "Epoch number 235, accuracy is 0.11746 and cost is 4.17524\n",
      "Epoch number 235, test accuracy is 0.17808 and test cost is 3.66358\n",
      "Epoch number 240, accuracy is 0.12490 and cost is 4.12470\n",
      "Epoch number 240, test accuracy is 0.20822 and test cost is 3.75660\n",
      "Saving step 240\n",
      "Epoch number 245, accuracy is 0.11668 and cost is 4.13346\n",
      "Epoch number 245, test accuracy is 0.25479 and test cost is 3.57251\n",
      "Epoch number 250, accuracy is 0.12647 and cost is 4.05681\n",
      "Epoch number 250, test accuracy is 0.19452 and test cost is 3.57471\n",
      "Saving step 250\n",
      "Epoch number 255, accuracy is 0.10298 and cost is 4.30447\n",
      "Epoch number 255, test accuracy is 0.17808 and test cost is 3.73026\n",
      "Epoch number 260, accuracy is 0.11746 and cost is 4.16541\n",
      "Epoch number 260, test accuracy is 0.20548 and test cost is 3.88414\n",
      "Saving step 260\n",
      "Epoch number 265, accuracy is 0.12060 and cost is 4.14420\n",
      "Epoch number 265, test accuracy is 0.23014 and test cost is 3.66639\n",
      "Epoch number 270, accuracy is 0.13312 and cost is 4.08383\n",
      "Epoch number 270, test accuracy is 0.23288 and test cost is 3.60459\n",
      "Saving step 270\n",
      "Epoch number 275, accuracy is 0.13078 and cost is 4.02800\n",
      "Epoch number 275, test accuracy is 0.24384 and test cost is 3.49646\n",
      "Epoch number 280, accuracy is 0.14017 and cost is 3.98445\n",
      "Epoch number 280, test accuracy is 0.23288 and test cost is 3.58531\n",
      "Saving step 280\n",
      "Epoch number 285, accuracy is 0.13978 and cost is 3.94533\n",
      "Epoch number 285, test accuracy is 0.23562 and test cost is 3.62951\n",
      "Epoch number 290, accuracy is 0.13117 and cost is 3.98227\n",
      "Epoch number 290, test accuracy is 0.23014 and test cost is 3.60842\n",
      "Saving step 290\n",
      "Epoch number 295, accuracy is 0.13156 and cost is 3.97993\n",
      "Epoch number 295, test accuracy is 0.20000 and test cost is 3.57730\n",
      "Epoch number 300, accuracy is 0.14448 and cost is 3.89841\n",
      "Epoch number 300, test accuracy is 0.24384 and test cost is 3.55193\n",
      "Saving step 300\n",
      "Epoch number 305, accuracy is 0.14252 and cost is 3.92821\n",
      "Epoch number 305, test accuracy is 0.26849 and test cost is 3.40819\n",
      "Epoch number 310, accuracy is 0.14135 and cost is 3.91509\n",
      "Epoch number 310, test accuracy is 0.21644 and test cost is 3.63061\n",
      "Saving step 310\n",
      "Epoch number 315, accuracy is 0.14056 and cost is 3.90990\n",
      "Epoch number 315, test accuracy is 0.26027 and test cost is 3.40934\n",
      "Epoch number 320, accuracy is 0.14565 and cost is 3.89394\n",
      "Epoch number 320, test accuracy is 0.26575 and test cost is 3.53775\n",
      "Saving step 320\n",
      "Epoch number 325, accuracy is 0.15192 and cost is 3.81991\n",
      "Epoch number 325, test accuracy is 0.23288 and test cost is 3.47795\n",
      "Epoch number 330, accuracy is 0.14683 and cost is 3.82079\n",
      "Epoch number 330, test accuracy is 0.25205 and test cost is 3.58643\n",
      "Saving step 330\n",
      "Epoch number 335, accuracy is 0.15427 and cost is 3.78094\n",
      "Epoch number 335, test accuracy is 0.23014 and test cost is 3.49082\n",
      "Epoch number 340, accuracy is 0.15583 and cost is 3.78602\n",
      "Epoch number 340, test accuracy is 0.27397 and test cost is 3.50172\n",
      "Saving step 340\n",
      "Epoch number 345, accuracy is 0.16132 and cost is 3.79401\n",
      "Epoch number 345, test accuracy is 0.27671 and test cost is 3.37844\n",
      "Epoch number 350, accuracy is 0.15231 and cost is 3.83014\n",
      "Epoch number 350, test accuracy is 0.22192 and test cost is 3.55265\n",
      "Saving step 350\n",
      "Epoch number 355, accuracy is 0.14096 and cost is 3.82745\n",
      "Epoch number 355, test accuracy is 0.21370 and test cost is 3.55661\n",
      "Epoch number 360, accuracy is 0.13391 and cost is 4.00222\n",
      "Epoch number 360, test accuracy is 0.22192 and test cost is 3.60914\n",
      "Saving step 360\n",
      "Epoch number 365, accuracy is 0.13626 and cost is 4.06726\n",
      "Epoch number 365, test accuracy is 0.22740 and test cost is 3.52371\n",
      "Epoch number 370, accuracy is 0.12099 and cost is 4.27493\n",
      "Epoch number 370, test accuracy is 0.21096 and test cost is 3.67937\n",
      "Saving step 370\n",
      "Epoch number 375, accuracy is 0.11276 and cost is 4.28919\n",
      "Epoch number 375, test accuracy is 0.22466 and test cost is 3.73060\n",
      "Epoch number 380, accuracy is 0.13078 and cost is 4.08669\n",
      "Epoch number 380, test accuracy is 0.21096 and test cost is 3.57248\n",
      "Saving step 380\n",
      "Epoch number 385, accuracy is 0.13665 and cost is 4.04641\n",
      "Epoch number 385, test accuracy is 0.20274 and test cost is 3.54890\n",
      "Epoch number 390, accuracy is 0.14722 and cost is 3.90507\n",
      "Epoch number 390, test accuracy is 0.24658 and test cost is 3.42000\n",
      "Saving step 390\n",
      "Epoch number 395, accuracy is 0.14644 and cost is 3.87881\n",
      "Epoch number 395, test accuracy is 0.20548 and test cost is 3.46253\n",
      "Epoch number 400, accuracy is 0.13978 and cost is 3.82441\n",
      "Epoch number 400, test accuracy is 0.23288 and test cost is 3.48402\n",
      "Saving step 400\n",
      "Epoch number 405, accuracy is 0.15192 and cost is 3.80585\n",
      "Epoch number 405, test accuracy is 0.28493 and test cost is 3.38815\n",
      "Epoch number 410, accuracy is 0.15936 and cost is 3.75129\n",
      "Epoch number 410, test accuracy is 0.22466 and test cost is 3.52012\n",
      "Saving step 410\n",
      "Epoch number 415, accuracy is 0.15701 and cost is 3.73359\n",
      "Epoch number 415, test accuracy is 0.29315 and test cost is 3.35819\n",
      "Epoch number 420, accuracy is 0.15388 and cost is 3.78252\n",
      "Epoch number 420, test accuracy is 0.23014 and test cost is 3.60488\n",
      "Saving step 420\n",
      "Epoch number 425, accuracy is 0.16092 and cost is 3.74155\n",
      "Epoch number 425, test accuracy is 0.26027 and test cost is 3.34280\n",
      "Epoch number 430, accuracy is 0.15505 and cost is 3.79279\n",
      "Epoch number 430, test accuracy is 0.26849 and test cost is 3.41796\n",
      "Saving step 430\n",
      "Epoch number 435, accuracy is 0.14644 and cost is 3.83283\n",
      "Epoch number 435, test accuracy is 0.28493 and test cost is 3.31404\n",
      "Epoch number 440, accuracy is 0.16327 and cost is 3.72208\n",
      "Epoch number 440, test accuracy is 0.24932 and test cost is 3.45297\n",
      "Saving step 440\n",
      "Epoch number 445, accuracy is 0.16406 and cost is 3.70970\n",
      "Epoch number 445, test accuracy is 0.28493 and test cost is 3.40264\n",
      "Epoch number 450, accuracy is 0.16641 and cost is 3.65443\n",
      "Epoch number 450, test accuracy is 0.26027 and test cost is 3.45822\n",
      "Saving step 450\n",
      "Epoch number 455, accuracy is 0.16797 and cost is 3.63045\n",
      "Epoch number 455, test accuracy is 0.23288 and test cost is 3.47621\n",
      "Epoch number 460, accuracy is 0.15035 and cost is 3.77598\n",
      "Epoch number 460, test accuracy is 0.26849 and test cost is 3.56539\n",
      "Saving step 460\n",
      "Epoch number 465, accuracy is 0.16680 and cost is 3.66481\n",
      "Epoch number 465, test accuracy is 0.28767 and test cost is 3.49907\n",
      "Epoch number 470, accuracy is 0.15388 and cost is 3.74563\n",
      "Epoch number 470, test accuracy is 0.24932 and test cost is 3.50338\n",
      "Saving step 470\n",
      "Epoch number 475, accuracy is 0.12216 and cost is 4.39380\n",
      "Epoch number 475, test accuracy is 0.21096 and test cost is 3.85438\n",
      "Epoch number 480, accuracy is 0.11159 and cost is 4.47833\n",
      "Epoch number 480, test accuracy is 0.21918 and test cost is 3.89315\n",
      "Saving step 480\n",
      "Epoch number 485, accuracy is 0.11276 and cost is 4.25177\n",
      "Epoch number 485, test accuracy is 0.21644 and test cost is 3.69558\n",
      "Epoch number 490, accuracy is 0.13156 and cost is 4.09518\n",
      "Epoch number 490, test accuracy is 0.29863 and test cost is 3.53626\n",
      "Saving step 490\n",
      "Epoch number 495, accuracy is 0.13978 and cost is 3.93367\n",
      "Epoch number 495, test accuracy is 0.23288 and test cost is 3.54996\n",
      "Epoch number 500, accuracy is 0.14957 and cost is 3.88399\n",
      "Epoch number 500, test accuracy is 0.27671 and test cost is 3.60796\n",
      "Saving step 500\n",
      "Training the model is done! (learn_rate=1.0E-01,one_hot=F,keep_prob=0.7,char_embed_dim=4,hidden_state_size=8/)\n"
     ]
    }
   ],
   "source": [
    "# lstm.feed_train(feed_dict_train=kwargs_feed_dict_train)\n",
    "# lstm.feed_val(feed_dict_val=kwargs_feed_dict_val)\n",
    "lstm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this works after training!\n",
    "# need to check if it's working after calling self.restore()\n",
    "# seems to work after calling self.restore(), yet numbers are not identical\n",
    "# need to solve the seed() problem\n",
    "# lstm.feed(feed_dict=lstm.feed_dict)\n",
    "with lstm.sess.as_default() as sess:\n",
    "# with lstm.sess as sess:\n",
    "#     with lstm.Graph().as_default():\n",
    "#         print(lstm.logits.eval(feed_dict=lstm.feed_dict))\n",
    "    print(lstm.embedding_matrix.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict = {'x': X, 'y': Y_dense}\n",
    "lstm_models = {}\n",
    "for learn_rate in list(np.logspace(-1, -2, 2)):\n",
    "    for keep_prob in [0.7, 1.0]:\n",
    "        for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "#             for char_embed_dim in list(np.linspace(2, 6, 3)):\n",
    "            for hidden_state_size in [4, 32]:\n",
    "                    current_kw_simple_lstm = {\n",
    "                        **kwargs_simple_lstm, \n",
    "                        **{'learn_rate': learn_rate, \n",
    "                           'one_hot': one_hot, \n",
    "                           'keep_prob': keep_prob, \n",
    "                           'char_embed_dim': char_embed_dim, \n",
    "                           'hidden_state_size': hidden_state_size}}\n",
    "                    hparam_str = make_hparam_string(learn_rate, \n",
    "                                                    one_hot, \n",
    "                                                    keep_prob, \n",
    "                                                    char_embed_dim, \n",
    "                                                    hidden_state_size)\n",
    "                    var = 'lstm_{}'.format(hparam_str)\n",
    "#                         print(var)\n",
    "                    lstm_models[var] = Lstm_model(feed_dict=kwargs_feed_dict, \n",
    "                                                  hparam_str=hparam_str, \n",
    "                                                  embed_vis_path=embed_vis_path, \n",
    "                                                  **current_kw_simple_lstm)\n",
    "                    lstm_models[var].train()\n",
    "\n",
    "\n",
    "    \n",
    "#     lstm_models[var] = Lstm_model(feed_dict=kwargs_feed_dict, \n",
    "#                                   hparam_str=hparam_str, \n",
    "#                                   embed_vis_path=embed_vis_path, \n",
    "#                                   **current_kw_simple_lstm)\n",
    "#     lstm_models[var].train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=kwargs_tf_simple.log_dir + 'data', one_hot=True)\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'labels_1024.tsv', kwargs_tf_simple.log_dir + 'labels_1024.tsv')\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'sprite_1024.png', kwargs_tf_simple.log_dir + 'sprite_1024.png')\n",
    "\n",
    "pcp1()\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "def mnist_model(learning_rate, use_two_conv, use_two_fc, hparam):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    tf.set_random_seed(seed())\n",
    "\n",
    "    # Setup placeholders, and reshape the data\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 3)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "    if use_two_conv:\n",
    "        conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "        conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    else:\n",
    "        conv1 = conv_layer(x_image, 1, 64, \"conv\")\n",
    "        conv_out = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "\n",
    "\n",
    "    if use_two_fc:\n",
    "        fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "        embedding_input = fc1\n",
    "        embedding_size = 1024\n",
    "        logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "    else:\n",
    "        embedding_input = flattened\n",
    "        embedding_size = 7*7*64\n",
    "        logits = fc_layer(flattened, 7*7*64, 10, \"fc\")\n",
    "\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y), name=\"xent\")\n",
    "        tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(kwargs_tf_simple.log_dir + hparam)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = kwargs_tf_simple.log_dir + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = kwargs_tf_simple.log_dir + 'labels_1024.tsv'\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    pcp3()\n",
    "    \n",
    "    for i in range(1000 + 1):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            \n",
    "#             pcp4()\n",
    "            \n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "            print('Iteration number {}, Accuracy is currently {}'.format(i, train_accuracy))\n",
    "        if i % 500 == 0:\n",
    "            sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "            saver.save(sess, os.path.join(kwargs_tf_simple.log_dir, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv):\n",
    "    conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "    fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "    return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "\n",
    "def main():\n",
    "    \n",
    "#     tf.set_random_seed(seed())\n",
    "    \n",
    "    # You can try adding some more learning rates\n",
    "    for learning_rate in [1E-4]:\n",
    "\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "        for use_two_fc in [False]:\n",
    "            for use_two_conv in [True]:\n",
    "                # Construct a hyperparameter string for each one (example: \"lr_1E-3,fc=2,conv=2)\n",
    "                \n",
    "                pcp2()\n",
    "                \n",
    "                hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv)\n",
    "                print('Starting run for %s' % hparam)\n",
    "\n",
    "                # Actually run with the new settings\n",
    "                mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
