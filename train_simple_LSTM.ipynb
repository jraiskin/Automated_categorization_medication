{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn import svm\n",
    "# from sklearn.metrics import accuracy_score  # gt, pred\n",
    "\n",
    "from utils.utils import user_opt_gen, nice_dict, seed, init_data, pcp1, pcp2, pcp3, pcp4\n",
    "from utils.utils_baseline_svm import filter_dict_by_val_atleast, char_freq_map\n",
    "\n",
    "# from collections import Counter\n",
    "# from math import isnan\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_dir_content(path):\n",
    "    if tf.gfile.Exists(path):\n",
    "        tf.gfile.DeleteRecursively(path)\n",
    "        print('Log directory was deleted.')\n",
    "    else:\n",
    "        print('Log directory was not found.')\n",
    "#         print(path)\n",
    "\n",
    "\n",
    "# pad a list to max_length with the pad_symbol\n",
    "def pad_list(*, input_list, max_length, pad_symbol):\n",
    "    output_list = input_list + [pad_symbol] * (max_length - len(input_list))\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "def index_to_dense(index, length):\n",
    "    output_list = [0.0] * length\n",
    "    output_list[index] = 1.0\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, n, _ = init_data()\n",
    "\n",
    "np.random.seed(seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory was not found.\n"
     ]
    }
   ],
   "source": [
    "kwargs_simple_lstm = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'char_filter': 100, \n",
    "    'n': n,\n",
    "    'batch_size': n, \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1,  # 1E-4\n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': False,\n",
    "    'hidden_state_size': 4, \n",
    "    'activate_bool': True, \n",
    "    'keep_prob': 1.0, \n",
    "    'epochs': 500,\n",
    "    'summary_step': 5, \n",
    "    'save_step': 100\n",
    "})\n",
    "\n",
    "if kwargs_simple_lstm.del_log: remove_dir_content(kwargs_simple_lstm.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter by character, appear at least 'char_filter' times in the input\n",
    "filter_keys_chars = list(\n",
    "    filter_dict_by_val_atleast(\n",
    "        input_dict=char_freq_map(input_data=x), \n",
    "        value=kwargs_simple_lstm.char_filter)\n",
    "    .keys())\n",
    "\n",
    "# create a list of character lists\n",
    "x_char = [list(line) for line in x]\n",
    "x_char_filtered = []\n",
    "unknown = '<unk-char>'\n",
    "# replace chars not in 'filter_keys_chars' with 'unknown'\n",
    "for line in x_char:\n",
    "    x_char_filtered.append([char if (char in filter_keys_chars) else unknown for char in line])\n",
    "    \n",
    "# pad lines, so that all lines are same length\n",
    "max_line_len = int(np.max([len(line) for line in x]))\n",
    "pad = '<pad-char>'\n",
    "x_char_filtered_pad = []\n",
    "for i, line in enumerate(x_char_filtered):\n",
    "    x_char_filtered_pad.append(pad_list(input_list=line, \n",
    "                                    max_length=max_line_len, \n",
    "                                    pad_symbol=pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# statistics based on filtered features\n",
    "label_set = y.unique()\n",
    "n_label = len(label_set)\n",
    "\n",
    "# number of unique characters iin input ('x_char_filtered')\n",
    "char_set = set([char for line in x_char_filtered_pad for char in line])\n",
    "n_char = len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs_simple_lstm = nice_dict({**kwargs_simple_lstm, **{\n",
    "    'seq_len': max_line_len,\n",
    "    'n_class': n_label,\n",
    "    'n_char': n_char\n",
    "}\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create lookup dict for characters (and inv)\n",
    "char_int = {}\n",
    "char_int_inv = {}\n",
    "for i, char in enumerate(char_set):\n",
    "    char_int[char] = i\n",
    "    char_int_inv[i] = char\n",
    "\n",
    "# transform x from a list of symbols into a list of ints\n",
    "X = []\n",
    "for line in x_char_filtered_pad:\n",
    "    X.append([char_int[char] for char in line])\n",
    "\n",
    "# same for labels\n",
    "label_int = {}\n",
    "label_int_inv = {}\n",
    "for i, label in enumerate(label_set):\n",
    "    label_int[label] = i\n",
    "    label_int_inv[i] = label\n",
    "# create Y as a list of list(int)\n",
    "Y = [[label_int[label]] for label in y]\n",
    "\n",
    "# transform into format acceptable by tf\n",
    "X = np.array(X)\n",
    "Y_dense = np.array(\n",
    "    [index_to_dense(label[0], \n",
    "                    kwargs_simple_lstm.n_class) for label in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed_matrix(index_size, \n",
    "                 embedding_dim, \n",
    "                 one_hot, \n",
    "                 stddev=0.1, \n",
    "                 seed=seed(), \n",
    "                 name=\"embedding_matrix\"):\n",
    "    # index_size would be the size of the character set\n",
    "        \n",
    "    with tf.name_scope(name):\n",
    "        if not one_hot:\n",
    "            embedding_matrix = tf.get_variable(\n",
    "                'embedding_matrix', \n",
    "                initializer=tf.truncated_normal([index_size, embedding_dim], \n",
    "                                                stddev=stddev, \n",
    "                                                seed=seed), \n",
    "                trainable=True)\n",
    "        else:\n",
    "            # creating a one-hot for each character corresponds to the identity matrix\n",
    "            embedding_matrix = tf.constant(value=np.identity(index_size), \n",
    "                                           name='embedding_matrix', \n",
    "                                           dtype=tf.float32)\n",
    "            \n",
    "        tf.summary.histogram('embedding_matrix', embedding_matrix)\n",
    "        return embedding_matrix\n",
    "\n",
    "\n",
    "def lstm_unit(input, \n",
    "              embeddings, \n",
    "              seq_length, \n",
    "              hidden_state_size, \n",
    "              keep_prob, \n",
    "              seed=seed(), \n",
    "              name='LSTM'):\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        rnn_inputs = [tf.squeeze(i) for i in \n",
    "                      tf.split(tf.nn.embedding_lookup(embeddings, input),\n",
    "                               seq_length, \n",
    "                               1)]\n",
    "\n",
    "        cell = rnn.BasicLSTMCell(num_units=hidden_state_size)\n",
    "        keep_prob = tf.constant(keep_prob)\n",
    "        cell = rnn.DropoutWrapper(cell, \n",
    "                                  output_keep_prob=keep_prob, \n",
    "                                  seed=seed)\n",
    "\n",
    "        outputs, states = rnn.static_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "        outputs = outputs[-1]\n",
    "#         outputs = tf.constant(value=outputs, \n",
    "#                               name='outputs')\n",
    "        tf.summary.histogram('outputs', outputs)\n",
    "        return outputs\n",
    "        \n",
    "\n",
    "def logit(*, \n",
    "          input, \n",
    "          size_in, \n",
    "          size_out, \n",
    "          stddev=0.1, \n",
    "          seed=seed(), \n",
    "          name='logit'):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], \n",
    "                                            stddev=stddev, \n",
    "                                            seed=seed), \n",
    "                       name='W')\n",
    "        b = tf.Variable(tf.constant(0.1, \n",
    "                                    shape=[size_out]), \n",
    "                        name='B')\n",
    "        logits = tf.matmul(input, w) + b\n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        return logits\n",
    "                        \n",
    "\n",
    "def lstm_simple_model(feed_dict, \n",
    "                      hparam_str, \n",
    "                      n, \n",
    "                      seq_len, \n",
    "                      n_class, \n",
    "                      n_char, \n",
    "                      char_embed_dim, \n",
    "                      one_hot, \n",
    "                      hidden_state_size, \n",
    "                      keep_prob, \n",
    "                      learn_rate, \n",
    "                      epochs, \n",
    "                      log_dir, \n",
    "                      summary_step, \n",
    "                      save_step, *args, **kwargs):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "#     tf.set_random_seed(seed())\n",
    "\n",
    "    # Setup placeholders, and reshape the data\n",
    "    x_ = tf.placeholder(tf.int32, [n, \n",
    "                                   seq_len])\n",
    "    y_ = tf.placeholder(tf.int32, [n, \n",
    "                                   n_class])\n",
    "\n",
    "    embedding_matrix = embed_matrix(index_size=n_char, \n",
    "                                    embedding_dim=char_embed_dim, \n",
    "                                    one_hot=one_hot)\n",
    "    outputs = lstm_unit(input=x_, \n",
    "                        embeddings=embedding_matrix, \n",
    "                        hidden_state_size=hidden_state_size, \n",
    "                        keep_prob=keep_prob, \n",
    "                        seq_length=seq_len)\n",
    "    \n",
    "    logits = logit(input=outputs, \n",
    "               size_in=hidden_state_size, \n",
    "               size_out=n_class)\n",
    "    \n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y_), name='cross_entropy')\n",
    "        tf.summary.scalar('cross_entropy', cost)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(\n",
    "            learn_rate).minimize(cost)\n",
    "          \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(log_dir + hparam_str)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    feed_dict = {x_: kwargs_feed_dict['x'], \n",
    "                 y_: kwargs_feed_dict['y']}\n",
    "    \n",
    "    for i in range(epochs + 1):\n",
    "        if i % summary_step == 0:\n",
    "            # minimizing cost (while also tracking accuracy, for summary)\n",
    "            [train_accuracy, train_cost, s] = sess.run([accuracy, cost, summ], feed_dict=feed_dict)\n",
    "            writer.add_summary(s, i)\n",
    "            print('Iteration number {}, '.format(i) +\n",
    "                  'accuracy is {:.5f} and '.format(train_accuracy) + \n",
    "                  'cost is {:.5f}'.format(train_cost))\n",
    "        if i % save_step == 0:\n",
    "#             sess.run(assignment, feed_dict=feed_dict)\n",
    "            saver.save(sess, os.path.join(log_dir, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        \n",
    "    print('Training is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0, accuracy is 0.00197 and cost is 6.32400\n",
      "Iteration number 5, accuracy is 0.03895 and cost is 5.92907\n",
      "Iteration number 10, accuracy is 0.03895 and cost is 6.18828\n",
      "Iteration number 15, accuracy is 0.02416 and cost is 5.81941\n",
      "Iteration number 20, accuracy is 0.03895 and cost is 5.82102\n",
      "Iteration number 25, accuracy is 0.02416 and cost is 5.79322\n",
      "Iteration number 30, accuracy is 0.04043 and cost is 5.76409\n",
      "Iteration number 35, accuracy is 0.03994 and cost is 5.74919\n",
      "Iteration number 40, accuracy is 0.04093 and cost is 5.74058\n",
      "Iteration number 45, accuracy is 0.04043 and cost is 5.73562\n",
      "Iteration number 50, accuracy is 0.04093 and cost is 5.73277\n",
      "Iteration number 55, accuracy is 0.04093 and cost is 5.73014\n",
      "Iteration number 60, accuracy is 0.04093 and cost is 5.72813\n",
      "Iteration number 65, accuracy is 0.04093 and cost is 5.72675\n",
      "Iteration number 70, accuracy is 0.04093 and cost is 5.72578\n",
      "Iteration number 75, accuracy is 0.04093 and cost is 5.72509\n",
      "Iteration number 80, accuracy is 0.04093 and cost is 5.72451\n",
      "Iteration number 85, accuracy is 0.04093 and cost is 5.72418\n",
      "Iteration number 90, accuracy is 0.04093 and cost is 5.72392\n",
      "Iteration number 95, accuracy is 0.04043 and cost is 5.72366\n",
      "Iteration number 100, accuracy is 0.04093 and cost is 5.72343\n",
      "Iteration number 105, accuracy is 0.04093 and cost is 5.72324\n",
      "Iteration number 110, accuracy is 0.04043 and cost is 5.72307\n",
      "Iteration number 115, accuracy is 0.04093 and cost is 5.72292\n",
      "Iteration number 120, accuracy is 0.04093 and cost is 5.72277\n",
      "Iteration number 125, accuracy is 0.04093 and cost is 5.72263\n",
      "Iteration number 130, accuracy is 0.04093 and cost is 5.72250\n",
      "Iteration number 135, accuracy is 0.04093 and cost is 5.72237\n",
      "Iteration number 140, accuracy is 0.04142 and cost is 5.72225\n",
      "Iteration number 145, accuracy is 0.04142 and cost is 5.72213\n",
      "Iteration number 150, accuracy is 0.04142 and cost is 5.72202\n",
      "Iteration number 155, accuracy is 0.04142 and cost is 5.72190\n",
      "Iteration number 160, accuracy is 0.04142 and cost is 5.72179\n",
      "Iteration number 165, accuracy is 0.04142 and cost is 5.72168\n",
      "Iteration number 170, accuracy is 0.04142 and cost is 5.72157\n",
      "Iteration number 175, accuracy is 0.04142 and cost is 5.72147\n",
      "Iteration number 180, accuracy is 0.04142 and cost is 5.72137\n",
      "Iteration number 185, accuracy is 0.04142 and cost is 5.72127\n",
      "Iteration number 190, accuracy is 0.04142 and cost is 5.72118\n",
      "Iteration number 195, accuracy is 0.04142 and cost is 5.72109\n",
      "Iteration number 200, accuracy is 0.04142 and cost is 5.72099\n",
      "Iteration number 205, accuracy is 0.04142 and cost is 5.72090\n",
      "Iteration number 210, accuracy is 0.04142 and cost is 5.72082\n",
      "Iteration number 215, accuracy is 0.04142 and cost is 5.72074\n",
      "Iteration number 220, accuracy is 0.04142 and cost is 5.72067\n",
      "Iteration number 225, accuracy is 0.04142 and cost is 5.72060\n",
      "Iteration number 230, accuracy is 0.04142 and cost is 5.72054\n",
      "Iteration number 235, accuracy is 0.04142 and cost is 5.72048\n",
      "Iteration number 240, accuracy is 0.04142 and cost is 5.72042\n",
      "Iteration number 245, accuracy is 0.04142 and cost is 5.72037\n",
      "Iteration number 250, accuracy is 0.04142 and cost is 5.72032\n",
      "Iteration number 255, accuracy is 0.04142 and cost is 5.72027\n",
      "Iteration number 260, accuracy is 0.04142 and cost is 5.72023\n",
      "Iteration number 265, accuracy is 0.04142 and cost is 5.72020\n",
      "Iteration number 270, accuracy is 0.04142 and cost is 5.72017\n",
      "Iteration number 275, accuracy is 0.04142 and cost is 5.72014\n",
      "Iteration number 280, accuracy is 0.04142 and cost is 5.72012\n",
      "Iteration number 285, accuracy is 0.04142 and cost is 5.72010\n",
      "Iteration number 290, accuracy is 0.04142 and cost is 5.72008\n",
      "Iteration number 295, accuracy is 0.04142 and cost is 5.72005\n",
      "Iteration number 300, accuracy is 0.04142 and cost is 5.72004\n",
      "Iteration number 305, accuracy is 0.04142 and cost is 5.72002\n",
      "Iteration number 310, accuracy is 0.04142 and cost is 5.72000\n",
      "Iteration number 315, accuracy is 0.04142 and cost is 5.71999\n",
      "Iteration number 320, accuracy is 0.04142 and cost is 5.71997\n",
      "Iteration number 325, accuracy is 0.04142 and cost is 5.71996\n",
      "Iteration number 330, accuracy is 0.04142 and cost is 5.71995\n",
      "Iteration number 335, accuracy is 0.04142 and cost is 5.71994\n",
      "Iteration number 340, accuracy is 0.04142 and cost is 5.71993\n",
      "Iteration number 345, accuracy is 0.04142 and cost is 5.71992\n",
      "Iteration number 350, accuracy is 0.04142 and cost is 5.71991\n",
      "Iteration number 355, accuracy is 0.04142 and cost is 5.71990\n",
      "Iteration number 360, accuracy is 0.04142 and cost is 5.71988\n",
      "Iteration number 365, accuracy is 0.04142 and cost is 5.71988\n",
      "Iteration number 370, accuracy is 0.04142 and cost is 5.71987\n",
      "Iteration number 375, accuracy is 0.04142 and cost is 5.71987\n",
      "Iteration number 380, accuracy is 0.04142 and cost is 5.71987\n",
      "Iteration number 385, accuracy is 0.04142 and cost is 5.71986\n",
      "Iteration number 390, accuracy is 0.04142 and cost is 5.71985\n",
      "Iteration number 395, accuracy is 0.04142 and cost is 5.71985\n",
      "Iteration number 400, accuracy is 0.04142 and cost is 5.71984\n",
      "Iteration number 405, accuracy is 0.04142 and cost is 5.71984\n",
      "Iteration number 410, accuracy is 0.04142 and cost is 5.71983\n",
      "Iteration number 415, accuracy is 0.04142 and cost is 5.71983\n",
      "Iteration number 420, accuracy is 0.04142 and cost is 5.71983\n",
      "Iteration number 425, accuracy is 0.04142 and cost is 5.71982\n",
      "Iteration number 430, accuracy is 0.04142 and cost is 5.71982\n",
      "Iteration number 435, accuracy is 0.04142 and cost is 5.71982\n",
      "Iteration number 440, accuracy is 0.04142 and cost is 5.71981\n",
      "Iteration number 445, accuracy is 0.04142 and cost is 5.71981\n",
      "Iteration number 450, accuracy is 0.04142 and cost is 5.71981\n",
      "Iteration number 455, accuracy is 0.04142 and cost is 5.71981\n",
      "Iteration number 460, accuracy is 0.04142 and cost is 5.71986\n",
      "Iteration number 465, accuracy is 0.04142 and cost is 5.71992\n",
      "Iteration number 470, accuracy is 0.04142 and cost is 5.71987\n",
      "Iteration number 475, accuracy is 0.04142 and cost is 5.71983\n",
      "Iteration number 480, accuracy is 0.04142 and cost is 5.71980\n",
      "Iteration number 485, accuracy is 0.04142 and cost is 5.71979\n",
      "Iteration number 490, accuracy is 0.04142 and cost is 5.71979\n",
      "Iteration number 495, accuracy is 0.04142 and cost is 5.71979\n",
      "Iteration number 500, accuracy is 0.04142 and cost is 5.71979\n",
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "kwargs_feed_dict = {'x': X, 'y': Y_dense}\n",
    "lstm_simple_model(feed_dict=kwargs_feed_dict, \n",
    "                  hparam_str='testrun', \n",
    "                  **kwargs_simple_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kwargs_tf_simple.log_dir\n",
    "# os.path.join(os.path.curdir + '/logdir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=kwargs_tf_simple.log_dir + 'data', one_hot=True)\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'labels_1024.tsv', kwargs_tf_simple.log_dir + 'labels_1024.tsv')\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'sprite_1024.png', kwargs_tf_simple.log_dir + 'sprite_1024.png')\n",
    "\n",
    "pcp1()\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "def mnist_model(learning_rate, use_two_conv, use_two_fc, hparam):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    tf.set_random_seed(seed())\n",
    "\n",
    "    # Setup placeholders, and reshape the data\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 3)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "    if use_two_conv:\n",
    "        conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "        conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    else:\n",
    "        conv1 = conv_layer(x_image, 1, 64, \"conv\")\n",
    "        conv_out = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "\n",
    "\n",
    "    if use_two_fc:\n",
    "        fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "        embedding_input = fc1\n",
    "        embedding_size = 1024\n",
    "        logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "    else:\n",
    "        embedding_input = flattened\n",
    "        embedding_size = 7*7*64\n",
    "        logits = fc_layer(flattened, 7*7*64, 10, \"fc\")\n",
    "\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y), name=\"xent\")\n",
    "        tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(kwargs_tf_simple.log_dir + hparam)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = kwargs_tf_simple.log_dir + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = kwargs_tf_simple.log_dir + 'labels_1024.tsv'\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    pcp3()\n",
    "    \n",
    "    for i in range(1000 + 1):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            \n",
    "#             pcp4()\n",
    "            \n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "            print('Iteration number {}, Accuracy is currently {}'.format(i, train_accuracy))\n",
    "        if i % 500 == 0:\n",
    "            sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "            saver.save(sess, os.path.join(kwargs_tf_simple.log_dir, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv):\n",
    "    conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "    fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "    return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "\n",
    "def main():\n",
    "    \n",
    "#     tf.set_random_seed(seed())\n",
    "    \n",
    "    # You can try adding some more learning rates\n",
    "    for learning_rate in [1E-4]:\n",
    "\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "        for use_two_fc in [False]:\n",
    "            for use_two_conv in [True]:\n",
    "                # Construct a hyperparameter string for each one (example: \"lr_1E-3,fc=2,conv=2)\n",
    "                \n",
    "                pcp2()\n",
    "                \n",
    "                hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv)\n",
    "                print('Starting run for %s' % hparam)\n",
    "\n",
    "                # Actually run with the new settings\n",
    "                mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
