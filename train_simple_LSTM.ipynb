{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "from utils.Rnn_model import Rnn_model\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "# from tensorflow.python.framework import ops  # for custom actiavation function definition\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs_neural_data_init = \\\n",
    "    {'mk_chars': True, \n",
    "               'model': 'neural', \n",
    "               'char_filter': 100, 'allowed_chars': None, \n",
    "               'mk_ngrams': False, 'ngram_width': 5, \n",
    "               'ngram_filter': 10, 'allowed_ngrams': None, \n",
    "               'keep_infreq_labels': False, 'label_count_thresh': 10, \n",
    "               'valid_ratio': 0.25, \n",
    "               'scale_func': unscale, 'to_permute': True, }\n",
    "\n",
    "x_feed_train, y_feed_train, x_feed_val, y_feed_val,\\\n",
    "    char_int, char_int_inv, label_int, label_int_inv, \\\n",
    "    statistics_dict =\\\n",
    "    data_load_preprocess(**kwargs_neural_data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs_simple_rnn = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'top_k': 5, \n",
    "    'seed': seed(), \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1E-2, \n",
    "    'dynamic_learn_rate': False, \n",
    "    'rnn_type': 'GRU',\n",
    "    'bidirection': False, \n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': True,\n",
    "    'hidden_state_size': 32, \n",
    "    'keep_prob': 0.7, \n",
    "    # noisy activation hyper params\n",
    "    'activation_function': 'noisy_tanh',  # tf.tanh / 'noisy_tanh'\n",
    "    'learn_p_delta_scale': False,   # noise scale param in noisy activation\n",
    "    'noise_act_alpha': 1.15,  # mixing in the linear activation\n",
    "    'noise_act_half_normal': False,\n",
    "    # regularization constants\n",
    "    'l2_weight_reg': 1E-3, \n",
    "    'target_rep': True, \n",
    "    'target_rep_weight': 0.3, \n",
    "    # training settings\n",
    "    'epochs': 20,\n",
    "    'summary_step': 10, \n",
    "    'save_step': np.inf,\n",
    "    'to_save': False, \n",
    "    'verbose_summary': False\n",
    "})\n",
    "\n",
    "if kwargs_simple_rnn.save_step == np.inf and \\\n",
    "    kwargs_simple_rnn.to_save: \n",
    "    kwargs_simple_rnn.save_step = kwargs_simple_rnn.epochs\n",
    "    \n",
    "kwargs_simple_rnn = {**kwargs_simple_rnn, \n",
    "                      **statistics_dict}\n",
    "\n",
    "kwargs_simple_rnn = nice_dict({**kwargs_simple_rnn, \n",
    "                               **{'scale_func': kwargs_neural_data_init['scale_func'], \n",
    "                                  'keep_infreq_labels': kwargs_neural_data_init['keep_infreq_labels']}})\n",
    "\n",
    "if kwargs_simple_rnn.del_log: remove_dir_content(kwargs_simple_rnn.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enable passing some keyword arguments from command line.\n",
    "This does not affect the Jupyter notebook.\n",
    "\"\"\"\n",
    "# try:\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--logdir', action='store', dest='logdir',\n",
    "                    help='Specify the log directory path', \n",
    "                    type=str,\n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--rnn_type', action='store', dest='rnn_type',\n",
    "                    help='Specify RNN type (\"GRU\" or \"LSTM\")', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--bidir', action='store', dest='bidir',\n",
    "                    help='Specify data feed direction '+\\\n",
    "                        '(False for forward-feed or True for bidirectional)', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "#     results = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "if args.logdir is not None and isinstance(args.logdir, str):\n",
    "    kwargs_simple_rnn.log_dir = str(args.logdir)\n",
    "if args.rnn_type is not None and isinstance(args.rnn_type, str):\n",
    "    kwargs_simple_rnn.rnn_type = str(args.rnn_type)\n",
    "if args.bidir is not None and isinstance(args.bidir, str):\n",
    "    kwargs_simple_rnn.bidirection = args.bidir in ['True', 'T']\n",
    "\n",
    "# except:\n",
    "#     pass\n",
    "# print(kwargs_simple_lstm.log_dir, type(kwargs_simple_lstm.log_dir))\n",
    "# print(kwargs_simple_lstm.rnn_type, type(kwargs_simple_lstm.rnn_type))\n",
    "# print(kwargs_simple_lstm.bidirection, type(kwargs_simple_lstm.bidirection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "look_at_some_examples = False\n",
    "\"\"\"\n",
    "Collect examples from training and validation sets,\n",
    "group by label and print two examples for each \n",
    "(e.g. for each label, print 2 training and 2 validation examples).\n",
    "This was done due to a suspicion raised by similar evaulation metrics on the training and test.\n",
    "\"\"\"\n",
    "if look_at_some_examples:\n",
    "    label_to_text_val = {}  # collect validation examples\n",
    "    for obs,label in zip(x_feed_val, y_feed_val):\n",
    "        label_to_text_val.setdefault(label,[]).append(obs)\n",
    "\n",
    "    label_to_text_train = {}  # collect training examples\n",
    "    for obs,label in zip(x_feed_train, y_feed_train):\n",
    "        label_to_text_train.setdefault(label,[]).append(obs)\n",
    "\n",
    "    unique_keys = list(label_to_text_train.keys())\n",
    "    unique_keys.sort()\n",
    "\n",
    "    label_to_text_merge = {}  # collect both\n",
    "    for key in unique_keys:\n",
    "        label_to_text_merge[key] = {'training': label_to_text_train[key], \n",
    "                                    'validation': label_to_text_val[key]}\n",
    "\n",
    "    for key in unique_keys:\n",
    "        cur_dict = label_to_text_merge[key]\n",
    "        print('Key:{}, training:'.format(key))\n",
    "        print(''.join([char for char in cur_dict['training'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['training'][1] if char != '<pad-char>']))\n",
    "        print('validation:')\n",
    "        print(''.join([char for char in cur_dict['validation'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['validation'][1] if char != '<pad-char>']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "# training data\n",
    "X_train, _, Y_train = index_transorm_xy(x=x_feed_train, \n",
    "                                        y=y_feed_train, \n",
    "                                        char_int=char_int, \n",
    "                                        label_int=label_int, \n",
    "                                        **kwargs_simple_rnn)\n",
    "\n",
    "# validation data\n",
    "X_val, _, Y_val = index_transorm_xy(x=x_feed_val, \n",
    "                                    y=y_feed_val, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    **kwargs_simple_rnn)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_simple_rnn.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep_first_k_chars(input=X_train, k=2, \n",
    "#                        model='neural', \n",
    "#                        char_int=char_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "hparam_str = make_hparam_string(**kwargs_simple_rnn)\n",
    "\n",
    "rnn_model = Rnn_model(hparam_str=hparam_str, \n",
    "                      embed_vis_path=embed_vis_path, \n",
    "                      feed_dict_train=kwargs_feed_dict_train, \n",
    "                      feed_dict_test=kwargs_feed_dict_test, \n",
    "#                       **{**kwargs_simple_rnn, \n",
    "#                       **{'epochs': 100}}\n",
    "                **kwargs_simple_rnn\n",
    "                      )\n",
    "\n",
    "rnn_model.train()\n",
    "rnn_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load weights from file\n",
    "\n",
    "# hparam_str = make_hparam_string(**kwargs_simple_rnn)\n",
    "\n",
    "rnn_new = Rnn_model(\n",
    "#                   hparam_str=hparam_str, \n",
    "#                 embed_vis_path=embed_vis_path, \n",
    "#                 feed_dict_train=kwargs_feed_dict_train, \n",
    "                feed_dict_test=kwargs_feed_dict_test, \n",
    "                **{**kwargs_simple_rnn, \n",
    "                   **{'epochs': 100}}\n",
    "#                 **kwargs_simple_rnn\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_new.restore(cp_path=os.path.join(kwargs_simple_rnn.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_new.update_test_dict(kwargs_feed_dict_train)\n",
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "# lstm.p_delta_scale.eval(session=lstm.sess)\n",
    "# lstm.p_delta_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.sess.run(lstm.outputs, feed_dict=lstm.feed_dict_train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "rnn_models = {}\n",
    "for keep_prob in [0.7]:\n",
    "#         for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "    for hidden_state_size in [64, 128]:\n",
    "        for l2_weight_reg in list(np.logspace(-3, -4, 2)):\n",
    "            for (activation_function,  # all interaction / permutations for noisy activation\n",
    "                 learn_p_delta_scale, \n",
    "                 noise_act_alpha, \n",
    "                 noise_act_half_normal) in [\n",
    "                [tf.tanh, False, 1.15, False]] +\\\n",
    "                [[*j] for j in\\\n",
    "                list(itertools.product(['noisy_tanh'] ,\n",
    "                               [True, False], \n",
    "                               [1.15, 0.9], \n",
    "                               [True, False]))]:\n",
    "                    \n",
    "                # collect new hyperparameters as args\n",
    "                current_kw_simple_rnn = {\n",
    "                    **kwargs_simple_rnn, \n",
    "                    **{'keep_prob': keep_prob, \n",
    "                       'hidden_state_size': hidden_state_size, \n",
    "                       'l2_weight_reg': l2_weight_reg, \n",
    "                       'activation_function': activation_function,\n",
    "                       'learn_p_delta_scale': learn_p_delta_scale,\n",
    "                       'noise_act_alpha': noise_act_alpha,\n",
    "                       'noise_act_half_normal': noise_act_half_normal\n",
    "    #                        'bidirection': bidirection, \n",
    "    #                        'target_rep': target_rep\n",
    "                      }}\n",
    "                hparam_str = make_hparam_string(**current_kw_simple_rnn)\n",
    "                print(hparam_str)\n",
    "                var = 'rnn_{}'.format(hparam_str)\n",
    "                rnn_models[var] = rnn_model(feed_dict_train=kwargs_feed_dict_train, \n",
    "                                              feed_dict_test=kwargs_feed_dict_test, \n",
    "                                              hparam_str=hparam_str, \n",
    "                                              embed_vis_path=embed_vis_path, \n",
    "                                              **current_kw_simple_rnn)\n",
    "                rnn_models[var].train()\n",
    "                rnn_models[var].close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting data directly from a tensorboard log dir\n",
    "from tensorflow.python.summary import event_multiplexer\n",
    "# specify path (for parent log dir)\n",
    "log_parent_dir = './logdir_exper_4_3/'\n",
    "ea = event_multiplexer.EventMultiplexer().AddRunsFromDirectory(log_parent_dir)\n",
    "ea.Reload()  # load\n",
    "\n",
    "child_dir = next(os.walk(log_parent_dir))[1]\n",
    "print(ea.Scalars(child_dir[0], 'accuracy/accuracy_test'))  # specify run, scalar_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/39921607/tensorflow-how-to-make-a-custom-activation-function-with-only-python\n",
    "\n",
    "# def custom_tanh(x):  # spiky\n",
    "#     return np.tanh(x)\n",
    "# #     return x ** 2\n",
    "# np_custom_tanh = np.vectorize(custom_tanh)  # np_spiky = np.vectorize(spiky)\n",
    "\n",
    "\n",
    "# def d_custom_tanh(x):  # d_spiky\n",
    "#     return 1 - np.tanh(x) ** 2\n",
    "# #     return 2 * x\n",
    "# np_d_custom_tanh = np.vectorize(d_custom_tanh)  # np_d_spiky = np.vectorize(d_spiky)\n",
    "\n",
    "\n",
    "\n",
    "# def tf_d_custom_tanh(x,name=None, stateful=False):  # tf_d_spiky\n",
    "#     \"\"\"\n",
    "#     Converting a Numpy function to a Tensorflow function.\n",
    "#     tf.py_func acts on lists of tensors and returns a list of tensors.\n",
    "#     stateful, if the same input might produce a different outputs (stochastic).\n",
    "#     \"\"\"\n",
    "#     with ops.name_scope(name, \n",
    "#                         default_name='d_custom_act', \n",
    "#                         values=[x]) as name:\n",
    "#         result = tf.py_func(lambda x: np_d_custom_tanh(x).astype(np.float32),\n",
    "#                         [x],\n",
    "#                         [tf.float32],\n",
    "#                         name=name,\n",
    "#                         stateful=stateful)\n",
    "#         return result[0]\n",
    "\n",
    "\n",
    "# def py_func(func, input, type_out, stateful=True, name=None, grad=None):\n",
    "#     \"\"\"\n",
    "#     Modify the tf.py_func function to make it define the gradient at the same time\n",
    "#     \"\"\"\n",
    "#     # Need to generate a unique name to avoid duplicates:\n",
    "#     rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "\n",
    "#     tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "#     g = tf.get_default_graph()\n",
    "#     with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "#         return tf.py_func(func, input, type_out, stateful=stateful, name=name)\n",
    "\n",
    "\n",
    "# def custom_tanh_grad(op, grad):  # spikygrad\n",
    "#     \"\"\"\n",
    "#     py_func requires a function of a particular form, \n",
    "#     taking an operation and a 'gradient' and returning the computed gradient.\n",
    "#     \"\"\"\n",
    "#     x = op.inputs[0]\n",
    "#     n_gr = tf_d_custom_tanh(x)\n",
    "# #     print('op is:', op)\n",
    "#     print('grad is:', grad)\n",
    "#     print('x is:', x)\n",
    "#     print('n_gr is:', n_gr)\n",
    "#     return grad * n_gr\n",
    "# #     return [g * tf_d_custom_tanh(inp) for g, inp in zip(grad ,x)]\n",
    "\n",
    "\n",
    "# def tf_custom_tanh(x, name=None):  # tf_spiky\n",
    "\n",
    "#     with ops.name_scope(name, \n",
    "#                         default_name='custom_act', \n",
    "#                         values=[x]) as name:\n",
    "#         result = py_func(lambda x: np_custom_tanh(x).astype(np.float32),\n",
    "#                         [x],\n",
    "#                         [tf.float32],\n",
    "#                         name=name,\n",
    "#                         grad=custom_tanh_grad)  # <-- here's the call to the gradient\n",
    "#         return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # based Noisy Activation Functions paper\n",
    "# # https://arxiv.org/abs/1603.00391\n",
    "# # https://github.com/caglar/noisy_units/blob/master/codes/tf/nunits.py\n",
    "\n",
    "# def lin_sigmoid(x):\n",
    "#     \"\"\"\n",
    "#     First-order Taylor expansion around zero of the sigmoid function\n",
    "#     \"\"\"\n",
    "#     return 0.25 * x + 0.5\n",
    "\n",
    "\n",
    "# def hard_sigmoid(x):\n",
    "#     \"\"\"\n",
    "#     Hard saturating sigmoid function, with clipping applied\n",
    "#     \"\"\"\n",
    "#     return tf.minimum(tf.maximum(lin_sigmoid(x), 0.0), 1.0)\n",
    "\n",
    "\n",
    "# def noise_hard_tanh_sat(x, use_noise, stddev=0.25):\n",
    "#     \"\"\"\n",
    "#     Noisy Hard Tanh Units at Saturation: NANIS as proposed in the paper\n",
    "#     https://arxiv.org/abs/1603.00391\n",
    "#     Arguments:\n",
    "#         x: input tensor variable.\n",
    "#         use_noise: bool, whether to add noise or not (useful for test time)\n",
    "#         c: float, standard deviation of the noise\n",
    "#     \"\"\"\n",
    "#     threshold = 1.001  # point where the unit is saturated, in abs terms\n",
    "#     def noise_func() :return tf.random_normal(tf.shape(x), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "#     def zero_func (): return tf.zeros(tf.shape(x), dtype=tf.float32, name=None)\n",
    "#     noise = tf.cond(use_noise,noise_func,zero_func)  # add noise or zeroes\n",
    "    \n",
    "#     test = tf.cast(tf.greater(tf.abs(x) , threshold), tf.float32)\n",
    "#     res = test * hard_tanh(x + stddev * noise) + (1.0 - test) * hard_tanh(x)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
