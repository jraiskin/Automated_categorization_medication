{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.utils import user_opt_gen, nice_dict, seed, init_data, pcp1, pcp2, pcp3, pcp4\n",
    "from utils.utils_nn import *\n",
    "\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(seed())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn import svm\n",
    "# from sklearn.metrics import accuracy_score  # gt, pred\n",
    "\n",
    "# from collections import Counter\n",
    "# from math import isnan\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, n, _ = init_data()\n",
    "\n",
    "# np.random.seed(seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs_simple_lstm = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'char_filter': 100, \n",
    "    'n': n,\n",
    "    'batch_size': n, \n",
    "    'seed': seed(), \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1E-1,  # 1E-4\n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': False,\n",
    "    'hidden_state_size': 8, \n",
    "    'activate_bool': True, \n",
    "    'keep_prob': 0.7, \n",
    "    'epochs': 500,\n",
    "    'summary_step': 5, \n",
    "    'save_step': 10\n",
    "})\n",
    "\n",
    "if kwargs_simple_lstm.del_log: remove_dir_content(kwargs_simple_lstm.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter characters according to 'char_filter',\n",
    "# makes all sequences the same (max) length and pads with 'unknown' character\n",
    "x_char_filtered_pad, statistics_dict = \\\n",
    "    text_filter_pad_to_index(text=x, y=y, **kwargs_simple_lstm)\n",
    "# update main dict with newly calculated figures\n",
    "kwargs_simple_lstm = nice_dict({**kwargs_simple_lstm, **statistics_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create look-up dictionaries (and inverse) for an index representation\n",
    "char_int, char_int_inv, label_int, label_int_inv = \\\n",
    "    lookup_dicts_chars_labels(**kwargs_simple_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "X, _, Y_dense = index_transorm_xy(x=x_char_filtered_pad, \n",
    "                                  y=y, \n",
    "                                  char_int=char_int, \n",
    "                                  label_int=label_int, \n",
    "                                  **kwargs_simple_lstm)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_simple_lstm.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hparam_string(learn_rate, one_hot, keep_prob, char_embed_dim, hidden_state_size, *args, **kwargs):\n",
    "    learn_rate_str = 'learn_rate={:.1E}'.format(learn_rate)\n",
    "    one_hot_str = 'one_hot={:.1}'.format(str(one_hot))\n",
    "    keep_prob_str = 'keep_prob={:.2}'.format(keep_prob)\n",
    "    char_embed_dim_str = 'char_embed_dim={}'.format(char_embed_dim if not one_hot else 'NA')\n",
    "    hidden_state_size_str = 'hidden_state_size={}'.format(hidden_state_size)\n",
    "    output_str = \",\".join([learn_rate_str, \n",
    "                           one_hot_str, \n",
    "                           keep_prob_str, \n",
    "                           char_embed_dim_str, \n",
    "                           hidden_state_size_str])\n",
    "    return '{}/'.format(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn_rate=1.0E-01,one_hot=T,keep_prob=0.8,char_embed_dim=NA,hidden_state_size=32/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_hparam_string(0.1, True, 0.8, 4, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Lstm_model(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 hparam_str, \n",
    "                 n, \n",
    "                 seq_len, \n",
    "                 n_class, \n",
    "                 n_char, \n",
    "                 char_embed_dim, \n",
    "                 one_hot, \n",
    "                 hidden_state_size, \n",
    "                 keep_prob, \n",
    "                 learn_rate, \n",
    "                 epochs, \n",
    "                 log_dir, \n",
    "                 embed_vis_path, \n",
    "                 summary_step, \n",
    "                 save_step, \n",
    "                 seed, \n",
    "                 *args, **kwargs):\n",
    "        self.feed_dict = {}\n",
    "        self.hparam_str = hparam_str\n",
    "        self.n = n\n",
    "        self.seq_len = seq_len \n",
    "        self.n_class = n_class \n",
    "        self.n_char = n_char\n",
    "        self.char_embed_dim = char_embed_dim\n",
    "        self.one_hot = one_hot\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.learn_rate = learn_rate\n",
    "        self.epochs = epochs\n",
    "        self.log_dir = log_dir\n",
    "        self.embed_vis_path = embed_vis_path\n",
    "        self.summary_step = summary_step \n",
    "        self.save_step = save_step\n",
    "        self.seed = seed\n",
    "        # placeholders\n",
    "        self.embedding_matrix = None\n",
    "                \n",
    "        # g = tf.Graph()\n",
    "        # with g.as_default():\n",
    "        #     tf.set_random_seed(1)\n",
    "        \n",
    "#         self.g = tf.Graph()\n",
    "#         self.g.seed = self.seed\n",
    "    #         with self.g.as_default():\n",
    "#         tf.set_random_seed(self.seed)\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "\n",
    "        # Setup placeholders, and reshape the data\n",
    "        self.x_ = tf.placeholder(tf.int32, [self.n, self.seq_len], \n",
    "                            name='Examples')\n",
    "        self.y_ = tf.placeholder(tf.int32, [self.n, self.n_class], \n",
    "                            name='Lables')\n",
    "\n",
    "        self.embedding_matrix = self.embed_matrix()\n",
    "\n",
    "        self.outputs = self.lstm_unit(input=self.x_)\n",
    "\n",
    "        self.logits = self.logit(input=self.outputs, \n",
    "                            size_in=self.hidden_state_size, \n",
    "                            size_out=self.n_class)\n",
    "\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=self.y_), name='cross_entropy')\n",
    "            tf.summary.scalar('cross_entropy', self.cost)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_step = tf.train.AdamOptimizer(\n",
    "                self.learn_rate).minimize(self.cost)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "\n",
    "        # embedding vis\n",
    "        self.embedding_vis = tf.Variable(tf.zeros(self.embedding_matrix.get_shape().as_list()), \n",
    "                                    trainable=False, \n",
    "                                    name='embedding_vis')\n",
    "#         tf.nn.embedding_lookup(embeddings, input)\n",
    "        self.assignment = self.embedding_vis.assign(self.embedding_matrix)\n",
    "\n",
    "        # summaries and saver object\n",
    "        self.summ = tf.summary.merge_all()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # init vars and setup writer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + self.hparam_str)\n",
    "        self.writer.add_graph(self.sess.graph)\n",
    "\n",
    "        # Add embedding tensorboard visualization. Need tensorflow version\n",
    "        self.config = projector.ProjectorConfig()\n",
    "        self.embed = self.config.embeddings.add()\n",
    "        self.embed.tensor_name = self.embedding_vis.name\n",
    "        self.embed.metadata_path = os.path.join(self.embed_vis_path)\n",
    "        projector.visualize_embeddings(self.writer, self.config)\n",
    "\n",
    "        # embedding vis\n",
    "\n",
    "        \n",
    "    def embed_matrix(self, stddev=0.1, name='embeddings'):\n",
    "        # index_size would be the size of the character set\n",
    "        with tf.name_scope(name):\n",
    "            if not self.one_hot:\n",
    "                embedding_matrix = tf.get_variable(\n",
    "                    'embedding_matrix', \n",
    "                    initializer=tf.truncated_normal([self.n_char, self.char_embed_dim], \n",
    "                                                    stddev=stddev, \n",
    "                                                    seed=self.seed), \n",
    "                    trainable=True)\n",
    "            else:\n",
    "                # creating a one-hot for each character corresponds to the identity matrix\n",
    "                embedding_matrix = tf.constant(value=np.identity(self.n_char), \n",
    "                                               name='embedding_matrix', \n",
    "                                               dtype=tf.float32)\n",
    "\n",
    "            tf.summary.histogram('embedding_matrix', embedding_matrix)\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            return self.embedding_matrix\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, \n",
    "                  input, \n",
    "                  name='LSTM'):\n",
    "        with tf.name_scope(name):\n",
    "\n",
    "            rnn_inputs = [tf.squeeze(i) for i in \n",
    "                          tf.split(tf.nn.embedding_lookup(self.embedding_matrix, input),\n",
    "                                   self.seq_len, \n",
    "                                   1)]\n",
    "\n",
    "            cell = rnn.BasicLSTMCell(num_units=self.hidden_state_size)\n",
    "            keep_prob = tf.constant(self.keep_prob)\n",
    "            cell = rnn.DropoutWrapper(cell, \n",
    "                                      output_keep_prob=keep_prob, \n",
    "                                      seed=self.seed)\n",
    "\n",
    "            outputs, states = rnn.static_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "            outputs = outputs[-1]\n",
    "    #         outputs = tf.constant(value=outputs, \n",
    "    #                               name='outputs')\n",
    "            tf.summary.histogram('outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "\n",
    "    def logit(self, \n",
    "              input, \n",
    "              size_in, \n",
    "              size_out, \n",
    "              stddev=0.1, \n",
    "              name='logit'):\n",
    "\n",
    "        with tf.name_scope(name):\n",
    "            w = tf.Variable(tf.truncated_normal([size_in, size_out], \n",
    "                                                stddev=stddev, \n",
    "                                                seed=self.seed), \n",
    "                           name='W')\n",
    "            b = tf.Variable(tf.constant(0.1, \n",
    "                                        shape=[size_out]), \n",
    "                            name='B')\n",
    "            logits = tf.matmul(input, w) + b\n",
    "            tf.summary.histogram('weights', w)\n",
    "            tf.summary.histogram('biases', b)\n",
    "            tf.summary.histogram('logits', logits)\n",
    "            return logits\n",
    "    \n",
    "    \n",
    "    def feed(self, feed_dict):\n",
    "        self.feed_dict = {self.x_: feed_dict['x'], \n",
    "                          self.y_: feed_dict['y']}\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "       \n",
    "        self.sess.graph.seed = self.seed\n",
    "        for i in range(self.epochs):\n",
    "            if (i+1) % self.summary_step == 0:\n",
    "                # minimizing cost (while also tracking accuracy, for summary)\n",
    "#                 [train_accuracy, train_cost, s] = self.sess.run([self.accuracy, self.cost, self.summ], \n",
    "#                                                                 feed_dict=self.feed_dict)\n",
    "                [train_accuracy, train_cost, s] = [self.run_accuracy(), \n",
    "                                                   self.run_cost(), self.run_summary()]\n",
    "                self.writer.add_summary(s, i+1)\n",
    "                print('Epoch number {}, '.format(i+1) +\n",
    "                      'accuracy is {:.5f} and '.format(train_accuracy) + \n",
    "                      'cost is {:.5f}'.format(train_cost))\n",
    "            if (i+1) % self.save_step == 0:\n",
    "                print('Saving step {}'.format(i+1))\n",
    "                self.sess.run(self.assignment, feed_dict=self.feed_dict)\n",
    "                self.saver.save(self.sess, os.path.join(self.log_dir, \n",
    "                                                        self.hparam_str, \n",
    "                                                        'model.ckpt'), i+1)\n",
    "            self.sess.run(self.train_step, feed_dict=self.feed_dict)\n",
    "\n",
    "        self.sess.close()\n",
    "        print('Training is done!')\n",
    "    \n",
    "    \n",
    "    def restore(self, cp_path, feed_dict = None):\n",
    "        \n",
    "        print('Loading variables from {:s}'.format(cp_path))\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(cp_path)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\"no checkpoint found\")\n",
    "\n",
    "        if feed_dict:\n",
    "            self.feed(feed_dict=feed_dict)\n",
    "        print('Loading successful')\n",
    "\n",
    "        \n",
    "    def run_accuracy(self):\n",
    "        train_accuracy = self.sess.run(self.accuracy, \n",
    "                                       feed_dict=self.feed_dict)\n",
    "        return train_accuracy\n",
    "        \n",
    "    def run_cost(self):\n",
    "        train_cost = self.sess.run(self.cost, \n",
    "                                   feed_dict=self.feed_dict)\n",
    "        return train_cost\n",
    "    \n",
    "    def run_summary(self):\n",
    "        train_summary = self.sess.run(self.summ, \n",
    "                                      feed_dict=self.feed_dict)\n",
    "        return train_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kwargs_feed_dict = {'x': X, 'y': Y_dense}\n",
    "\n",
    "hparam_str = make_hparam_string(**kwargs_simple_lstm)\n",
    "\n",
    "lstm = Lstm_model(hparam_str=hparam_str, \n",
    "                  embed_vis_path=embed_vis_path, \n",
    "                  **{**kwargs_simple_lstm, \n",
    "                     **{'epochs': 500}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 5, accuracy is 0.02170 and cost is 5.88356\n",
      "Epoch number 10, accuracy is 0.03895 and cost is 5.82876\n",
      "Saving step 10\n",
      "Epoch number 15, accuracy is 0.03895 and cost is 5.80759\n",
      "Epoch number 20, accuracy is 0.03945 and cost is 5.79686\n",
      "Saving step 20\n",
      "Epoch number 25, accuracy is 0.04093 and cost is 5.77183\n",
      "Epoch number 30, accuracy is 0.04241 and cost is 5.74667\n",
      "Saving step 30\n",
      "Epoch number 35, accuracy is 0.04438 and cost is 5.68487\n",
      "Epoch number 40, accuracy is 0.04783 and cost is 5.60156\n",
      "Saving step 40\n",
      "Epoch number 45, accuracy is 0.04684 and cost is 5.49835\n",
      "Epoch number 50, accuracy is 0.04635 and cost is 5.36914\n",
      "Saving step 50\n",
      "Epoch number 55, accuracy is 0.05671 and cost is 5.21063\n",
      "Epoch number 60, accuracy is 0.06410 and cost is 5.04405\n",
      "Saving step 60\n",
      "Epoch number 65, accuracy is 0.06903 and cost is 5.09188\n",
      "Epoch number 70, accuracy is 0.07150 and cost is 4.92018\n",
      "Saving step 70\n",
      "Epoch number 75, accuracy is 0.07594 and cost is 4.85614\n",
      "Epoch number 80, accuracy is 0.08826 and cost is 4.76643\n",
      "Saving step 80\n",
      "Epoch number 85, accuracy is 0.09073 and cost is 4.71778\n",
      "Epoch number 90, accuracy is 0.09172 and cost is 4.66462\n",
      "Saving step 90\n",
      "Epoch number 95, accuracy is 0.09911 and cost is 4.56637\n",
      "Epoch number 100, accuracy is 0.09418 and cost is 4.54746\n",
      "Saving step 100\n",
      "Epoch number 105, accuracy is 0.10059 and cost is 4.49707\n",
      "Epoch number 110, accuracy is 0.10306 and cost is 4.46376\n",
      "Saving step 110\n",
      "Epoch number 115, accuracy is 0.08531 and cost is 4.54507\n",
      "Epoch number 120, accuracy is 0.10207 and cost is 4.42045\n",
      "Saving step 120\n",
      "Epoch number 125, accuracy is 0.10059 and cost is 4.34315\n",
      "Epoch number 130, accuracy is 0.11193 and cost is 4.30529\n",
      "Saving step 130\n",
      "Epoch number 135, accuracy is 0.10799 and cost is 4.29019\n",
      "Epoch number 140, accuracy is 0.11933 and cost is 4.28133\n",
      "Saving step 140\n",
      "Epoch number 145, accuracy is 0.11933 and cost is 4.18275\n",
      "Epoch number 150, accuracy is 0.12771 and cost is 4.15621\n",
      "Saving step 150\n",
      "Epoch number 155, accuracy is 0.12229 and cost is 4.14067\n",
      "Epoch number 160, accuracy is 0.12870 and cost is 4.11512\n",
      "Saving step 160\n",
      "Epoch number 165, accuracy is 0.12968 and cost is 4.06036\n",
      "Epoch number 170, accuracy is 0.13314 and cost is 4.01604\n",
      "Saving step 170\n",
      "Epoch number 175, accuracy is 0.12968 and cost is 3.99536\n",
      "Epoch number 180, accuracy is 0.12722 and cost is 3.98450\n",
      "Saving step 180\n",
      "Epoch number 185, accuracy is 0.14250 and cost is 3.98201\n",
      "Epoch number 190, accuracy is 0.14398 and cost is 3.99227\n",
      "Saving step 190\n",
      "Epoch number 195, accuracy is 0.13067 and cost is 3.91631\n",
      "Epoch number 200, accuracy is 0.14398 and cost is 3.90383\n",
      "Saving step 200\n",
      "Epoch number 205, accuracy is 0.14596 and cost is 3.85917\n",
      "Epoch number 210, accuracy is 0.14793 and cost is 3.90180\n",
      "Saving step 210\n",
      "Epoch number 215, accuracy is 0.13659 and cost is 3.83075\n",
      "Epoch number 220, accuracy is 0.15385 and cost is 3.83869\n",
      "Saving step 220\n",
      "Epoch number 225, accuracy is 0.16371 and cost is 3.78404\n",
      "Epoch number 230, accuracy is 0.12475 and cost is 4.20606\n",
      "Saving step 230\n",
      "Epoch number 235, accuracy is 0.13807 and cost is 3.94413\n",
      "Epoch number 240, accuracy is 0.15138 and cost is 3.88259\n",
      "Saving step 240\n",
      "Epoch number 245, accuracy is 0.13807 and cost is 3.87629\n",
      "Epoch number 250, accuracy is 0.14744 and cost is 3.77326\n",
      "Saving step 250\n",
      "Epoch number 255, accuracy is 0.15680 and cost is 3.74203\n",
      "Epoch number 260, accuracy is 0.16223 and cost is 3.69883\n",
      "Saving step 260\n",
      "Epoch number 265, accuracy is 0.16815 and cost is 3.65270\n",
      "Epoch number 270, accuracy is 0.16667 and cost is 3.66903\n",
      "Saving step 270\n",
      "Epoch number 275, accuracy is 0.16765 and cost is 3.63288\n",
      "Epoch number 280, accuracy is 0.17653 and cost is 3.63121\n",
      "Saving step 280\n",
      "Epoch number 285, accuracy is 0.15730 and cost is 3.82589\n",
      "Epoch number 290, accuracy is 0.15237 and cost is 3.85595\n",
      "Saving step 290\n",
      "Epoch number 295, accuracy is 0.15039 and cost is 3.79468\n",
      "Epoch number 300, accuracy is 0.17258 and cost is 3.73910\n",
      "Saving step 300\n",
      "Epoch number 305, accuracy is 0.15878 and cost is 3.74747\n",
      "Epoch number 310, accuracy is 0.10404 and cost is 4.70336\n",
      "Saving step 310\n",
      "Epoch number 315, accuracy is 0.10454 and cost is 4.34571\n",
      "Epoch number 320, accuracy is 0.12821 and cost is 4.26624\n",
      "Saving step 320\n",
      "Epoch number 325, accuracy is 0.13215 and cost is 4.07662\n",
      "Epoch number 330, accuracy is 0.13609 and cost is 3.96647\n",
      "Saving step 330\n",
      "Epoch number 335, accuracy is 0.15730 and cost is 3.93538\n",
      "Epoch number 340, accuracy is 0.15730 and cost is 3.82269\n",
      "Saving step 340\n",
      "Epoch number 345, accuracy is 0.16174 and cost is 3.77651\n",
      "Epoch number 350, accuracy is 0.15828 and cost is 3.72411\n",
      "Saving step 350\n",
      "Epoch number 355, accuracy is 0.16667 and cost is 3.72612\n",
      "Epoch number 360, accuracy is 0.15187 and cost is 3.69564\n",
      "Saving step 360\n",
      "Epoch number 365, accuracy is 0.16568 and cost is 3.68665\n",
      "Epoch number 370, accuracy is 0.17554 and cost is 3.61748\n",
      "Saving step 370\n",
      "Epoch number 375, accuracy is 0.18343 and cost is 3.62703\n",
      "Epoch number 380, accuracy is 0.18442 and cost is 3.62310\n",
      "Saving step 380\n",
      "Epoch number 385, accuracy is 0.18294 and cost is 3.56153\n",
      "Epoch number 390, accuracy is 0.17209 and cost is 3.66064\n",
      "Saving step 390\n",
      "Epoch number 395, accuracy is 0.17505 and cost is 3.58658\n",
      "Epoch number 400, accuracy is 0.18195 and cost is 3.53716\n",
      "Saving step 400\n",
      "Epoch number 405, accuracy is 0.18097 and cost is 3.59324\n",
      "Epoch number 410, accuracy is 0.18688 and cost is 3.55416\n",
      "Saving step 410\n",
      "Epoch number 415, accuracy is 0.17456 and cost is 3.57578\n",
      "Epoch number 420, accuracy is 0.18590 and cost is 3.56739\n",
      "Saving step 420\n",
      "Epoch number 425, accuracy is 0.18590 and cost is 3.55114\n",
      "Epoch number 430, accuracy is 0.18343 and cost is 3.53616\n",
      "Saving step 430\n",
      "Epoch number 435, accuracy is 0.20513 and cost is 3.51584\n",
      "Epoch number 440, accuracy is 0.19132 and cost is 3.51216\n",
      "Saving step 440\n",
      "Epoch number 445, accuracy is 0.19822 and cost is 3.47193\n",
      "Epoch number 450, accuracy is 0.20365 and cost is 3.46632\n",
      "Saving step 450\n",
      "Epoch number 455, accuracy is 0.18195 and cost is 3.45478\n",
      "Epoch number 460, accuracy is 0.19329 and cost is 3.47914\n",
      "Saving step 460\n",
      "Epoch number 465, accuracy is 0.18146 and cost is 3.58733\n",
      "Epoch number 470, accuracy is 0.19625 and cost is 3.55905\n",
      "Saving step 470\n",
      "Epoch number 475, accuracy is 0.20020 and cost is 3.48536\n",
      "Epoch number 480, accuracy is 0.19675 and cost is 3.62877\n",
      "Saving step 480\n",
      "Epoch number 485, accuracy is 0.17949 and cost is 3.52638\n",
      "Epoch number 490, accuracy is 0.19527 and cost is 3.49665\n",
      "Saving step 490\n",
      "Epoch number 495, accuracy is 0.19970 and cost is 3.46991\n",
      "Epoch number 500, accuracy is 0.20661 and cost is 3.45590\n",
      "Saving step 500\n",
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "lstm.feed(feed_dict=kwargs_feed_dict)\n",
    "lstm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Epoch number 5, accuracy is 0.01085 and cost is 5.87448\n",
    "# Epoch number 10, accuracy is 0.03797 and cost is 5.79841\n",
    "# Saving step 10\n",
    "# Epoch number 15, accuracy is 0.03945 and cost is 5.71189\n",
    "# Epoch number 20, accuracy is 0.03402 and cost is 5.63142\n",
    "# Saving step 20\n",
    "# Training is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this works after training!\n",
    "# need to check if it's working after calling self.restore()\n",
    "# seems to work after calling self.restore(), yet numbers are not identical\n",
    "# need to solve the seed() problem\n",
    "# lstm.feed(feed_dict=lstm.feed_dict)\n",
    "with lstm.sess.as_default() as sess:\n",
    "# with lstm.sess as sess:\n",
    "#     with lstm.Graph().as_default():\n",
    "#         print(lstm.logits.eval(feed_dict=lstm.feed_dict))\n",
    "    print(lstm.embedding_matrix.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict = {'x': X, 'y': Y_dense}\n",
    "lstm_models = {}\n",
    "for learn_rate in list(np.logspace(-1, -2, 2)):\n",
    "    for keep_prob in [0.7, 1.0]:\n",
    "        for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "#             for char_embed_dim in list(np.linspace(2, 6, 3)):\n",
    "            for hidden_state_size in [4, 32]:\n",
    "                    current_kw_simple_lstm = {\n",
    "                        **kwargs_simple_lstm, \n",
    "                        **{'learn_rate': learn_rate, \n",
    "                           'one_hot': one_hot, \n",
    "                           'keep_prob': keep_prob, \n",
    "                           'char_embed_dim': char_embed_dim, \n",
    "                           'hidden_state_size': hidden_state_size}}\n",
    "                    hparam_str = make_hparam_string(learn_rate, \n",
    "                                                    one_hot, \n",
    "                                                    keep_prob, \n",
    "                                                    char_embed_dim, \n",
    "                                                    hidden_state_size)\n",
    "                    var = 'lstm_{}'.format(hparam_str)\n",
    "#                         print(var)\n",
    "                    lstm_models[var] = Lstm_model(feed_dict=kwargs_feed_dict, \n",
    "                                                  hparam_str=hparam_str, \n",
    "                                                  embed_vis_path=embed_vis_path, \n",
    "                                                  **current_kw_simple_lstm)\n",
    "                    lstm_models[var].train()\n",
    "\n",
    "\n",
    "    \n",
    "#     lstm_models[var] = Lstm_model(feed_dict=kwargs_feed_dict, \n",
    "#                                   hparam_str=hparam_str, \n",
    "#                                   embed_vis_path=embed_vis_path, \n",
    "#                                   **current_kw_simple_lstm)\n",
    "#     lstm_models[var].train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=kwargs_tf_simple.log_dir + 'data', one_hot=True)\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'labels_1024.tsv', kwargs_tf_simple.log_dir + 'labels_1024.tsv')\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'sprite_1024.png', kwargs_tf_simple.log_dir + 'sprite_1024.png')\n",
    "\n",
    "pcp1()\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "def mnist_model(learning_rate, use_two_conv, use_two_fc, hparam):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    tf.set_random_seed(seed())\n",
    "\n",
    "    # Setup placeholders, and reshape the data\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 3)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "    if use_two_conv:\n",
    "        conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "        conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    else:\n",
    "        conv1 = conv_layer(x_image, 1, 64, \"conv\")\n",
    "        conv_out = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "\n",
    "\n",
    "    if use_two_fc:\n",
    "        fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "        embedding_input = fc1\n",
    "        embedding_size = 1024\n",
    "        logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "    else:\n",
    "        embedding_input = flattened\n",
    "        embedding_size = 7*7*64\n",
    "        logits = fc_layer(flattened, 7*7*64, 10, \"fc\")\n",
    "\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y), name=\"xent\")\n",
    "        tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(kwargs_tf_simple.log_dir + hparam)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = kwargs_tf_simple.log_dir + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = kwargs_tf_simple.log_dir + 'labels_1024.tsv'\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    pcp3()\n",
    "    \n",
    "    for i in range(1000 + 1):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            \n",
    "#             pcp4()\n",
    "            \n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "            print('Iteration number {}, Accuracy is currently {}'.format(i, train_accuracy))\n",
    "        if i % 500 == 0:\n",
    "            sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "            saver.save(sess, os.path.join(kwargs_tf_simple.log_dir, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv):\n",
    "    conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "    fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "    return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "\n",
    "def main():\n",
    "    \n",
    "#     tf.set_random_seed(seed())\n",
    "    \n",
    "    # You can try adding some more learning rates\n",
    "    for learning_rate in [1E-4]:\n",
    "\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "        for use_two_fc in [False]:\n",
    "            for use_two_conv in [True]:\n",
    "                # Construct a hyperparameter string for each one (example: \"lr_1E-3,fc=2,conv=2)\n",
    "                \n",
    "                pcp2()\n",
    "                \n",
    "                hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv)\n",
    "                print('Starting run for %s' % hparam)\n",
    "\n",
    "                # Actually run with the new settings\n",
    "                mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
