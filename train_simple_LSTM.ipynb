{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs_neural_data_init = \\\n",
    "    {'mk_chars': True, \n",
    "               'model': 'neural', \n",
    "               'char_filter': 100, 'allowed_chars': None, \n",
    "               'mk_ngrams': False, 'ngram_width': 5, \n",
    "               'ngram_filter': 10, 'allowed_ngrams': None, \n",
    "               'keep_infreq_labels': False, 'label_count_thresh': 10, \n",
    "               'valid_ratio': 0.25, \n",
    "               'scale_func': unscale, 'to_permute': True, }\n",
    "\n",
    "x_feed_train, y_feed_train, x_feed_val, y_feed_val,\\\n",
    "    char_int, char_int_inv, label_int, label_int_inv, \\\n",
    "    statistics_dict =\\\n",
    "    data_load_preprocess(**kwargs_neural_data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs_simple_lstm = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'top_k': 5, \n",
    "    'seed': seed(), \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1E-1,  # 1E-4\n",
    "    'dynamic_learn_rate': False, \n",
    "    'rnn_type': 'LSTM',\n",
    "    'bidirection': False, \n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': False,\n",
    "    'hidden_state_size': 32, \n",
    "    'keep_prob': 0.7, \n",
    "    'l2_wieght_reg': 1E-4, \n",
    "    'target_rep': True, \n",
    "    'target_rep_weight': 0.1, \n",
    "    'epochs': 1000,\n",
    "    'summary_step': 10, \n",
    "    'save_step': np.inf,\n",
    "    'verbose_summary': False\n",
    "})\n",
    "\n",
    "if kwargs_simple_lstm.save_step == np.inf: \n",
    "    kwargs_simple_lstm.save_step = kwargs_simple_lstm.epochs\n",
    "    \n",
    "kwargs_simple_lstm = {**kwargs_simple_lstm, \n",
    "                      **statistics_dict}\n",
    "\n",
    "kwargs_simple_lstm = nice_dict({**kwargs_simple_lstm, \n",
    "                                **{'scale_func': kwargs_neural_data_init['scale_func'], \n",
    "                                   'keep_infreq_labels': kwargs_neural_data_init['keep_infreq_labels']}})\n",
    "\n",
    "if kwargs_simple_lstm.del_log: remove_dir_content(kwargs_simple_lstm.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "look_at_some_examples = False\n",
    "\"\"\"\n",
    "Collect examples from training and validation sets,\n",
    "group by label and print two examples for each \n",
    "(e.g. for each label, print 2 training and 2 validation examples).\n",
    "This was done due to a suspicion raised by similar evaulation metrics on the training and test.\n",
    "\"\"\"\n",
    "if look_at_some_examples:\n",
    "    label_to_text_val = {}  # collect validation examples\n",
    "    for obs,label in zip(x_feed_val, y_feed_val):\n",
    "        label_to_text_val.setdefault(label,[]).append(obs)\n",
    "\n",
    "    label_to_text_train = {}  # collect training examples\n",
    "    for obs,label in zip(x_feed_train, y_feed_train):\n",
    "        label_to_text_train.setdefault(label,[]).append(obs)\n",
    "\n",
    "    unique_keys = list(label_to_text_train.keys())\n",
    "    unique_keys.sort()\n",
    "\n",
    "    label_to_text_merge = {}  # collect both\n",
    "    for key in unique_keys:\n",
    "        label_to_text_merge[key] = {'training': label_to_text_train[key], \n",
    "                                    'validation': label_to_text_val[key]}\n",
    "\n",
    "    for key in unique_keys:\n",
    "        cur_dict = label_to_text_merge[key]\n",
    "        print('Key:{}, training:'.format(key))\n",
    "        print(''.join([char for char in cur_dict['training'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['training'][1] if char != '<pad-char>']))\n",
    "        print('validation:')\n",
    "        print(''.join([char for char in cur_dict['validation'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['validation'][1] if char != '<pad-char>']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "# training data\n",
    "X_train, _, Y_train = index_transorm_xy(x=x_feed_train, \n",
    "                                        y=y_feed_train, \n",
    "                                        char_int=char_int, \n",
    "                                        label_int=label_int, \n",
    "                                        **kwargs_simple_lstm)\n",
    "\n",
    "# validation data\n",
    "X_val, _, Y_val = index_transorm_xy(x=x_feed_val, \n",
    "                                    y=y_feed_val, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    **kwargs_simple_lstm)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_simple_lstm.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Lstm_model(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 *args, \n",
    "                 hparam_str, \n",
    "                 seq_len, \n",
    "                 n_class, \n",
    "                 n_char, \n",
    "                 char_embed_dim, \n",
    "                 one_hot, \n",
    "                 hidden_state_size, \n",
    "                 keep_prob, \n",
    "                 learn_rate, \n",
    "                 dynamic_learn_rate, \n",
    "                 rnn_type, \n",
    "                 bidirection, \n",
    "                 top_k, \n",
    "                 epochs, \n",
    "                 log_dir, \n",
    "                 embed_vis_path, \n",
    "                 summary_step, \n",
    "                 save_step, \n",
    "                 seed, \n",
    "                 l2_wieght_reg, \n",
    "                 target_rep, \n",
    "                 target_rep_weight, \n",
    "                 verbose_summary, \n",
    "                 feed_dict_train, \n",
    "                 feed_dict_test, \n",
    "                 **kwargs):\n",
    "        \n",
    "        self.hparam_str = hparam_str\n",
    "        self.seq_len = seq_len \n",
    "        self.n_class = n_class \n",
    "        self.n_char = n_char\n",
    "        self.char_embed_dim = char_embed_dim\n",
    "        self.one_hot = one_hot\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.learn_rate = learn_rate\n",
    "        self.dynamic_learn_rate = dynamic_learn_rate\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirection = bidirection\n",
    "        self.top_k = top_k\n",
    "        self.epochs = epochs\n",
    "        self.log_dir = log_dir\n",
    "        self.embed_vis_path = embed_vis_path\n",
    "        self.summary_step = summary_step \n",
    "        self.save_step = save_step\n",
    "        self.seed = seed\n",
    "        self.l2_wieght_reg = l2_wieght_reg\n",
    "        self.target_rep = target_rep\n",
    "        self.verbose_summary = verbose_summary\n",
    "        self.target_rep_weight = target_rep_weight if self.target_rep else 0.0\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "        # clear tf graph and set seeds\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        \n",
    "        # Setup placeholders, and reshape the data\n",
    "        self.x_ = tf.placeholder(tf.int32, [None, self.seq_len], \n",
    "                            name='Examples')\n",
    "        self.y_ = tf.placeholder(tf.int32, [None, self.n_class], \n",
    "                            name='Lables')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], \n",
    "                            name='Keep_probability')\n",
    "\n",
    "        self.feed_dict_train = {self.x_: feed_dict_train['x'], \n",
    "                                self.y_: feed_dict_train['y'], \n",
    "                                self.keep_prob: keep_prob}\n",
    "\n",
    "        self.feed_dict_train_eval = {**self.feed_dict_train, \n",
    "                                     **{self.keep_prob: 1.0}}\n",
    "\n",
    "        self.feed_dict_test = {self.x_: feed_dict_test['x'], \n",
    "                               self.y_: feed_dict_test['y'], \n",
    "                               self.keep_prob: 1.0}\n",
    "\n",
    "        self.embedding_matrix = self.embed_matrix()\n",
    "\n",
    "        self.outputs = self.lstm_unit(input=self.x_)\n",
    "        with tf.name_scope('logits_seq'):\n",
    "            if self.bidirection: logit_in_size = 2 * self.hidden_state_size\n",
    "            else: logit_in_size = self.hidden_state_size\n",
    "            self.logits = [self.logit(input=out, \n",
    "                                      size_in=logit_in_size, \n",
    "                                      size_out=self.n_class) \n",
    "                           for out in self.outputs]\n",
    "\n",
    "        with tf.name_scope('Cost_function'):\n",
    "            # cross entropy loss with target replication and\n",
    "            # regularization terms based on the weights' L2 norm\n",
    "            with tf.name_scope('target_replication_loss'):\n",
    "                self.cost_targetrep = tf.reduce_mean(\n",
    "                    [tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=log, labels=self.y_) \n",
    "                     for log in self.logits])\n",
    "            with tf.name_scope('cross_entropy'):\n",
    "                self.cost_crossent = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=self.logits[-1], labels=self.y_))\n",
    "            with tf.name_scope('L2_norm_reg'):\n",
    "                self.cost_l2reg = tf.reduce_mean([tf.nn.l2_loss(weight) \n",
    "                                                  for weight in tf.trainable_variables()])\n",
    "            with tf.name_scope('total_cost'):\n",
    "                self.cost = self.target_rep_weight * self.cost_targetrep + \\\n",
    "                    (1 - self.target_rep_weight) * self.cost_crossent + \\\n",
    "                    self.l2_wieght_reg * self.cost_l2reg\n",
    "            # add summaries\n",
    "            tf.summary.scalar('Total_cost_train', \n",
    "                              self.cost, collections=['train'])\n",
    "            tf.summary.scalar('Total_cost_test', \n",
    "                              self.cost, collections=['test'])\n",
    "            \n",
    "        with tf.name_scope('Cost_function_additional_metrics'):\n",
    "            tf.summary.scalar('Target_rep_cost_train', \n",
    "                              self.cost_targetrep, collections=['train'])\n",
    "            tf.summary.scalar('Target_rep_cost_test', \n",
    "                              self.cost_targetrep, collections=['test'])\n",
    "            tf.summary.scalar('Cross_entropy_train', \n",
    "                              self.cost_crossent, collections=['train'])\n",
    "            tf.summary.scalar('Cross_entropy_test', \n",
    "                              self.cost_crossent, collections=['test'])\n",
    "            tf.summary.scalar('L2_norm_train', \n",
    "                              self.cost_l2reg, collections=['train'])\n",
    "            tf.summary.scalar('L2_norm_test', \n",
    "                              self.cost_l2reg, collections=['test'])            \n",
    "            \n",
    "        with tf.name_scope('Train'):\n",
    "            if self.dynamic_learn_rate:\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(self.learn_rate)\n",
    "            else:\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learn_rate)\n",
    "            self.train_step = self.optimizer.minimize(self.cost)\n",
    "\n",
    "        with tf.name_scope('Accuracy'):  # takes the last element of logits\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.logits[-1], 1), tf.argmax(self.y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('Accuracy_train', self.accuracy, collections=['train'])\n",
    "            tf.summary.scalar('Accuracy_test', self.accuracy, collections=['test'])\n",
    "        \n",
    "        with tf.name_scope('Mean_Reciprocal_Rank'):  # takes the last element of logits\n",
    "            self.recip_rank = tf.reduce_mean(\n",
    "                self.get_reciprocal_rank(self.logits[-1], \n",
    "                                         self.y_, \n",
    "                                         True))\n",
    "            tf.summary.scalar('Mean_Reciprocal_Rank_train', \n",
    "                              self.recip_rank, collections=['train'])\n",
    "            tf.summary.scalar('Mean_Reciprocal_Rank_test', \n",
    "                              self.recip_rank, collections=['test'])        \n",
    "        \n",
    "        with tf.name_scope('In_top_{}'.format(self.top_k)):  # takes the last element of logits\n",
    "            self.y_targets = tf.argmax(self.y_, 1)\n",
    "            self.top_k_res = tf.reduce_mean(tf.cast(\n",
    "                tf.nn.in_top_k(self.logits[-1], self.y_targets, self.top_k), \n",
    "                tf.float32))\n",
    "            tf.summary.scalar('In_top_{}_train'.format(self.top_k), self.top_k_res, collections=['train'])\n",
    "            tf.summary.scalar('In_top_{}_test'.format(self.top_k), self.top_k_res, collections=['test'])\n",
    "\n",
    "        # summaries per collection and saver object\n",
    "        self.summ_train = tf.summary.merge_all('train')\n",
    "        self.summ_test = tf.summary.merge_all('test')\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        # init vars and setup writer\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init_op)\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + self.hparam_str)\n",
    "        self.writer.add_graph(self.sess.graph)\n",
    "        \n",
    "        # Add embedding tensorboard visualization. Need tensorflow version\n",
    "        self.config = projector.ProjectorConfig()\n",
    "        self.embed = self.config.embeddings.add()\n",
    "        self.embed.tensor_name = self.embedding_matrix.name\n",
    "        self.embed.metadata_path = os.path.join(self.embed_vis_path)\n",
    "        projector.visualize_embeddings(self.writer, self.config)\n",
    "        \n",
    "        \n",
    "    def embed_matrix(self, stddev=0.1, name='embeddings'):\n",
    "        # index_size would be the size of the character set\n",
    "        with tf.name_scope(name):\n",
    "            if not self.one_hot:\n",
    "                embedding_matrix = tf.get_variable(\n",
    "                    'embedding_matrix', \n",
    "                    initializer=tf.truncated_normal([self.n_char, self.char_embed_dim], \n",
    "                                                    stddev=stddev, \n",
    "                                                    seed=self.seed), \n",
    "                    trainable=True)\n",
    "            else:\n",
    "                # creating a one-hot for each character corresponds to the identity matrix\n",
    "                embedding_matrix = tf.constant(value=np.identity(self.n_char), \n",
    "                                               name='embedding_matrix', \n",
    "                                               dtype=tf.float32)\n",
    "                self.char_embed_dim = self.n_char\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('embedding_matrix', embedding_matrix, collections=['train'])\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            return self.embedding_matrix\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, \n",
    "                  input, \n",
    "                  name='LSTM'):\n",
    "        # check, then set the right name\n",
    "        assert self.rnn_type in ['LSTM', 'GRU'], \\\n",
    "            'rnn_type has to be either LSTM or GRU'\n",
    "        name = 'LSTM' if self.rnn_type == 'LSTM' else 'GRU'\n",
    "        if self.bidirection: name += '_bidir'\n",
    "        with tf.name_scope(name):\n",
    "            input = tf.nn.embedding_lookup(self.embedding_matrix, input)\n",
    "            # reshaping\n",
    "            # Permuting batch_size and n_steps\n",
    "            input = tf.transpose(input, [1, 0, 2])\n",
    "            # Reshaping to (n_steps*batch_size, n_input)\n",
    "            input = tf.reshape(input, [-1, self.char_embed_dim])\n",
    "            # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "            rnn_inputs = tf.split(input, self.seq_len, 0)\n",
    "            \n",
    "            # setting the correct RNN cell type (LSTM of GRU)\n",
    "            rnn_cell = rnn.BasicLSTMCell if self.rnn_type == 'LSTM' \\\n",
    "                else rnn.GRUCell\n",
    "            # setting the args (forget_bias applies only to LSTM)\n",
    "            rnn_cell_args = {'num_units': self.hidden_state_size}\n",
    "            if 'LSTMCell' in str(rnn_cell.__call__ ):\n",
    "                rnn_cell_args['forget_bias'] = 1.0\n",
    "            rnn_cell(**rnn_cell_args)\n",
    "            \n",
    "            cell_fw = rnn_cell(**rnn_cell_args)\n",
    "            cell_fw = rnn.DropoutWrapper(cell_fw, \n",
    "                                         output_keep_prob=self.keep_prob, \n",
    "                                         seed=self.seed)\n",
    "            \n",
    "            if self.bidirection:\n",
    "                # add another cell for backwards direction and a dropout wrapper\n",
    "                cell_bw = rnn_cell(**rnn_cell_args)\n",
    "                cell_bw = rnn.DropoutWrapper(cell_bw, \n",
    "                                             output_keep_prob=self.keep_prob, \n",
    "                                             seed=self.seed)\n",
    "                outputs, _, _ = rnn.static_bidirectional_rnn(\n",
    "                    cell_fw, cell_bw, rnn_inputs, dtype=tf.float32, scope=name)\n",
    "            else:\n",
    "                outputs, _ = rnn.static_rnn(cell_fw, rnn_inputs, dtype=tf.float32, scope=name)\n",
    "            \n",
    "            if not self.target_rep:  # take only last output (list for structure consistency)\n",
    "                outputs = [outputs[-1]]\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('outputs', outputs, collections=['train'])\n",
    "            return outputs\n",
    "\n",
    "\n",
    "    def logit(self, \n",
    "              input, \n",
    "              size_in, \n",
    "              size_out, \n",
    "              stddev=0.1, \n",
    "              name='logit'):\n",
    "\n",
    "        with tf.name_scope(name):\n",
    "            w = tf.Variable(tf.truncated_normal([size_in, size_out], \n",
    "                                                stddev=stddev, \n",
    "                                                seed=self.seed), \n",
    "                            name='W')\n",
    "            b = tf.Variable(tf.constant(0.1, \n",
    "                                        shape=[size_out]), \n",
    "                            name='B')\n",
    "            logits = tf.matmul(input, w) + b\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('weights', w, collections=['train'])\n",
    "                tf.summary.histogram('biases', b, collections=['train'])\n",
    "                tf.summary.histogram('logits', logits, collections=['train'])\n",
    "            return logits\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        print('Starting to train model {:s}'.format(self.hparam_str))\n",
    "        for i in range(1, self.epochs+1):\n",
    "            # update learning rate, if it is dynamic\n",
    "            if self.dynamic_learn_rate: self.update_lr(epoch=i)\n",
    "            # train step\n",
    "            self.sess.run(self.train_step, feed_dict=self.feed_dict_train)\n",
    "            if i % self.summary_step == 0:\n",
    "                # train summary\n",
    "                # use self.feed_dict_train_eval for evaluation (keep probability set to 1.0)\n",
    "                [train_accuracy, train_cost,_ , _, _, _, train_top_k, s] = \\\n",
    "                    self.sess.run([self.accuracy, \n",
    "                                   self.cost, self.cost_targetrep, self.cost_crossent, self.cost_l2reg, \n",
    "                                   self.recip_rank, \n",
    "                                   self.top_k_res, \n",
    "                                   self.summ_train],\n",
    "                                  feed_dict=self.feed_dict_train_eval)\n",
    "                self.writer.add_summary(s, i)\n",
    "                print('{:.3f} of observations in the top is {}'.format(train_top_k, self.top_k))\n",
    "                # test summary\n",
    "                [test_accuracy, test_cost,_ , _, _, _, test_top_k, s] = \\\n",
    "                    self.sess.run([self.accuracy, \n",
    "                                   self.cost, self.cost_targetrep, self.cost_crossent, self.cost_l2reg, \n",
    "                                   self.recip_rank, \n",
    "                                   self.top_k_res, \n",
    "                                   self.summ_test],\n",
    "                                  feed_dict=self.feed_dict_test)\n",
    "                self.writer.add_summary(s, i)\n",
    "                \n",
    "                print('Epoch number {}, '.format(i) +\n",
    "                      'training accuracy is {:.5f} and '.format(train_accuracy) + \n",
    "                      'test accuracy is {:.5f}, '.format(test_accuracy))\n",
    "                print('training cost is {:.5f} and '.format(train_cost) + \n",
    "                      'test cost is {:.5f} and '.format(test_cost))\n",
    "                \n",
    "            if i % self.save_step == 0:\n",
    "                print('Saving step {}'.format(i))\n",
    "                self.saver.save(self.sess, os.path.join(self.log_dir, \n",
    "                                                        self.hparam_str, \n",
    "                                                        'model.ckpt'), i)\n",
    "            \n",
    "        print('Training the model is done! ({:s})'.format(self.hparam_str))\n",
    "    \n",
    "    \n",
    "    def tf_get_rank_order(self, input, reciprocal):\n",
    "        \"\"\"\n",
    "        Returns a tensor of the rank of the input tensor's elements.\n",
    "        rank(highest element) = 1.\n",
    "        \"\"\"\n",
    "        assert isinstance(reciprocal, bool), 'reciprocal has to be bool'\n",
    "        size = tf.size(input)\n",
    "        indices_of_ranks = tf.nn.top_k(-input, k=size)[1]\n",
    "        indices_of_ranks = size - tf.nn.top_k(-indices_of_ranks, k=size)[1]\n",
    "        if reciprocal:\n",
    "            indices_of_ranks = tf.cast(indices_of_ranks, tf.float32)\n",
    "            indices_of_ranks = tf.map_fn(\n",
    "                lambda x: tf.reciprocal(x), indices_of_ranks, \n",
    "                dtype=tf.float32)\n",
    "            return indices_of_ranks\n",
    "        else:\n",
    "            return indices_of_ranks\n",
    "    \n",
    "    \n",
    "    def get_reciprocal_rank(self, logits, targets, reciprocal=True):\n",
    "        \"\"\"\n",
    "        Returns a tensor containing the (reciprocal) ranks\n",
    "        of the logits tensor (wrt the targets tensor).\n",
    "        The targets tensor should be a 'one hot' vector \n",
    "        (otherwise apply one_hot on targets, such that index_mask is a one_hot).\n",
    "        \"\"\"\n",
    "        function_to_map = lambda x: self.tf_get_rank_order(x, reciprocal=reciprocal)\n",
    "        ordered_array_dtype = tf.float32 if reciprocal is not None else tf.int32\n",
    "        ordered_array = tf.map_fn(function_to_map, logits, \n",
    "                                  dtype=ordered_array_dtype)\n",
    "\n",
    "        size = int(logits.shape[1])\n",
    "        index_mask = tf.reshape(\n",
    "                targets, [-1,size])\n",
    "        if reciprocal:\n",
    "            index_mask = tf.cast(index_mask, tf.float32)\n",
    "\n",
    "        return tf.reduce_sum(ordered_array * index_mask,1)\n",
    "    \n",
    "    \n",
    "    def restore(self, cp_path, feed_dict = None):\n",
    "        \n",
    "        print('Loading variables from {:s}'.format(cp_path))\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(cp_path)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\"no checkpoint found\")\n",
    "\n",
    "#         if feed_dict:\n",
    "#             self.feed(feed_dict=feed_dict)\n",
    "        print('Loading successful')\n",
    "    \n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "    \n",
    "    def update_lr(self, epoch):\n",
    "        self.learn_rate = 1.0 / np.sqrt(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "# kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "# hparam_str = make_hparam_string(**kwargs_simple_lstm)\n",
    "\n",
    "# lstm = Lstm_model(hparam_str=hparam_str, \n",
    "#                   embed_vis_path=embed_vis_path, \n",
    "#                   feed_dict_train=kwargs_feed_dict_train, \n",
    "#                   feed_dict_test=kwargs_feed_dict_test, \n",
    "# #                   **{**kwargs_simple_lstm, \n",
    "# #                      **{'epochs': 40}}\n",
    "#                   **kwargs_simple_lstm\n",
    "#                  )\n",
    "\n",
    "# lstm.train()\n",
    "# lstm.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.sess.run(lstm.outputs, feed_dict=lstm.feed_dict_train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "lstm_models = {}\n",
    "for learn_rate in list(np.logspace(-2, -3, 2)):\n",
    "    for keep_prob in [0.8]:\n",
    "        for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "            for hidden_state_size in [64, 128]:\n",
    "                for l2_wieght_reg in list(np.logspace(-3, -4, 2)):\n",
    "                    for target_rep_weight in [0.1, 0.3]:\n",
    "                        # collect new hyperparameters as args\n",
    "                        current_kw_simple_lstm = {\n",
    "                            **kwargs_simple_lstm, \n",
    "                            **{'learn_rate': learn_rate, \n",
    "                               'keep_prob': keep_prob, \n",
    "                               'one_hot': one_hot, \n",
    "                               'char_embed_dim': char_embed_dim, \n",
    "                               'hidden_state_size': hidden_state_size, \n",
    "                               'l2_wieght_reg': l2_wieght_reg, \n",
    "                               'target_rep_weight': target_rep_weight\n",
    "        #                        'bidirection': bidirection, \n",
    "        #                        'target_rep': target_rep\n",
    "                              }}\n",
    "                        hparam_str = make_hparam_string(**current_kw_simple_lstm)\n",
    "                        var = 'lstm_{}'.format(hparam_str)\n",
    "                        lstm_models[var] = Lstm_model(feed_dict_train=kwargs_feed_dict_train, \n",
    "                                                      feed_dict_test=kwargs_feed_dict_test, \n",
    "                                                      hparam_str=hparam_str, \n",
    "                                                      embed_vis_path=embed_vis_path, \n",
    "                                                      **current_kw_simple_lstm)\n",
    "                        lstm_models[var].train()\n",
    "                        lstm_models[var].close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting data directly from a tensorboard log dir\n",
    "from tensorflow.python.summary import event_multiplexer\n",
    "# specify path (for parent log dir)\n",
    "log_parent_dir = './logdir_exper_4_3/'\n",
    "ea = event_multiplexer.EventMultiplexer().AddRunsFromDirectory(log_parent_dir)\n",
    "ea.Reload()  # load\n",
    "\n",
    "child_dir = next(os.walk(log_parent_dir))[1]\n",
    "print(ea.Scalars(child_dir[0], 'accuracy/accuracy_test'))  # specify run, scalar_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
