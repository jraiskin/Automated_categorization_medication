{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(seed())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "\n",
    "# from collections import Counter\n",
    "# from math import ceil\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize data from main (original) CSV file\n",
    "x, y, n, main_data = init_data()\n",
    "freq = [i for i in main_data['CNT'][:n]]  # frequencies, turned into a list\n",
    "# initialize data from suggestions CSV file\n",
    "x_suggest, y_suggest, freq_suggest = init_data_suggest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory was not found.\n"
     ]
    }
   ],
   "source": [
    "kwargs_simple_lstm = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'char_filter': 100, \n",
    "    'n': n, \n",
    "#     'batch_size': n, \n",
    "    'scale_func': log_scale, \n",
    "    'to_permute': True, \n",
    "    'seed': seed(), \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1E-1,  # 1E-4\n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': False,\n",
    "    'hidden_state_size': 8, \n",
    "#     'activate_bool': True, \n",
    "    'keep_prob': 0.7, \n",
    "    'epochs': 500,\n",
    "    'summary_step': 5, \n",
    "    'save_step': 10\n",
    "})\n",
    "\n",
    "if kwargs_simple_lstm.del_log: remove_dir_content(kwargs_simple_lstm.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter characters according to 'char_filter',\n",
    "# makes all sequences the same (max) length and pads with 'unknown' character\n",
    "x_char_filtered_pad, statistics_dict = \\\n",
    "    text_filter_pad_to_index(text=x, y=y, **kwargs_simple_lstm)\n",
    "# update main dict with newly calculated figures\n",
    "kwargs_simple_lstm = nice_dict({**kwargs_simple_lstm, **statistics_dict})\n",
    "\n",
    "# create look-up dictionaries (and inverse) for an index representation\n",
    "char_int, char_int_inv, label_int, label_int_inv = \\\n",
    "    lookup_dicts_chars_labels(**kwargs_simple_lstm)\n",
    "\n",
    "# transform x_suggest in a similar manner\n",
    "# taking into consideration the given character set\n",
    "x_suggest_char_filtered_pad, statistics_dict = \\\n",
    "    text_filter_pad_to_index(text=x_suggest, y=y_suggest, **kwargs_simple_lstm)\n",
    "\n",
    "# check that there are no \"new\" statistics popping out\n",
    "assert_no_stats_change(new_dict=statistics_dict, \n",
    "                       kwargs=kwargs_simple_lstm)\n",
    "\n",
    "# merge original and suggested data\n",
    "x_merge, y_merge, freq_merge = \\\n",
    "    x_char_filtered_pad + x_suggest_char_filtered_pad, \\\n",
    "    y + y_suggest, \\\n",
    "    freq + freq_suggest\n",
    "# y_merge = y + y_suggest\n",
    "# freq_merge = freq + freq_suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of potential labels to draw from: 82\n",
      "number of potential observation to draw from: 1587\n",
      "365 observations sampled for validation\n",
      "2919 total number of observations\n",
      "2554 observations for training\n",
      "The ratio of validation to total observations is about 0.125\n"
     ]
    }
   ],
   "source": [
    "# split to training and validation sets\n",
    "x_val, x_train, y_val, y_train, freq_val, freq_train, valid_index = \\\n",
    "    train_validation_split(x=x_merge, y=y_merge, freq=freq_merge, \n",
    "                           label_count_thresh=10, \n",
    "                           valid_ratio=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n",
      "2554\n",
      "365\n",
      "2554\n",
      "365\n",
      "2554\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(len([x_val, x_train, y_val, y_train, freq_val, freq_train][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 7, 8, 18, 22, 36, 37, 43, 50]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_index[:10]\n",
    "# [1, 9, 11, 20, 21, 22, 24, 26, 27, 35]\n",
    "# [2, 3, 9, 10, 15, 16, 18, 20, 22, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scale data (proportional to frequency)\n",
    "x_scaled, y_scaled, kwargs_simple_lstm['n'] = \\\n",
    "    scale_permute_data(x=x_merge, \n",
    "                       y=y_merge, \n",
    "                       freq=freq_merge, \n",
    "                       scale_func=kwargs_simple_lstm.scale_func, \n",
    "                       to_permute=kwargs_simple_lstm.to_permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_feed, y_feed = x_scaled, y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "X, _, Y_dense = index_transorm_xy(x=x_feed, \n",
    "                                  y=y_feed, \n",
    "                                  char_int=char_int, \n",
    "                                  label_int=label_int, \n",
    "                                  **kwargs_simple_lstm)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_simple_lstm.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Lstm_model(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 hparam_str, \n",
    "                 n, \n",
    "                 seq_len, \n",
    "                 n_class, \n",
    "                 n_char, \n",
    "                 char_embed_dim, \n",
    "                 one_hot, \n",
    "                 hidden_state_size, \n",
    "                 keep_prob, \n",
    "                 learn_rate, \n",
    "                 epochs, \n",
    "                 log_dir, \n",
    "                 embed_vis_path, \n",
    "                 summary_step, \n",
    "                 save_step, \n",
    "                 seed, \n",
    "                 *args, **kwargs):\n",
    "        self.feed_dict = {}\n",
    "        self.hparam_str = hparam_str\n",
    "        self.n = n\n",
    "        self.seq_len = seq_len \n",
    "        self.n_class = n_class \n",
    "        self.n_char = n_char\n",
    "        self.char_embed_dim = char_embed_dim\n",
    "        self.one_hot = one_hot\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.learn_rate = learn_rate\n",
    "        self.epochs = epochs\n",
    "        self.log_dir = log_dir\n",
    "        self.embed_vis_path = embed_vis_path\n",
    "        self.summary_step = summary_step \n",
    "        self.save_step = save_step\n",
    "        self.seed = seed\n",
    "        # placeholders\n",
    "        self.embedding_matrix = None\n",
    "                \n",
    "        # g = tf.Graph()\n",
    "        # with g.as_default():\n",
    "        #     tf.set_random_seed(1)\n",
    "        \n",
    "#         self.g = tf.Graph()\n",
    "#         self.g.seed = self.seed\n",
    "    #         with self.g.as_default():\n",
    "#         tf.set_random_seed(self.seed)\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "\n",
    "        # Setup placeholders, and reshape the data\n",
    "        self.x_ = tf.placeholder(tf.int32, [self.n, self.seq_len], \n",
    "                            name='Examples')\n",
    "        self.y_ = tf.placeholder(tf.int32, [self.n, self.n_class], \n",
    "                            name='Lables')\n",
    "\n",
    "        self.embedding_matrix = self.embed_matrix()\n",
    "\n",
    "        self.outputs = self.lstm_unit(input=self.x_)\n",
    "\n",
    "        self.logits = self.logit(input=self.outputs, \n",
    "                            size_in=self.hidden_state_size, \n",
    "                            size_out=self.n_class)\n",
    "\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=self.y_), name='cross_entropy')\n",
    "            tf.summary.scalar('cross_entropy', self.cost)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_step = tf.train.AdamOptimizer(\n",
    "                self.learn_rate).minimize(self.cost)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "\n",
    "        # embedding vis\n",
    "        self.embedding_vis = tf.Variable(tf.zeros(self.embedding_matrix.get_shape().as_list()), \n",
    "                                    trainable=False, \n",
    "                                    name='embedding_vis')\n",
    "#         tf.nn.embedding_lookup(embeddings, input)\n",
    "        self.assignment = self.embedding_vis.assign(self.embedding_matrix)\n",
    "\n",
    "        # summaries and saver object\n",
    "        self.summ = tf.summary.merge_all()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # init vars and setup writer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + self.hparam_str)\n",
    "        self.writer.add_graph(self.sess.graph)\n",
    "\n",
    "        # Add embedding tensorboard visualization. Need tensorflow version\n",
    "        self.config = projector.ProjectorConfig()\n",
    "        self.embed = self.config.embeddings.add()\n",
    "        self.embed.tensor_name = self.embedding_vis.name\n",
    "        self.embed.metadata_path = os.path.join(self.embed_vis_path)\n",
    "        projector.visualize_embeddings(self.writer, self.config)\n",
    "\n",
    "        # embedding vis\n",
    "\n",
    "        \n",
    "    def embed_matrix(self, stddev=0.1, name='embeddings'):\n",
    "        # index_size would be the size of the character set\n",
    "        with tf.name_scope(name):\n",
    "            if not self.one_hot:\n",
    "                embedding_matrix = tf.get_variable(\n",
    "                    'embedding_matrix', \n",
    "                    initializer=tf.truncated_normal([self.n_char, self.char_embed_dim], \n",
    "                                                    stddev=stddev, \n",
    "                                                    seed=self.seed), \n",
    "                    trainable=True)\n",
    "            else:\n",
    "                # creating a one-hot for each character corresponds to the identity matrix\n",
    "                embedding_matrix = tf.constant(value=np.identity(self.n_char), \n",
    "                                               name='embedding_matrix', \n",
    "                                               dtype=tf.float32)\n",
    "\n",
    "            tf.summary.histogram('embedding_matrix', embedding_matrix)\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            return self.embedding_matrix\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, \n",
    "                  input, \n",
    "                  name='LSTM'):\n",
    "        with tf.name_scope(name):\n",
    "\n",
    "            rnn_inputs = [tf.squeeze(i) for i in \n",
    "                          tf.split(tf.nn.embedding_lookup(self.embedding_matrix, input),\n",
    "                                   self.seq_len, \n",
    "                                   1)]\n",
    "\n",
    "            cell = rnn.BasicLSTMCell(num_units=self.hidden_state_size)\n",
    "            keep_prob = tf.constant(self.keep_prob)\n",
    "            cell = rnn.DropoutWrapper(cell, \n",
    "                                      output_keep_prob=keep_prob, \n",
    "                                      seed=self.seed)\n",
    "\n",
    "            outputs, states = rnn.static_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "            outputs = outputs[-1]\n",
    "    #         outputs = tf.constant(value=outputs, \n",
    "    #                               name='outputs')\n",
    "            tf.summary.histogram('outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "\n",
    "    def logit(self, \n",
    "              input, \n",
    "              size_in, \n",
    "              size_out, \n",
    "              stddev=0.1, \n",
    "              name='logit'):\n",
    "\n",
    "        with tf.name_scope(name):\n",
    "            w = tf.Variable(tf.truncated_normal([size_in, size_out], \n",
    "                                                stddev=stddev, \n",
    "                                                seed=self.seed), \n",
    "                           name='W')\n",
    "            b = tf.Variable(tf.constant(0.1, \n",
    "                                        shape=[size_out]), \n",
    "                            name='B')\n",
    "            logits = tf.matmul(input, w) + b\n",
    "            tf.summary.histogram('weights', w)\n",
    "            tf.summary.histogram('biases', b)\n",
    "            tf.summary.histogram('logits', logits)\n",
    "            return logits\n",
    "    \n",
    "    \n",
    "    def feed(self, feed_dict):\n",
    "        self.feed_dict = {self.x_: feed_dict['x'], \n",
    "                          self.y_: feed_dict['y']}\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        print('Starting to train model {:s}'.format(self.hparam_str))\n",
    "        \n",
    "        self.sess.graph.seed = self.seed\n",
    "        for i in range(self.epochs):\n",
    "            if (i+1) % self.summary_step == 0:\n",
    "                # minimizing cost (while also tracking accuracy, for summary)\n",
    "#                 [train_accuracy, train_cost, s] = self.sess.run([self.accuracy, self.cost, self.summ], \n",
    "#                                                                 feed_dict=self.feed_dict)\n",
    "                [train_accuracy, train_cost, s] = [self.run_accuracy(), \n",
    "                                                   self.run_cost(), self.run_summary()]\n",
    "                self.writer.add_summary(s, i+1)\n",
    "                print('Epoch number {}, '.format(i+1) +\n",
    "                      'accuracy is {:.5f} and '.format(train_accuracy) + \n",
    "                      'cost is {:.5f}'.format(train_cost))\n",
    "            if (i+1) % self.save_step == 0:\n",
    "                print('Saving step {}'.format(i+1))\n",
    "                self.sess.run(self.assignment, feed_dict=self.feed_dict)\n",
    "                self.saver.save(self.sess, os.path.join(self.log_dir, \n",
    "                                                        self.hparam_str, \n",
    "                                                        'model.ckpt'), i+1)\n",
    "            self.sess.run(self.train_step, feed_dict=self.feed_dict)\n",
    "\n",
    "#         self.sess.close()\n",
    "        print('Training model {:s} is done!'.format(self.hparam_str))\n",
    "    \n",
    "    \n",
    "    def restore(self, cp_path, feed_dict = None):\n",
    "        \n",
    "        print('Loading variables from {:s}'.format(cp_path))\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(cp_path)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\"no checkpoint found\")\n",
    "\n",
    "        if feed_dict:\n",
    "            self.feed(feed_dict=feed_dict)\n",
    "        print('Loading successful')\n",
    "\n",
    "        \n",
    "    def run_accuracy(self):\n",
    "        train_accuracy = self.sess.run(self.accuracy, \n",
    "                                       feed_dict=self.feed_dict)\n",
    "        return train_accuracy\n",
    "        \n",
    "    def run_cost(self):\n",
    "        train_cost = self.sess.run(self.cost, \n",
    "                                   feed_dict=self.feed_dict)\n",
    "        return train_cost\n",
    "    \n",
    "    def run_summary(self):\n",
    "        train_summary = self.sess.run(self.summ, \n",
    "                                      feed_dict=self.feed_dict)\n",
    "        return train_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kwargs_feed_dict = {'x': X, 'y': Y_dense}\n",
    "\n",
    "hparam_str = make_hparam_string(**kwargs_simple_lstm)\n",
    "\n",
    "lstm = Lstm_model(hparam_str=hparam_str, \n",
    "                  embed_vis_path=embed_vis_path, \n",
    "                  **{**kwargs_simple_lstm, \n",
    "                     **{'epochs': 30}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm.feed(feed_dict=kwargs_feed_dict)\n",
    "lstm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this works after training!\n",
    "# need to check if it's working after calling self.restore()\n",
    "# seems to work after calling self.restore(), yet numbers are not identical\n",
    "# need to solve the seed() problem\n",
    "# lstm.feed(feed_dict=lstm.feed_dict)\n",
    "with lstm.sess.as_default() as sess:\n",
    "# with lstm.sess as sess:\n",
    "#     with lstm.Graph().as_default():\n",
    "#         print(lstm.logits.eval(feed_dict=lstm.feed_dict))\n",
    "    print(lstm.embedding_matrix.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict = {'x': X, 'y': Y_dense}\n",
    "lstm_models = {}\n",
    "for learn_rate in list(np.logspace(-1, -2, 2)):\n",
    "    for keep_prob in [0.7, 1.0]:\n",
    "        for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "#             for char_embed_dim in list(np.linspace(2, 6, 3)):\n",
    "            for hidden_state_size in [4, 32]:\n",
    "                    current_kw_simple_lstm = {\n",
    "                        **kwargs_simple_lstm, \n",
    "                        **{'learn_rate': learn_rate, \n",
    "                           'one_hot': one_hot, \n",
    "                           'keep_prob': keep_prob, \n",
    "                           'char_embed_dim': char_embed_dim, \n",
    "                           'hidden_state_size': hidden_state_size}}\n",
    "                    hparam_str = make_hparam_string(learn_rate, \n",
    "                                                    one_hot, \n",
    "                                                    keep_prob, \n",
    "                                                    char_embed_dim, \n",
    "                                                    hidden_state_size)\n",
    "                    var = 'lstm_{}'.format(hparam_str)\n",
    "#                         print(var)\n",
    "                    lstm_models[var] = Lstm_model(feed_dict=kwargs_feed_dict, \n",
    "                                                  hparam_str=hparam_str, \n",
    "                                                  embed_vis_path=embed_vis_path, \n",
    "                                                  **current_kw_simple_lstm)\n",
    "                    lstm_models[var].train()\n",
    "\n",
    "\n",
    "    \n",
    "#     lstm_models[var] = Lstm_model(feed_dict=kwargs_feed_dict, \n",
    "#                                   hparam_str=hparam_str, \n",
    "#                                   embed_vis_path=embed_vis_path, \n",
    "#                                   **current_kw_simple_lstm)\n",
    "#     lstm_models[var].train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=kwargs_tf_simple.log_dir + 'data', one_hot=True)\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'labels_1024.tsv', kwargs_tf_simple.log_dir + 'labels_1024.tsv')\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'sprite_1024.png', kwargs_tf_simple.log_dir + 'sprite_1024.png')\n",
    "\n",
    "pcp1()\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "def mnist_model(learning_rate, use_two_conv, use_two_fc, hparam):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    tf.set_random_seed(seed())\n",
    "\n",
    "    # Setup placeholders, and reshape the data\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 3)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "    if use_two_conv:\n",
    "        conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "        conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    else:\n",
    "        conv1 = conv_layer(x_image, 1, 64, \"conv\")\n",
    "        conv_out = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "\n",
    "\n",
    "    if use_two_fc:\n",
    "        fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "        embedding_input = fc1\n",
    "        embedding_size = 1024\n",
    "        logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "    else:\n",
    "        embedding_input = flattened\n",
    "        embedding_size = 7*7*64\n",
    "        logits = fc_layer(flattened, 7*7*64, 10, \"fc\")\n",
    "\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y), name=\"xent\")\n",
    "        tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(kwargs_tf_simple.log_dir + hparam)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = kwargs_tf_simple.log_dir + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = kwargs_tf_simple.log_dir + 'labels_1024.tsv'\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    pcp3()\n",
    "    \n",
    "    for i in range(1000 + 1):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            \n",
    "#             pcp4()\n",
    "            \n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "            print('Iteration number {}, Accuracy is currently {}'.format(i, train_accuracy))\n",
    "        if i % 500 == 0:\n",
    "            sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "            saver.save(sess, os.path.join(kwargs_tf_simple.log_dir, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv):\n",
    "    conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "    fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "    return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "\n",
    "def main():\n",
    "    \n",
    "#     tf.set_random_seed(seed())\n",
    "    \n",
    "    # You can try adding some more learning rates\n",
    "    for learning_rate in [1E-4]:\n",
    "\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "        for use_two_fc in [False]:\n",
    "            for use_two_conv in [True]:\n",
    "                # Construct a hyperparameter string for each one (example: \"lr_1E-3,fc=2,conv=2)\n",
    "                \n",
    "                pcp2()\n",
    "                \n",
    "                hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv)\n",
    "                print('Starting run for %s' % hparam)\n",
    "\n",
    "                # Actually run with the new settings\n",
    "                mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
