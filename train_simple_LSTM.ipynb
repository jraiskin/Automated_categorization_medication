{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "# from tensorflow.python.framework import ops  # for custom actiavation function definition\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The are 2919 observations\n",
      "Sampling from allowed 82 labels\n",
      "82 labels in the validation set, with\n",
      "1587 potential observation to draw from.\n",
      "365 observations sampled for validation\n",
      "1222 observations for training\n",
      "The ratio of validation to *training* is about 0.299\n"
     ]
    }
   ],
   "source": [
    "kwargs_neural_data_init = \\\n",
    "    {'mk_chars': True, \n",
    "               'model': 'neural', \n",
    "               'char_filter': 100, 'allowed_chars': None, \n",
    "               'mk_ngrams': False, 'ngram_width': 5, \n",
    "               'ngram_filter': 10, 'allowed_ngrams': None, \n",
    "               'keep_infreq_labels': False, 'label_count_thresh': 10, \n",
    "               'valid_ratio': 0.25, \n",
    "               'scale_func': unscale, 'to_permute': True, }\n",
    "\n",
    "x_feed_train, y_feed_train, x_feed_val, y_feed_val,\\\n",
    "    char_int, char_int_inv, label_int, label_int_inv, \\\n",
    "    statistics_dict =\\\n",
    "    data_load_preprocess(**kwargs_neural_data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory was deleted.\n"
     ]
    }
   ],
   "source": [
    "kwargs_simple_rnn = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'top_k': 5, \n",
    "    'seed': seed(), \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1E-2, \n",
    "    'dynamic_learn_rate': False, \n",
    "    'rnn_type': 'GRU',\n",
    "    'bidirection': False, \n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': True,\n",
    "    'hidden_state_size': 32, \n",
    "    'keep_prob': 0.7, \n",
    "    # noisy activation hyper params\n",
    "    'activation_function': 'noisy_tanh',  # tf.tanh / 'noisy_tanh'\n",
    "    'learn_p_delta_scale': False,   # noise scale param in noisy activation\n",
    "    'noise_act_alpha': 1.15,  # mixing in the linear activation\n",
    "    'noise_act_half_normal': False,\n",
    "    # regularization constants\n",
    "    'l2_wieght_reg': 1E-3, \n",
    "    'target_rep': True, \n",
    "    'target_rep_weight': 0.3, \n",
    "    # training settings\n",
    "    'epochs': 100,\n",
    "    'summary_step': 10, \n",
    "    'save_step': np.inf,\n",
    "    'to_save': True, \n",
    "    'verbose_summary': False\n",
    "})\n",
    "\n",
    "if kwargs_simple_rnn.save_step == np.inf and \\\n",
    "    kwargs_simple_rnn.to_save: \n",
    "    kwargs_simple_rnn.save_step = kwargs_simple_rnn.epochs\n",
    "    \n",
    "kwargs_simple_rnn = {**kwargs_simple_rnn, \n",
    "                      **statistics_dict}\n",
    "\n",
    "kwargs_simple_rnn = nice_dict({**kwargs_simple_rnn, \n",
    "                               **{'scale_func': kwargs_neural_data_init['scale_func'], \n",
    "                                  'keep_infreq_labels': kwargs_neural_data_init['keep_infreq_labels']}})\n",
    "\n",
    "if kwargs_simple_rnn.del_log: remove_dir_content(kwargs_simple_rnn.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enable passing some keyword arguments from command line.\n",
    "This does not affect the Jupyter notebook.\n",
    "\"\"\"\n",
    "# try:\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--logdir', action='store', dest='logdir',\n",
    "                    help='Specify the log directory path', \n",
    "                    type=str,\n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--rnn_type', action='store', dest='rnn_type',\n",
    "                    help='Specify RNN type (\"GRU\" or \"LSTM\")', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--bidir', action='store', dest='bidir',\n",
    "                    help='Specify data feed direction '+\\\n",
    "                        '(False for forward-feed or True for bidirectional)', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "#     results = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "if args.logdir is not None and isinstance(args.logdir, str):\n",
    "    kwargs_simple_rnn.log_dir = str(args.logdir)\n",
    "if args.rnn_type is not None and isinstance(args.rnn_type, str):\n",
    "    kwargs_simple_rnn.rnn_type = str(args.rnn_type)\n",
    "if args.bidir is not None and isinstance(args.bidir, str):\n",
    "    kwargs_simple_rnn.bidirection = args.bidir in ['True', 'T']\n",
    "\n",
    "# except:\n",
    "#     pass\n",
    "# print(kwargs_simple_lstm.log_dir, type(kwargs_simple_lstm.log_dir))\n",
    "# print(kwargs_simple_lstm.rnn_type, type(kwargs_simple_lstm.rnn_type))\n",
    "# print(kwargs_simple_lstm.bidirection, type(kwargs_simple_lstm.bidirection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "look_at_some_examples = False\n",
    "\"\"\"\n",
    "Collect examples from training and validation sets,\n",
    "group by label and print two examples for each \n",
    "(e.g. for each label, print 2 training and 2 validation examples).\n",
    "This was done due to a suspicion raised by similar evaulation metrics on the training and test.\n",
    "\"\"\"\n",
    "if look_at_some_examples:\n",
    "    label_to_text_val = {}  # collect validation examples\n",
    "    for obs,label in zip(x_feed_val, y_feed_val):\n",
    "        label_to_text_val.setdefault(label,[]).append(obs)\n",
    "\n",
    "    label_to_text_train = {}  # collect training examples\n",
    "    for obs,label in zip(x_feed_train, y_feed_train):\n",
    "        label_to_text_train.setdefault(label,[]).append(obs)\n",
    "\n",
    "    unique_keys = list(label_to_text_train.keys())\n",
    "    unique_keys.sort()\n",
    "\n",
    "    label_to_text_merge = {}  # collect both\n",
    "    for key in unique_keys:\n",
    "        label_to_text_merge[key] = {'training': label_to_text_train[key], \n",
    "                                    'validation': label_to_text_val[key]}\n",
    "\n",
    "    for key in unique_keys:\n",
    "        cur_dict = label_to_text_merge[key]\n",
    "        print('Key:{}, training:'.format(key))\n",
    "        print(''.join([char for char in cur_dict['training'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['training'][1] if char != '<pad-char>']))\n",
    "        print('validation:')\n",
    "        print(''.join([char for char in cur_dict['validation'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['validation'][1] if char != '<pad-char>']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [[char if i < 1 else '<pad-char>' \n",
    "#   for i, char in enumerate(line)] \n",
    "#  for line in x_feed_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "# training data\n",
    "X_train, _, Y_train = index_transorm_xy(x=x_feed_train, \n",
    "                                        y=y_feed_train, \n",
    "                                        char_int=char_int, \n",
    "                                        label_int=label_int, \n",
    "                                        **kwargs_simple_rnn)\n",
    "\n",
    "# validation data\n",
    "X_val, _, Y_val = index_transorm_xy(x=x_feed_val, \n",
    "                                    y=y_feed_val, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    **kwargs_simple_rnn)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_simple_rnn.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Rnn_model(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 *args, \n",
    "                 hparam_str=None, \n",
    "                 seq_len, \n",
    "                 n_class, \n",
    "                 n_char, \n",
    "                 char_embed_dim, \n",
    "                 one_hot, \n",
    "                 hidden_state_size, \n",
    "                 keep_prob, \n",
    "                 learn_rate, \n",
    "                 dynamic_learn_rate, \n",
    "                 rnn_type, \n",
    "                 bidirection, \n",
    "                 top_k, \n",
    "                 epochs, \n",
    "                 log_dir, \n",
    "                 embed_vis_path=None, \n",
    "                 summary_step, \n",
    "                 save_step, \n",
    "                 seed, \n",
    "                 activation_function, \n",
    "                 learn_p_delta_scale, \n",
    "                 noise_act_alpha, \n",
    "                 noise_act_half_normal, \n",
    "                 l2_wieght_reg, \n",
    "                 target_rep, \n",
    "                 target_rep_weight, \n",
    "                 verbose_summary, \n",
    "                 feed_dict_train=None, \n",
    "                 feed_dict_test=None, \n",
    "                 **kwargs):\n",
    "        \n",
    "        self.hparam_str = hparam_str\n",
    "        self.seq_len = seq_len \n",
    "        self.n_class = n_class \n",
    "        self.n_char = n_char\n",
    "        self.char_embed_dim = char_embed_dim\n",
    "        self.one_hot = one_hot\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.learn_rate = learn_rate\n",
    "        self.dynamic_learn_rate = dynamic_learn_rate\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirection = bidirection\n",
    "        self.top_k = top_k\n",
    "        self.epochs = epochs\n",
    "        self.log_dir = log_dir\n",
    "        self.embed_vis_path = embed_vis_path\n",
    "        self.summary_step = summary_step \n",
    "        self.save_step = save_step\n",
    "        self.seed = seed\n",
    "        self.activation_function = activation_function \n",
    "        self.learn_p_delta_scale = learn_p_delta_scale \n",
    "        self.noise_act_alpha = noise_act_alpha\n",
    "        self.noise_act_half_normal = noise_act_half_normal\n",
    "        self.l2_wieght_reg = l2_wieght_reg\n",
    "        self.target_rep = target_rep\n",
    "        self.verbose_summary = verbose_summary\n",
    "        self.target_rep_weight = target_rep_weight if self.target_rep else 0.0\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "        # clear tf graph and set seeds\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        \n",
    "        # Setup placeholders, and reshape the data\n",
    "        self.x_ = tf.placeholder(tf.int32, [None, self.seq_len], \n",
    "                            name='Examples')\n",
    "        self.y_ = tf.placeholder(tf.int32, [None, self.n_class], \n",
    "                            name='Lables')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], \n",
    "                            name='Keep_probability')\n",
    "        self.use_noise = tf.placeholder(tf.bool, [], \n",
    "                            name='Use_noise')\n",
    "        # indicator that there will be no training\n",
    "        self.no_train = self.hparam_str is None and self.embed_vis_path is None\n",
    "        # set hyper-parameter p,\n",
    "        # as part of noise scaling in the noisy activation function\n",
    "        if self.learn_p_delta_scale and self.activation_function == 'noisy_tanh':\n",
    "#             self.p_delta_scale = tf.Variable(\n",
    "#                 tf.truncated_normal([2 * self.hidden_state_size \n",
    "#                                      if self.bidirection else self.hidden_state_size], \n",
    "#                                     stddev=0.1, \n",
    "#                                     seed=self.seed), \n",
    "            self.p_delta_scale = tf.Variable(\n",
    "                tf.truncated_normal([self.hidden_state_size], \n",
    "                                    stddev=0.1, \n",
    "                                    seed=self.seed), \n",
    "                name='p_delta_scale')\n",
    "        else:\n",
    "            self.p_delta_scale = 1.0\n",
    "        \n",
    "        # set activation function\n",
    "        if hasattr(self.activation_function, '__call__'):\n",
    "            pass\n",
    "        elif self.activation_function == 'noisy_tanh':\n",
    "            self.activation_function = self.noise_tanh_p\n",
    "        else:\n",
    "            raise ValueError('Received an unknown activation function')\n",
    "        \n",
    "        if feed_dict_train is not None:\n",
    "            self.feed_dict_train = {self.x_: feed_dict_train['x'], \n",
    "                                    self.y_: feed_dict_train['y'], \n",
    "                                    self.keep_prob: keep_prob, \n",
    "                                    self.use_noise: True}\n",
    "\n",
    "            self.feed_dict_train_eval = {**self.feed_dict_train, \n",
    "                                         **{self.keep_prob: 1.0, \n",
    "                                            self.use_noise: False}}\n",
    "        \n",
    "        if feed_dict_test is not None:\n",
    "            self.feed_dict_test = {self.x_: feed_dict_test['x'], \n",
    "                                   self.y_: feed_dict_test['y'], \n",
    "                                   self.keep_prob: 1.0, \n",
    "                                   self.use_noise: False}\n",
    "\n",
    "        self.embedding_matrix = self.embed_matrix()\n",
    "\n",
    "        self.outputs = self.rnn_unit(input=self.x_)\n",
    "        with tf.name_scope('logits_seq'):\n",
    "            if self.bidirection: logit_in_size = 2 * self.hidden_state_size\n",
    "            else: logit_in_size = self.hidden_state_size\n",
    "            self.logits = [self.logit(input=out, \n",
    "                                      size_in=logit_in_size, \n",
    "                                      size_out=self.n_class) \n",
    "                           for out in self.outputs]\n",
    "\n",
    "        with tf.name_scope('Cost_function'):\n",
    "            \"\"\"\n",
    "            Cross entropy loss with target replication and\n",
    "            regularization terms based on the weights' L2 norm\n",
    "            \"\"\"\n",
    "            with tf.name_scope('target_replication_loss'):\n",
    "                self.cost_targetrep = tf.reduce_mean(\n",
    "                    [tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=log, labels=self.y_) \n",
    "                     for log in self.logits])\n",
    "            with tf.name_scope('cross_entropy'):\n",
    "                self.cost_crossent = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=self.logits[-1], labels=self.y_))\n",
    "            with tf.name_scope('L2_norm_reg'):\n",
    "                self.cost_l2reg = tf.reduce_mean([tf.nn.l2_loss(weight) \n",
    "                                                  for weight in tf.trainable_variables()])\n",
    "            with tf.name_scope('total_cost'):\n",
    "                self.cost = self.target_rep_weight * self.cost_targetrep + \\\n",
    "                    (1 - self.target_rep_weight) * self.cost_crossent + \\\n",
    "                    self.l2_wieght_reg * self.cost_l2reg\n",
    "            # add summaries\n",
    "            tf.summary.scalar('Total_cost_train', \n",
    "                              self.cost, collections=['train'])\n",
    "            tf.summary.scalar('Total_cost_test', \n",
    "                              self.cost, collections=['test'])\n",
    "            \n",
    "        with tf.name_scope('Cost_function_additional_metrics'):\n",
    "            tf.summary.scalar('Target_rep_cost_train', \n",
    "                              self.cost_targetrep, collections=['train'])\n",
    "            tf.summary.scalar('Target_rep_cost_test', \n",
    "                              self.cost_targetrep, collections=['test'])\n",
    "            tf.summary.scalar('Cross_entropy_train', \n",
    "                              self.cost_crossent, collections=['train'])\n",
    "            tf.summary.scalar('Cross_entropy_test', \n",
    "                              self.cost_crossent, collections=['test'])\n",
    "            tf.summary.scalar('L2_norm_train', \n",
    "                              self.cost_l2reg, collections=['train'])\n",
    "            tf.summary.scalar('L2_norm_test', \n",
    "                              self.cost_l2reg, collections=['test'])            \n",
    "            \n",
    "        with tf.name_scope('Train'):\n",
    "            if self.dynamic_learn_rate:\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(self.learn_rate)\n",
    "            else:\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learn_rate)\n",
    "            self.train_step = self.optimizer.minimize(self.cost)\n",
    "\n",
    "        with tf.name_scope('Accuracy'):  # takes the last element of logits\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.logits[-1], 1), tf.argmax(self.y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('Accuracy_train', self.accuracy, collections=['train'])\n",
    "            tf.summary.scalar('Accuracy_test', self.accuracy, collections=['test'])\n",
    "        \n",
    "        with tf.name_scope('Mean_Reciprocal_Rank'):  # takes the last element of logits\n",
    "            self.recip_rank = tf.reduce_mean(\n",
    "                self.get_reciprocal_rank(self.logits[-1], \n",
    "                                         self.y_, \n",
    "                                         True))\n",
    "            tf.summary.scalar('Mean_Reciprocal_Rank_train', \n",
    "                              self.recip_rank, collections=['train'])\n",
    "            tf.summary.scalar('Mean_Reciprocal_Rank_test', \n",
    "                              self.recip_rank, collections=['test'])        \n",
    "        \n",
    "        with tf.name_scope('In_top_{}'.format(self.top_k)):  # takes the last element of logits\n",
    "            self.y_targets = tf.argmax(self.y_, 1)\n",
    "            self.top_k_res = tf.reduce_mean(tf.cast(\n",
    "                tf.nn.in_top_k(self.logits[-1], self.y_targets, self.top_k), \n",
    "                tf.float32))\n",
    "            tf.summary.scalar('In_top_{}_train'.format(self.top_k), self.top_k_res, collections=['train'])\n",
    "            tf.summary.scalar('In_top_{}_test'.format(self.top_k), self.top_k_res, collections=['test'])\n",
    "\n",
    "        # summaries per collection and saver object\n",
    "        self.summ_train = tf.summary.merge_all('train')\n",
    "        self.summ_test = tf.summary.merge_all('test')\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        # init vars and setup writer\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init_op)\n",
    "        if not self.no_train:\n",
    "            self.writer = tf.summary.FileWriter(self.log_dir + self.hparam_str)\n",
    "            self.writer.add_graph(self.sess.graph)\n",
    "\n",
    "            # Add embedding tensorboard visualization. Need tensorflow version\n",
    "            self.config = projector.ProjectorConfig()\n",
    "            self.embed = self.config.embeddings.add()\n",
    "            self.embed.tensor_name = self.embedding_matrix.name\n",
    "            self.embed.metadata_path = os.path.join(self.embed_vis_path)\n",
    "            projector.visualize_embeddings(self.writer, self.config)\n",
    "        \n",
    "        \n",
    "    def embed_matrix(self, stddev=0.1, name='embeddings'):\n",
    "        # index_size would be the size of the character set\n",
    "        with tf.name_scope(name):\n",
    "            if not self.one_hot:\n",
    "                embedding_matrix = tf.get_variable(\n",
    "                    'embedding_matrix', \n",
    "                    initializer=tf.truncated_normal([self.n_char, self.char_embed_dim], \n",
    "                                                    stddev=stddev, \n",
    "                                                    seed=self.seed), \n",
    "                    trainable=True)\n",
    "            else:\n",
    "                # creating a one-hot for each character corresponds to the identity matrix\n",
    "                embedding_matrix = tf.constant(value=np.identity(self.n_char), \n",
    "                                               name='embedding_matrix', \n",
    "                                               dtype=tf.float32)\n",
    "                self.char_embed_dim = self.n_char\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('embedding_matrix', embedding_matrix, collections=['train'])\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            return self.embedding_matrix\n",
    "        \n",
    "        \n",
    "    def rnn_unit(self, \n",
    "                  input, \n",
    "                  name='LSTM'):\n",
    "        # check, then set the right name\n",
    "        assert self.rnn_type in ['LSTM', 'GRU'], \\\n",
    "            'rnn_type has to be either LSTM or GRU'\n",
    "        name = 'LSTM' if self.rnn_type == 'LSTM' else 'GRU'\n",
    "        if self.bidirection: name += '_bidir'\n",
    "        with tf.name_scope(name):\n",
    "            input = tf.nn.embedding_lookup(self.embedding_matrix, input)\n",
    "            # reshaping\n",
    "            # Permuting batch_size and n_steps\n",
    "            input = tf.transpose(input, [1, 0, 2])\n",
    "            # Reshaping to (n_steps*batch_size, n_input)\n",
    "            input = tf.reshape(input, [-1, self.char_embed_dim])\n",
    "            # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "            rnn_inputs = tf.split(input, self.seq_len, 0)\n",
    "            \n",
    "            # setting the correct RNN cell type (LSTM of GRU)\n",
    "            rnn_cell = rnn.BasicLSTMCell if self.rnn_type == 'LSTM' \\\n",
    "                else rnn.GRUCell\n",
    "            # setting the args (forget_bias applies only to LSTM)\n",
    "            rnn_cell_args = {'num_units': self.hidden_state_size, \n",
    "                             'activation': self.activation_function}\n",
    "            \n",
    "            if 'LSTMCell' in str(rnn_cell.__call__ ):\n",
    "                rnn_cell_args['forget_bias'] = 1.0\n",
    "            rnn_cell(**rnn_cell_args)\n",
    "            \n",
    "            cell_fw = rnn_cell(**rnn_cell_args)\n",
    "            cell_fw = rnn.DropoutWrapper(cell_fw, \n",
    "                                         output_keep_prob=self.keep_prob, \n",
    "                                         seed=self.seed)\n",
    "            \n",
    "            if self.bidirection:\n",
    "                # add another cell for backwards direction and a dropout wrapper\n",
    "                cell_bw = rnn_cell(**rnn_cell_args)\n",
    "                cell_bw = rnn.DropoutWrapper(cell_bw, \n",
    "                                             output_keep_prob=self.keep_prob, \n",
    "                                             seed=self.seed)\n",
    "                outputs, _, _ = rnn.static_bidirectional_rnn(\n",
    "                    cell_fw, cell_bw, rnn_inputs, dtype=tf.float32, scope=name)\n",
    "            else:\n",
    "                outputs, _ = rnn.static_rnn(cell_fw, rnn_inputs, dtype=tf.float32, scope=name)\n",
    "            \n",
    "            if not self.target_rep:  # take only last output (list for structure consistency)\n",
    "                outputs = [outputs[-1]]\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('outputs', outputs, collections=['train'])\n",
    "            return outputs\n",
    "\n",
    "\n",
    "    def logit(self, \n",
    "              input, \n",
    "              size_in, \n",
    "              size_out, \n",
    "              stddev=0.1, \n",
    "              name='logit'):\n",
    "\n",
    "        with tf.name_scope(name):\n",
    "            w = tf.Variable(tf.truncated_normal([size_in, size_out], \n",
    "                                                stddev=stddev, \n",
    "                                                seed=self.seed), \n",
    "                            name='W')\n",
    "            b = tf.Variable(tf.constant(0.1, \n",
    "                                        shape=[size_out]), \n",
    "                            name='B')\n",
    "            logits = tf.matmul(input, w) + b\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('weights', w, collections=['train'])\n",
    "                tf.summary.histogram('biases', b, collections=['train'])\n",
    "                tf.summary.histogram('logits', logits, collections=['train'])\n",
    "            return logits\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        print('Starting to train model {:s}'.format(self.hparam_str))\n",
    "        for i in range(1, self.epochs+1):\n",
    "            # update learning rate, if it is dynamic\n",
    "            if self.dynamic_learn_rate: self.update_lr(epoch=i)\n",
    "            # train step\n",
    "            self.sess.run(self.train_step, feed_dict=self.feed_dict_train)\n",
    "            if i % self.summary_step == 0:\n",
    "                # train summary\n",
    "                # use self.feed_dict_train_eval for evaluation (keep probability set to 1.0)\n",
    "                [train_accuracy, train_cost, _, train_top_k] = \\\n",
    "                    self.run_eval(feed_dict=self.feed_dict_train_eval,\n",
    "                                  step=i, \n",
    "                                  summary=self.summ_train)\n",
    "                \n",
    "#                 [train_accuracy, train_cost,_ , _, _, _, train_top_k, s] = \\\n",
    "#                     self.sess.run([self.accuracy, \n",
    "#                                    self.cost, self.cost_targetrep, self.cost_crossent, self.cost_l2reg, \n",
    "#                                    self.recip_rank, \n",
    "#                                    self.top_k_res, \n",
    "#                                    self.summ_train],\n",
    "#                                   feed_dict=self.feed_dict_train_eval)\n",
    "#                 self.writer.add_summary(s, i)\n",
    "                \n",
    "                print('{:.3f} of observations in the top is {}'.format(train_top_k, self.top_k))\n",
    "                # test summary\n",
    "                [test_accuracy, test_cost, _, test_top_k] = \\\n",
    "                    self.run_eval(feed_dict=self.feed_dict_test,\n",
    "                                  step=i, \n",
    "                                  summary=self.summ_test)\n",
    "                \n",
    "#                 [test_accuracy, test_cost,_ , _, _, _, test_top_k, s] = \\\n",
    "#                     self.sess.run([self.accuracy, \n",
    "#                                    self.cost, self.cost_targetrep, self.cost_crossent, self.cost_l2reg, \n",
    "#                                    self.recip_rank, \n",
    "#                                    self.top_k_res, \n",
    "#                                    self.summ_test],\n",
    "#                                   feed_dict=self.feed_dict_test)\n",
    "#                 self.writer.add_summary(s, i)\n",
    "                \n",
    "                print('Epoch number {}, '.format(i) +\n",
    "                      'training accuracy is {:.5f} and '.format(train_accuracy) + \n",
    "                      'test accuracy is {:.5f}, '.format(test_accuracy))\n",
    "                print('training cost is {:.5f} and '.format(train_cost) + \n",
    "                      'test cost is {:.5f} and '.format(test_cost))\n",
    "                \n",
    "            if i % self.save_step == 0:\n",
    "                print('Saving step {}'.format(i))\n",
    "                self.saver.save(self.sess, os.path.join(self.log_dir, \n",
    "                                                        self.hparam_str, \n",
    "                                                        'model.ckpt'), i)\n",
    "            \n",
    "        print('Training the model is done! ({:s})'.format(self.hparam_str))\n",
    "        \n",
    "\n",
    "    def run_eval(self, *, feed_dict=None, \n",
    "                 step=None, summary=None, session=None):\n",
    "        \"\"\"\n",
    "        Run all evaluation metrics with the given feed_dict.\n",
    "        Use summary (optional) to specify a summary group, \n",
    "        by providing the merged summary object.\n",
    "        session defaults to the internal session (self.sess).\n",
    "        Returns [accuracy, cost, recip_rank, top_k]\n",
    "        \"\"\"\n",
    "        assert not (summary is not None and step is None), \\\n",
    "            'If summary is chosen, a step must be specified.'\n",
    "        \n",
    "        if session is None:\n",
    "            session = self.sess\n",
    "        \n",
    "        if feed_dict is None:\n",
    "            feed_dict = self.feed_dict_test\n",
    "        \n",
    "        if summary is not None:\n",
    "            [accuracy, cost,_ , _, _, recip_rank, top_k, s] = \\\n",
    "                session.run([self.accuracy, \n",
    "                             self.cost, \n",
    "                             self.cost_targetrep, \n",
    "                             self.cost_crossent, \n",
    "                             self.cost_l2reg, \n",
    "                             self.recip_rank, \n",
    "                             self.top_k_res, \n",
    "                             summary],\n",
    "                            feed_dict=feed_dict)\n",
    "            self.writer.add_summary(s, step)\n",
    "        else:\n",
    "            [accuracy, cost,_ , _, _, recip_rank, top_k] = \\\n",
    "                session.run([self.accuracy, \n",
    "                             self.cost, \n",
    "                             self.cost_targetrep, \n",
    "                             self.cost_crossent, \n",
    "                             self.cost_l2reg, \n",
    "                             self.recip_rank, \n",
    "                             self.top_k_res],\n",
    "                            feed_dict=feed_dict)\n",
    "\n",
    "        return [accuracy, cost, recip_rank, top_k]\n",
    "    \n",
    "    \n",
    "    def update_test_dict(self, feed_dict_test):\n",
    "        self.feed_dict_test = {self.x_: feed_dict_test['x'], \n",
    "                               self.y_: feed_dict_test['y'], \n",
    "                               self.keep_prob: 1.0, \n",
    "                               self.use_noise: False}\n",
    "    \n",
    "    \n",
    "    def noise_tanh_p(self,\n",
    "                     x,\n",
    "                     p=None,\n",
    "                     use_noise=None,\n",
    "                     alpha=None,\n",
    "                     c=0.5,\n",
    "    #                  noise=None,\n",
    "                     half_normal=None):\n",
    "        \"\"\"\n",
    "        Noisy Hard Tanh Units: NAN with learning p\n",
    "        https://arxiv.org/abs/1603.00391\n",
    "        Arguments:\n",
    "            x: input tensor variable.\n",
    "            p: tensorflow variable, a vector of parameters for p\n",
    "            use_noise: bool, whether to add noise or not (useful for test time)\n",
    "            c: float, standard deviation of the noise\n",
    "            alpha: float, the leakage rate from the linearized function to the clipped function.\n",
    "            half_normal: bool, whether the noise should be sampled from half-normal or\n",
    "            normal distribution.\n",
    "        \"\"\"\n",
    "        if p is None:\n",
    "            p=self.p_delta_scale\n",
    "        if use_noise is None:\n",
    "            use_noise=self.use_noise\n",
    "        if alpha is None:\n",
    "            alpha = self.noise_act_alpha\n",
    "        if half_normal is None:\n",
    "            half_normal = self.noise_act_half_normal\n",
    "        \n",
    "        signs = tf.sign(x)\n",
    "    #     delta = HardTanh(x) - x\n",
    "        delta = x - hard_tanh(x)\n",
    "\n",
    "        scale = c * (0.5 - tf.sigmoid(p * delta))**2\n",
    "#         scale = c * (0.5 - tf.sigmoid(delta))**2\n",
    "        if alpha > 1.0 and half_normal:\n",
    "               scale *= -1.0\n",
    "\n",
    "        zeros = tf.zeros(tf.shape(x), dtype=tf.float32, name=None)\n",
    "        rn_noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "    #     def noise_func() :return tf.abs(rn_noise) if half_normal else zeros\n",
    "        def noise_func() :return tf.abs(rn_noise) if half_normal else rn_noise\n",
    "        def zero_func (): return zeros + 0.7979 if half_normal else zeros\n",
    "        noise = tf.cond(use_noise,noise_func,zero_func)\n",
    "        \n",
    "        res = alpha * hard_tanh(x) + (1.0 - alpha) * x - signs * scale * noise\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def tf_get_rank_order(self, input, reciprocal):\n",
    "        \"\"\"\n",
    "        Returns a tensor of the rank of the input tensor's elements.\n",
    "        rank(highest element) = 1.\n",
    "        \"\"\"\n",
    "        assert isinstance(reciprocal, bool), 'reciprocal has to be bool'\n",
    "        size = tf.size(input)\n",
    "        indices_of_ranks = tf.nn.top_k(-input, k=size)[1]\n",
    "        indices_of_ranks = size - tf.nn.top_k(-indices_of_ranks, k=size)[1]\n",
    "        if reciprocal:\n",
    "            indices_of_ranks = tf.cast(indices_of_ranks, tf.float32)\n",
    "            indices_of_ranks = tf.map_fn(\n",
    "                lambda x: tf.reciprocal(x), indices_of_ranks, \n",
    "                dtype=tf.float32)\n",
    "            return indices_of_ranks\n",
    "        else:\n",
    "            return indices_of_ranks\n",
    "    \n",
    "    \n",
    "    def get_reciprocal_rank(self, logits, targets, reciprocal=True):\n",
    "        \"\"\"\n",
    "        Returns a tensor containing the (reciprocal) ranks\n",
    "        of the logits tensor (wrt the targets tensor).\n",
    "        The targets tensor should be a 'one hot' vector \n",
    "        (otherwise apply one_hot on targets, such that index_mask is a one_hot).\n",
    "        \"\"\"\n",
    "        function_to_map = lambda x: self.tf_get_rank_order(x, reciprocal=reciprocal)\n",
    "        ordered_array_dtype = tf.float32 if reciprocal is not None else tf.int32\n",
    "        ordered_array = tf.map_fn(function_to_map, logits, \n",
    "                                  dtype=ordered_array_dtype)\n",
    "\n",
    "        size = int(logits.shape[1])\n",
    "        index_mask = tf.reshape(\n",
    "                targets, [-1,size])\n",
    "        if reciprocal:\n",
    "            index_mask = tf.cast(index_mask, tf.float32)\n",
    "\n",
    "        return tf.reduce_sum(ordered_array * index_mask,1)\n",
    "    \n",
    "    \n",
    "    def restore(self, cp_path, feed_dict = None):\n",
    "        \n",
    "        print('Loading variables from {:s}'.format(cp_path))\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(cp_path)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\"no checkpoint found\")\n",
    "\n",
    "#         if feed_dict:\n",
    "#             self.feed(feed_dict=feed_dict)\n",
    "        print('Loading successful')\n",
    "    \n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "    \n",
    "    def update_lr(self, epoch):\n",
    "        self.learn_rate = 1.0 / np.sqrt(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train model GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=1.15,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=32,l2_wieght_reg=1.0E-03,target_rep_weight=0.3/\n",
      "0.259 of observations in the top is 5\n",
      "Epoch number 10, training accuracy is 0.09083 and test accuracy is 0.10137, \n",
      "training cost is 4.15330 and test cost is 4.10491 and \n",
      "0.290 of observations in the top is 5\n",
      "Epoch number 20, training accuracy is 0.09411 and test accuracy is 0.10411, \n",
      "training cost is 3.96682 and test cost is 3.93484 and \n",
      "0.411 of observations in the top is 5\n",
      "Epoch number 30, training accuracy is 0.16039 and test accuracy is 0.17260, \n",
      "training cost is 3.52212 and test cost is 3.52162 and \n",
      "0.475 of observations in the top is 5\n",
      "Epoch number 40, training accuracy is 0.17185 and test accuracy is 0.19178, \n",
      "training cost is 3.27472 and test cost is 3.28067 and \n",
      "0.555 of observations in the top is 5\n",
      "Epoch number 50, training accuracy is 0.20786 and test accuracy is 0.21370, \n",
      "training cost is 3.01536 and test cost is 3.04029 and \n",
      "0.635 of observations in the top is 5\n",
      "Epoch number 60, training accuracy is 0.25696 and test accuracy is 0.26301, \n",
      "training cost is 2.80210 and test cost is 2.85799 and \n",
      "0.753 of observations in the top is 5\n",
      "Epoch number 70, training accuracy is 0.36661 and test accuracy is 0.33425, \n",
      "training cost is 2.47292 and test cost is 2.56105 and \n",
      "0.827 of observations in the top is 5\n",
      "Epoch number 80, training accuracy is 0.42881 and test accuracy is 0.41096, \n",
      "training cost is 2.22163 and test cost is 2.34792 and \n",
      "0.872 of observations in the top is 5\n",
      "Epoch number 90, training accuracy is 0.49427 and test accuracy is 0.46575, \n",
      "training cost is 2.02185 and test cost is 2.19341 and \n",
      "0.913 of observations in the top is 5\n",
      "Epoch number 100, training accuracy is 0.59083 and test accuracy is 0.56438, \n",
      "training cost is 1.77794 and test cost is 1.96033 and \n",
      "Saving step 100\n",
      "Training the model is done! (GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=1.15,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=32,l2_wieght_reg=1.0E-03,target_rep_weight=0.3/)\n"
     ]
    }
   ],
   "source": [
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "hparam_str = make_hparam_string(**kwargs_simple_rnn)\n",
    "\n",
    "rnn_model = Rnn_model(hparam_str=hparam_str, \n",
    "                      embed_vis_path=embed_vis_path, \n",
    "                      feed_dict_train=kwargs_feed_dict_train, \n",
    "                      feed_dict_test=kwargs_feed_dict_test, \n",
    "#                       **{**kwargs_simple_rnn, \n",
    "#                       **{'epochs': 100}}\n",
    "                **kwargs_simple_rnn\n",
    "                      )\n",
    "\n",
    "rnn_model.train()\n",
    "rnn_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load weights from file\n",
    "\n",
    "# hparam_str = make_hparam_string(**kwargs_simple_rnn)\n",
    "\n",
    "rnn_new = Rnn_model(\n",
    "#                   hparam_str=hparam_str, \n",
    "#                 embed_vis_path=embed_vis_path, \n",
    "#                 feed_dict_train=kwargs_feed_dict_train, \n",
    "                feed_dict_test=kwargs_feed_dict_test, \n",
    "                **{**kwargs_simple_rnn, \n",
    "                   **{'epochs': 100}}\n",
    "#                 **kwargs_simple_rnn\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading variables from logdir/GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=1.15,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=32,l2_wieght_reg=1.0E-03,target_rep_weight=0.3/\n",
      "Loading successful\n"
     ]
    }
   ],
   "source": [
    "rnn_new.restore(cp_path=os.path.join(kwargs_simple_rnn.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.56438\n"
     ]
    }
   ],
   "source": [
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.59083\n"
     ]
    }
   ],
   "source": [
    "rnn_new.update_test_dict(kwargs_feed_dict_train)\n",
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "# lstm.p_delta_scale.eval(session=lstm.sess)\n",
    "# lstm.p_delta_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.sess.run(lstm.outputs, feed_dict=lstm.feed_dict_train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "rnn_models = {}\n",
    "for keep_prob in [0.7]:\n",
    "#         for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "    for hidden_state_size in [64, 128]:\n",
    "        for l2_wieght_reg in list(np.logspace(-3, -4, 2)):\n",
    "            for (activation_function,  # all interaction / permutations for noisy activation\n",
    "                 learn_p_delta_scale, \n",
    "                 noise_act_alpha, \n",
    "                 noise_act_half_normal) in [\n",
    "                [tf.tanh, False, 1.15, False]] +\\\n",
    "                [[*j] for j in\\\n",
    "                list(itertools.product(['noisy_tanh'] ,\n",
    "                               [True, False], \n",
    "                               [1.15, 0.9], \n",
    "                               [True, False]))]:\n",
    "                    \n",
    "                # collect new hyperparameters as args\n",
    "                current_kw_simple_rnn = {\n",
    "                    **kwargs_simple_rnn, \n",
    "                    **{'keep_prob': keep_prob, \n",
    "                       'hidden_state_size': hidden_state_size, \n",
    "                       'l2_wieght_reg': l2_wieght_reg, \n",
    "                       'activation_function': activation_function,\n",
    "                       'learn_p_delta_scale': learn_p_delta_scale,\n",
    "                       'noise_act_alpha': noise_act_alpha,\n",
    "                       'noise_act_half_normal': noise_act_half_normal\n",
    "    #                        'bidirection': bidirection, \n",
    "    #                        'target_rep': target_rep\n",
    "                      }}\n",
    "                hparam_str = make_hparam_string(**current_kw_simple_rnn)\n",
    "                print(hparam_str)\n",
    "                var = 'rnn_{}'.format(hparam_str)\n",
    "                rnn_models[var] = rnn_model(feed_dict_train=kwargs_feed_dict_train, \n",
    "                                              feed_dict_test=kwargs_feed_dict_test, \n",
    "                                              hparam_str=hparam_str, \n",
    "                                              embed_vis_path=embed_vis_path, \n",
    "                                              **current_kw_simple_rnn)\n",
    "                rnn_models[var].train()\n",
    "                rnn_models[var].close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting data directly from a tensorboard log dir\n",
    "from tensorflow.python.summary import event_multiplexer\n",
    "# specify path (for parent log dir)\n",
    "log_parent_dir = './logdir_exper_4_3/'\n",
    "ea = event_multiplexer.EventMultiplexer().AddRunsFromDirectory(log_parent_dir)\n",
    "ea.Reload()  # load\n",
    "\n",
    "child_dir = next(os.walk(log_parent_dir))[1]\n",
    "print(ea.Scalars(child_dir[0], 'accuracy/accuracy_test'))  # specify run, scalar_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/39921607/tensorflow-how-to-make-a-custom-activation-function-with-only-python\n",
    "\n",
    "# def custom_tanh(x):  # spiky\n",
    "#     return np.tanh(x)\n",
    "# #     return x ** 2\n",
    "# np_custom_tanh = np.vectorize(custom_tanh)  # np_spiky = np.vectorize(spiky)\n",
    "\n",
    "\n",
    "# def d_custom_tanh(x):  # d_spiky\n",
    "#     return 1 - np.tanh(x) ** 2\n",
    "# #     return 2 * x\n",
    "# np_d_custom_tanh = np.vectorize(d_custom_tanh)  # np_d_spiky = np.vectorize(d_spiky)\n",
    "\n",
    "\n",
    "\n",
    "# def tf_d_custom_tanh(x,name=None, stateful=False):  # tf_d_spiky\n",
    "#     \"\"\"\n",
    "#     Converting a Numpy function to a Tensorflow function.\n",
    "#     tf.py_func acts on lists of tensors and returns a list of tensors.\n",
    "#     stateful, if the same input might produce a different outputs (stochastic).\n",
    "#     \"\"\"\n",
    "#     with ops.name_scope(name, \n",
    "#                         default_name='d_custom_act', \n",
    "#                         values=[x]) as name:\n",
    "#         result = tf.py_func(lambda x: np_d_custom_tanh(x).astype(np.float32),\n",
    "#                         [x],\n",
    "#                         [tf.float32],\n",
    "#                         name=name,\n",
    "#                         stateful=stateful)\n",
    "#         return result[0]\n",
    "\n",
    "\n",
    "# def py_func(func, input, type_out, stateful=True, name=None, grad=None):\n",
    "#     \"\"\"\n",
    "#     Modify the tf.py_func function to make it define the gradient at the same time\n",
    "#     \"\"\"\n",
    "#     # Need to generate a unique name to avoid duplicates:\n",
    "#     rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "\n",
    "#     tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "#     g = tf.get_default_graph()\n",
    "#     with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "#         return tf.py_func(func, input, type_out, stateful=stateful, name=name)\n",
    "\n",
    "\n",
    "# def custom_tanh_grad(op, grad):  # spikygrad\n",
    "#     \"\"\"\n",
    "#     py_func requires a function of a particular form, \n",
    "#     taking an operation and a 'gradient' and returning the computed gradient.\n",
    "#     \"\"\"\n",
    "#     x = op.inputs[0]\n",
    "#     n_gr = tf_d_custom_tanh(x)\n",
    "# #     print('op is:', op)\n",
    "#     print('grad is:', grad)\n",
    "#     print('x is:', x)\n",
    "#     print('n_gr is:', n_gr)\n",
    "#     return grad * n_gr\n",
    "# #     return [g * tf_d_custom_tanh(inp) for g, inp in zip(grad ,x)]\n",
    "\n",
    "\n",
    "# def tf_custom_tanh(x, name=None):  # tf_spiky\n",
    "\n",
    "#     with ops.name_scope(name, \n",
    "#                         default_name='custom_act', \n",
    "#                         values=[x]) as name:\n",
    "#         result = py_func(lambda x: np_custom_tanh(x).astype(np.float32),\n",
    "#                         [x],\n",
    "#                         [tf.float32],\n",
    "#                         name=name,\n",
    "#                         grad=custom_tanh_grad)  # <-- here's the call to the gradient\n",
    "#         return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # based Noisy Activation Functions paper\n",
    "# # https://arxiv.org/abs/1603.00391\n",
    "# # https://github.com/caglar/noisy_units/blob/master/codes/tf/nunits.py\n",
    "\n",
    "# def lin_sigmoid(x):\n",
    "#     \"\"\"\n",
    "#     First-order Taylor expansion around zero of the sigmoid function\n",
    "#     \"\"\"\n",
    "#     return 0.25 * x + 0.5\n",
    "\n",
    "\n",
    "# def hard_sigmoid(x):\n",
    "#     \"\"\"\n",
    "#     Hard saturating sigmoid function, with clipping applied\n",
    "#     \"\"\"\n",
    "#     return tf.minimum(tf.maximum(lin_sigmoid(x), 0.0), 1.0)\n",
    "\n",
    "\n",
    "# def noise_hard_tanh_sat(x, use_noise, stddev=0.25):\n",
    "#     \"\"\"\n",
    "#     Noisy Hard Tanh Units at Saturation: NANIS as proposed in the paper\n",
    "#     https://arxiv.org/abs/1603.00391\n",
    "#     Arguments:\n",
    "#         x: input tensor variable.\n",
    "#         use_noise: bool, whether to add noise or not (useful for test time)\n",
    "#         c: float, standard deviation of the noise\n",
    "#     \"\"\"\n",
    "#     threshold = 1.001  # point where the unit is saturated, in abs terms\n",
    "#     def noise_func() :return tf.random_normal(tf.shape(x), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "#     def zero_func (): return tf.zeros(tf.shape(x), dtype=tf.float32, name=None)\n",
    "#     noise = tf.cond(use_noise,noise_func,zero_func)  # add noise or zeroes\n",
    "    \n",
    "#     test = tf.cast(tf.greater(tf.abs(x) , threshold), tf.float32)\n",
    "#     res = test * hard_tanh(x + stddev * noise) + (1.0 - test) * hard_tanh(x)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
