{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn import svm\n",
    "# from sklearn.metrics import accuracy_score  # gt, pred\n",
    "\n",
    "from utils.utils import user_opt_gen, nice_dict, seed, init_data, pcp1, pcp2, pcp3, pcp4\n",
    "from utils.utils_baseline_svm import filter_dict_by_val_atleast, char_freq_map\n",
    "\n",
    "# from collections import Counter\n",
    "# from math import isnan\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_dir_content(path):\n",
    "    if tf.gfile.Exists(path):\n",
    "        tf.gfile.DeleteRecursively(path)\n",
    "        print('Log directory was deleted.')\n",
    "    else:\n",
    "        print('Log directory was not found.')\n",
    "#         print(path)\n",
    "\n",
    "\n",
    "# pad a list to max_length with the pad_symbol\n",
    "def pad_list(*, input_list, max_length, pad_symbol):\n",
    "    output_list = input_list + [pad_symbol] * (max_length - len(input_list))\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "def index_to_dense(index, length):\n",
    "    output_list = [0.0] * length\n",
    "    output_list[index] = 1.0\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, n, _ = init_data()\n",
    "\n",
    "np.random.seed(seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory was not found.\n"
     ]
    }
   ],
   "source": [
    "kwargs_simple_lstm = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'char_filter': 100, \n",
    "    'n': n,\n",
    "    'batch_size': n, \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1,  # 1E-4\n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': False,\n",
    "    'hidden_state_size': 4, \n",
    "    'activate_bool': True, \n",
    "    'keep_prob': 1.0, \n",
    "    'epochs': 10,\n",
    "    'save_step': 5, \n",
    "    'print_step': 100\n",
    "})\n",
    "\n",
    "if kwargs_simple_lstm.del_log: remove_dir_content(kwargs_simple_lstm.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter by character, appear at least 'char_filter' times in the input\n",
    "filter_keys_chars = list(\n",
    "    filter_dict_by_val_atleast(\n",
    "        input_dict=char_freq_map(input_data=x), \n",
    "        value=kwargs_simple_lstm.char_filter)\n",
    "    .keys())\n",
    "\n",
    "# create a list of character lists\n",
    "x_char = [list(line) for line in x]\n",
    "x_char_filtered = []\n",
    "unknown = '<unk-char>'\n",
    "# replace chars not in 'filter_keys_chars' with 'unknown'\n",
    "for line in x_char:\n",
    "    x_char_filtered.append([char if (char in filter_keys_chars) else unknown for char in line])\n",
    "    \n",
    "# pad lines, so that all lines are same length\n",
    "max_line_len = int(np.max([len(line) for line in x]))\n",
    "pad = '<pad-char>'\n",
    "x_char_filtered_pad = []\n",
    "for i, line in enumerate(x_char_filtered):\n",
    "    x_char_filtered_pad.append(pad_list(input_list=line, \n",
    "                                    max_length=max_line_len, \n",
    "                                    pad_symbol=pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# statistics based on filtered features\n",
    "label_set = y.unique()\n",
    "n_label = len(label_set)\n",
    "\n",
    "# number of unique characters iin input ('x_char_filtered')\n",
    "char_set = set([char for line in x_char_filtered_pad for char in line])\n",
    "n_char = len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs_simple_lstm = nice_dict({**kwargs_simple_lstm, **{\n",
    "    'seq_len': max_line_len,\n",
    "    'n_class': n_label,\n",
    "    'n_char': n_char\n",
    "}\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create lookup dict for characters (and inv)\n",
    "char_int = {}\n",
    "char_int_inv = {}\n",
    "for i, char in enumerate(char_set):\n",
    "    char_int[char] = i\n",
    "    char_int_inv[i] = char\n",
    "\n",
    "# transform x from a list of symbols into a list of ints\n",
    "X = []\n",
    "for line in x_char_filtered_pad:\n",
    "    X.append([char_int[char] for char in line])\n",
    "\n",
    "# same for labels\n",
    "label_int = {}\n",
    "label_int_inv = {}\n",
    "for i, label in enumerate(label_set):\n",
    "    label_int[label] = i\n",
    "    label_int_inv[i] = label\n",
    "# create Y as a list of list(int)\n",
    "Y = [[label_int[label]] for label in y]\n",
    "\n",
    "# transform into format acceptable by tf\n",
    "X = np.array(X)\n",
    "Y_dense = np.array(\n",
    "    [index_to_dense(label[0], \n",
    "                    kwargs_simple_lstm.n_class) for label in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed_matrix(index_size, \n",
    "                 embedding_dim, \n",
    "                 one_hot, \n",
    "                 stddev=0.1, \n",
    "                 seed=seed(), \n",
    "                 name=\"embedding_matrix\"):\n",
    "    # index_size would be the size of the character set\n",
    "        \n",
    "    with tf.name_scope(name):\n",
    "        if not one_hot:\n",
    "            embedding_matrix = tf.get_variable(\n",
    "                'embedding_matrix', \n",
    "                initializer=tf.truncated_normal([index_size, embedding_dim], \n",
    "                                                stddev=stddev, \n",
    "                                                seed=seed), \n",
    "                trainable=True)\n",
    "        else:\n",
    "            # creating a one-hot for each character corresponds to the identity matrix\n",
    "            embedding_matrix = tf.constant(value=np.identity(index_size), \n",
    "                                           name='embedding_matrix', \n",
    "                                           dtype=tf.float32)\n",
    "            \n",
    "        tf.summary.histogram('embedding_matrix', embedding_matrix)\n",
    "        return embedding_matrix\n",
    "\n",
    "\n",
    "def lstm_unit(input, \n",
    "              embeddings, \n",
    "              seq_length, \n",
    "              hidden_state_size, \n",
    "              seed=seed(), \n",
    "              keep_prob=kwargs_simple_lstm.keep_prob, \n",
    "              name='LSTM'):\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        rnn_inputs = [tf.squeeze(i) for i in \n",
    "                      tf.split(tf.nn.embedding_lookup(embeddings, input),\n",
    "                               seq_length, \n",
    "                               1)]\n",
    "\n",
    "        cell = rnn.BasicLSTMCell(num_units=hidden_state_size)\n",
    "        keep_prob = tf.constant(keep_prob)\n",
    "        cell = rnn.DropoutWrapper(cell, \n",
    "                                  output_keep_prob=keep_prob, \n",
    "                                  seed=seed)\n",
    "\n",
    "        outputs, states = rnn.static_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "        outputs = outputs[-1]\n",
    "#         outputs = tf.constant(value=outputs, \n",
    "#                               name='outputs')\n",
    "        tf.summary.histogram('outputs', outputs)\n",
    "        return outputs\n",
    "        \n",
    "\n",
    "def logit(input, \n",
    "          size_in, \n",
    "          size_out, \n",
    "          stddev=0.1, \n",
    "          seed=seed(), \n",
    "          name='logit'):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], \n",
    "                                            stddev=stddev, \n",
    "                                            seed=seed), \n",
    "                       name='W')\n",
    "        b = tf.Variable(tf.constant(0.1, \n",
    "                                    shape=[size_out]), \n",
    "                        name='B')\n",
    "        logits = tf.matmul(input, w) + b\n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        return logits\n",
    "                        \n",
    "\n",
    "def lstm_simple_model(feed_dict, \n",
    "                      hparam_str, \n",
    "                      n, \n",
    "                      seq_len, \n",
    "                      n_class, \n",
    "                      n_char, \n",
    "                      char_embed_dim, \n",
    "                      one_hot, \n",
    "                      hidden_state_size, \n",
    "                      log_dir, *args, **kwargs):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "#     tf.set_random_seed(seed())\n",
    "\n",
    "    # Setup placeholders, and reshape the data\n",
    "    x_ = tf.placeholder(tf.int32, [n, \n",
    "                                   seq_len])\n",
    "    y_ = tf.placeholder(tf.int32, [n, \n",
    "                                   n_class])\n",
    "\n",
    "    embedding_matrix = embed_matrix(index_size=n_char, \n",
    "                                    embedding_dim=char_embed_dim, \n",
    "                                    one_hot=one_hot)\n",
    "    outputs = lstm_unit(input=x_, \n",
    "                        embeddings=embedding_matrix, \n",
    "                        hidden_state_size=hidden_state_size, \n",
    "                        seq_length=seq_len)\n",
    "    \n",
    "    logits = logit(input=outputs, \n",
    "               size_in=hidden_state_size, \n",
    "               size_out=kwargs_simple_lstm.n_class)\n",
    "    \n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y_), name='cross_entropy')\n",
    "    tf.summary.scalar('cross_entropy', cost)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(\n",
    "            kwargs_simple_lstm.learn_rate).minimize(cost)\n",
    "# train_op = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "          \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(log_dir + hparam_str)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    \n",
    "    \n",
    "    feed_dict = {x_: kwargs_feed_dict['x'], \n",
    "                 y_: kwargs_feed_dict['y']}\n",
    "    \n",
    "    for i in range(kwargs_simple_lstm.epochs + 1):\n",
    "#         batch = mnist.train.next_batch(100)\n",
    "        if i % print_step == 0:\n",
    "            \n",
    "#             pcp4()\n",
    "            # minimizing cost, but tracking accuracy\n",
    "            [train_accuracy, train_cost, s] = sess.run([accuracy, cost, summ], feed_dict=feed_dict)\n",
    "            writer.add_summary(s, i)\n",
    "            print('Iteration number {}, '.format(i) +\n",
    "                  'accuracy is {:.5f} and '.format(train_accuracy) + \n",
    "                  'cost is {:.5f}'.format(train_cost))\n",
    "        if i % save_step == 0:\n",
    "#             sess.run(assignment, feed_dict=feed_dict)\n",
    "            saver.save(sess, os.path.join(log_dir, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        \n",
    "    print('Training is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs_feed_dict = {'x': X, 'y': Y_dense}\n",
    "lstm_simple_model(feed_dict=kwargs_feed_dict, \n",
    "                  hparam_str='testrun', \n",
    "                  **kwargs_simple_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "tf.set_random_seed(seed())\n",
    "\n",
    "x_ = tf.placeholder(tf.int32, [kwargs_simple_lstm.n, \n",
    "                               kwargs_simple_lstm.seq_len])\n",
    "y_ = tf.placeholder(tf.int32, [kwargs_simple_lstm.n, \n",
    "                               kwargs_simple_lstm.n_class])\n",
    "\n",
    "embedding_matrix = embed_matrix(index_size=kwargs_simple_lstm.n_char, \n",
    "                                embedding_dim=kwargs_simple_lstm.char_embed_dim, \n",
    "                                one_hot=kwargs_simple_lstm.one_hot)\n",
    "\n",
    "outputs = lstm_unit(input=x_, \n",
    "                    embeddings=embedding_matrix, \n",
    "                    seq_length=kwargs_simple_lstm.seq_len)\n",
    "# rnn_inputs = [tf.squeeze(i) for i in tf.split(tf.nn.embedding_lookup(embedding_matrix, x_),\n",
    "#                                               kwargs_simple_lstm.seq_len, \n",
    "#                                               1)]\n",
    "# # gives: list index out of range\n",
    "# # rnn_inputs = [tf.nn.embedding_lookup(embedding_matrix, x_)]\n",
    "\n",
    "# cell = rnn.BasicLSTMCell(num_units=kwargs_simple_lstm.hidden_state_size)\n",
    "# keep_prob = tf.constant(kwargs_simple_lstm.keep_prob)\n",
    "# cell = rnn.DropoutWrapper(cell, \n",
    "#                           output_keep_prob=keep_prob, \n",
    "#                           seed=seed())\n",
    "\n",
    "# outputs, states = rnn.static_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "# print('outputs are of length {}.'.format(len(outputs)))\n",
    "# outputs = outputs[-1]\n",
    "\n",
    "# W = tf.Variable(tf.truncated_normal([kwargs_simple_lstm.hidden_state_size, \n",
    "#                                      kwargs_simple_lstm.n_class], \n",
    "#                                     stddev=0.1, \n",
    "#                                     seed=seed()))\n",
    "# b = tf.Variable(tf.truncated_normal([kwargs_simple_lstm.n_class], \n",
    "#                                     stddev=0.1, \n",
    "#                                     seed=seed()))\n",
    "\n",
    "# logits = tf.matmul(outputs, W) + b\n",
    "\n",
    "# logits = logit(input=outputs, \n",
    "#                size_in=kwargs_simple_lstm.hidden_state_size, \n",
    "#                size_out=kwargs_simple_lstm.n_class)\n",
    "\n",
    "# cost = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=y_), name=\"xent\")\n",
    "\n",
    "# # train_op = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "# train_op = tf.train.AdamOptimizer(kwargs_simple_lstm.learn_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# init = tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feed dict into the network\n",
    "kwargs_feed_dict = {x_: X, y_: Y_dense}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(kwargs_simple_lstm.epochs):\n",
    "        sess.run(train_op, feed_dict=kwargs_feed_dict)\n",
    "        if i % kwargs_simple_lstm.print_step == 0:\n",
    "            c = sess.run(cost, feed_dict=kwargs_feed_dict)\n",
    "            print('training cost:', c)\n",
    "\n",
    "    response = sess.run(tf.nn.softmax(logits), feed_dict=kwargs_feed_dict)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.trainable_variables())\n",
    "    print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one_hot True\n",
    "# learning rate 1\n",
    "# epochs 1000\n",
    "# training cost: 5.69061\n",
    "# training cost: 5.69047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hold_eighty = kwargs_simple_lstm.seq_len\n",
    "# tf.split(tf.nn.embedding_lookup(embedding_matrix, x_), hold_eighty, 1)\n",
    "# tf.split(tf.nn.embedding_lookup(embedding_matrix, x_), 80, 1)\n",
    "# tf.split(tf.nn.embedding_lookup(embedding_matrix, x_), tf.constant(kwargs_simple_lstm.seq_len), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/wpm/tfrnnlm\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb\n",
    "# http://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define weights\n",
    "with tf.variable_scope('softmax'):\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "EPOCHS = 10000\n",
    "PRINT_STEP = 1000\n",
    "\n",
    "data = np.array([[1, 2, 3, 4, 5], \n",
    "                 [ 2, 3, 4, 5, 6], \n",
    "                 [3, 4, 5, 6, 7], \n",
    "                [1, 2, 2, 4, 5]])\n",
    "\n",
    "# making sure that embedding index starts from 0 (to match embeddings index range)\n",
    "new_data = []\n",
    "for line in data:\n",
    "    new_data.append([el - 1 for el in line])\n",
    "data = np.array(new_data)\n",
    "data\n",
    "\n",
    "target = np.array([[0], [1], [2], [0]])\n",
    "\n",
    "embed_dim = 4\n",
    "seq_len = data.shape[1]\n",
    "n = data.shape[0]\n",
    "n_labels = 3\n",
    "\n",
    "target_dense = np.array(\n",
    "    [index_to_dense(label[0], n_labels) for label in target])\n",
    "\n",
    "x_ = tf.placeholder(tf.int32, [n, seq_len])\n",
    "y_ = tf.placeholder(tf.int32, [n, n_labels])\n",
    "\n",
    "embedding_matrix = tf.Variable(\n",
    "    tf.truncated_normal([7, embed_dim], \n",
    "                        stddev=0.1, \n",
    "                        seed=seed()))\n",
    "\n",
    "rnn_inputs = [tf.squeeze(i) for i in tf.split(tf.nn.embedding_lookup(embedding_matrix, x_),\n",
    "                                              seq_len, \n",
    "                                              1)]\n",
    "cell = rnn.BasicLSTMCell(num_units=embed_dim)\n",
    "cell = rnn.DropoutWrapper(cell, \n",
    "                          output_keep_prob=kwargs_simple_lstm.keep_prob)\n",
    "\n",
    "outputs, states = rnn.static_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "# print(len(outputs))\n",
    "outputs = outputs[-1]\n",
    "# print(outputs.shape)\n",
    "W = tf.Variable(tf.random_normal([embed_dim, n_labels]))     \n",
    "b = tf.Variable(tf.random_normal([n_labels]))\n",
    "\n",
    "y = tf.matmul(outputs, W) + b\n",
    "# print(y.shape)\n",
    "# print(y)\n",
    "# cost = tf.reduce_mean(tf.square(y - y_))\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=y, labels=y_), name=\"xent\")\n",
    "\n",
    "# train_op = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "\n",
    "# init = tf.initialize_all_variables().run()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(EPOCHS):\n",
    "        sess.run(train_op, feed_dict={x_:data, y_:target_dense})\n",
    "        if i % PRINT_STEP == 0:\n",
    "            c = sess.run(cost, feed_dict={x_:data, y_:target_dense})\n",
    "            print('training cost:', c)\n",
    "\n",
    "    response = sess.run(tf.nn.softmax(y), feed_dict={x_:data})\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training cost: 1.57332\n",
    "# training cost: 0.873486\n",
    "# training cost: 0.530809\n",
    "# training cost: 0.346015\n",
    "# training cost: 0.205075\n",
    "# training cost: 0.110332\n",
    "# training cost: 0.0573604\n",
    "# training cost: 0.0307796\n",
    "# training cost: 0.0176844\n",
    "# training cost: 0.0107775\n",
    "# [[  9.95861769e-01   4.09586774e-03   4.23801393e-05]\n",
    "#  [  5.38118649e-03   9.88556504e-01   6.06230181e-03]\n",
    "#  [  1.61634802e-04   8.70981626e-03   9.91128564e-01]\n",
    "#  [  9.97194529e-01   2.77369726e-03   3.17150661e-05]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a simple LSTM architecture\n",
    "\n",
    "# sess = tf.Session()\n",
    "\n",
    "# tf.set_random_seed(seed())\n",
    "\n",
    "def build_graph(*, \n",
    "                vocab_size,\n",
    "                state_size,\n",
    "                batch_size,\n",
    "                num_classes,\n",
    "                keep_prob, \n",
    "                embedding_matrix):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.constant(keep_prob)\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "#     cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(state_size, forget_bias=1.0)\n",
    "    \n",
    "#     init_state = tf.get_variable('init_state', [1, state_size],\n",
    "#                                  initializer=tf.constant_initializer(0.0))\n",
    "#     init_state = tf.tile(init_state, [batch_size, 1])\n",
    "#     rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "#                                                  initial_state=init_state)\n",
    "    outputs, states = rnn.static_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    cell_outputs = tf.nn.dropout(outputs, keep_prob)\n",
    "\n",
    "    \"\"\"\n",
    "    Obtain the last relevant output. The best approach in the future will be to use:\n",
    "\n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    which is the Tensorflow equivalent of numpy's rnn_outputs[range(30), seqlen-1, :], but the\n",
    "    gradient for this op has not been implemented as of this writing.\n",
    "\n",
    "    The below solution works, but throws a UserWarning re: the gradient.\n",
    "    \"\"\"\n",
    "#     idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "#     last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(cell_outputs[-1], W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    \n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# activate_bool = True\n",
    "# if activate_bool:\n",
    "#     # vectorizer transforms dict into sparse matrix\n",
    "#     v = DictVectorizer(sparse=True)\n",
    "\n",
    "#     # create a sparse X matrix with character and n-grams features\n",
    "#     X = v.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lstm_unit(*, \n",
    "              input, \n",
    "              size_in, \n",
    "              size_out, \n",
    "              name=\"LSTM\", \n",
    "              activate = tf.nn.relu, \n",
    "              activate_bool = True,\n",
    "              n_input, \n",
    "#               n_steps, \n",
    "              n_hidden):\n",
    "    # activate can be (commonly):\n",
    "    # tf.sigmoid\n",
    "    # tf.nn.relu\n",
    "    # tf.tanh\n",
    "    # tf.nn.relu6\n",
    "    assert activate in [tf.nn.relu, \n",
    "                        tf.nn.relu6, \n",
    "                        tf.sigmoid, \n",
    "                        tf.tanh], 'Please choose activation function from the given set'\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        \n",
    "#         # Permuting batch_size and n_steps\n",
    "#         x = tf.transpose(input, [1, 0, 2])\n",
    "#         # Reshape to (n_steps*batch_size, n_input)\n",
    "#         x = tf.reshape(x, [-1, n_input])\n",
    "#         # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "#         x = tf.split(0, n_steps, x)\n",
    "        \n",
    "#         lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        lstm_fw_cell = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        \n",
    "        output, state = lstm_fw_cell(input, state, dtype=tf.float32)        \n",
    "        \n",
    "        lin_activations = tf.matmul(output[-1], w) + b\n",
    "        \n",
    "        # apply activation if 'activate_bool'\n",
    "        if activate_bool:\n",
    "            act = activate(lin_activations)\n",
    "        else:\n",
    "            act = lin_activations\n",
    "        \n",
    "#         act = activate(tf.matmul(input, w) + b)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "    \n",
    "def make_hparam_string(learning_rate, fc, conv):\n",
    "    conv_param = 'conv={:d}'.format(conv)\n",
    "    fc_param = 'conv={:d}'.format(fc)\n",
    "#     return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "    return 'lr_{:.0E},{},{}'.format(learning_rate, conv_param, fc_param)\n",
    "\n",
    "\n",
    "def simple_lstm_model(*, \n",
    "                      input_x, \n",
    "                      input_y, \n",
    "                      learn_rate, \n",
    "                      n, \n",
    "                      class_dim, \n",
    "                      char_embed_dim, \n",
    "                      n_char,\n",
    "                      keep_prob,\n",
    "                      iterations, \n",
    "                      hparam_str):\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    \n",
    "    tf.set_random_seed(seed())\n",
    "    \n",
    "#     x = tf.placeholder(tf.float32, shape=[None, char_embed_dim], name=\"x\")\n",
    "#     x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "#     tf.summary.image('input', x_image, 3)\n",
    "#     y = tf.placeholder(tf.float32, shape=[None, class_dim], name=\"labels\")\n",
    "    \n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None], name='x') # [batch_size, num_steps]\n",
    "    seq_len = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size], name='labels')\n",
    "    keep_prob = tf.constant(keep_prob)\n",
    "    \n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [n_char, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)    \n",
    "    \n",
    "    \n",
    "    logits = lstm_unit(x)\n",
    "\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y), name=\"xent\")\n",
    "    tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for i in range(iterations + 1):\n",
    "        if i % 5 == 0:  \n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: input_x, y: input_y})\n",
    "            writer.add_summary(s, i)\n",
    "            print('Iteration number {}, Accuracy is currently {}'.format(i, train_accuracy))\n",
    "        if i % 500 == 0:\n",
    "            saver.save(sess, os.path.join(kwargs_tf_simple.log_dir, \"model.ckpt\"), i)\n",
    "        \n",
    "        sess.run(train_step, feed_dict={x: input_x, y: input_y})\n",
    "        \n",
    "    \n",
    "# https://www.tensorflow.org/tutorials/recurrent\n",
    "# https://www.tensorflow.org/programmers_guide/reading_data#preloaded_data\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup\n",
    "# https://www.tensorflow.org/api_guides/python/nn#Embeddings\n",
    "# https://github.com/dhwajraj/deep-siamese-text-similarity/blob/master/siamese_network.py\n",
    "\n",
    "# source code, from lines 125, 141\n",
    "# https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_lstm_model(input_x, \n",
    "                  input_y, \n",
    "                  learn_rate, \n",
    "                  n, \n",
    "                  class_dim, \n",
    "                  char_embed_dim, \n",
    "                  iterations, \n",
    "                  hparam_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kwargs_tf_simple.log_dir\n",
    "# os.path.join(os.path.curdir + '/logdir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=kwargs_tf_simple.log_dir + 'data', one_hot=True)\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'labels_1024.tsv', kwargs_tf_simple.log_dir + 'labels_1024.tsv')\n",
    "urllib.request.urlretrieve(kwargs_tf_simple.GIST_URL + 'sprite_1024.png', kwargs_tf_simple.log_dir + 'sprite_1024.png')\n",
    "\n",
    "pcp1()\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "def mnist_model(learning_rate, use_two_conv, use_two_fc, hparam):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    tf.set_random_seed(seed())\n",
    "\n",
    "    # Setup placeholders, and reshape the data\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 3)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "    if use_two_conv:\n",
    "        conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "        conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    else:\n",
    "        conv1 = conv_layer(x_image, 1, 64, \"conv\")\n",
    "        conv_out = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "\n",
    "\n",
    "    if use_two_fc:\n",
    "        fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "        embedding_input = fc1\n",
    "        embedding_size = 1024\n",
    "        logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "    else:\n",
    "        embedding_input = flattened\n",
    "        embedding_size = 7*7*64\n",
    "        logits = fc_layer(flattened, 7*7*64, 10, \"fc\")\n",
    "\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y), name=\"xent\")\n",
    "    tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(kwargs_tf_simple.log_dir + hparam)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = kwargs_tf_simple.log_dir + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = kwargs_tf_simple.log_dir + 'labels_1024.tsv'\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    pcp3()\n",
    "    \n",
    "    for i in range(1000 + 1):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            \n",
    "#             pcp4()\n",
    "            \n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "            print('Iteration number {}, Accuracy is currently {}'.format(i, train_accuracy))\n",
    "        if i % 500 == 0:\n",
    "            sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "            saver.save(sess, os.path.join(kwargs_tf_simple.log_dir, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv):\n",
    "    conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "    fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "    return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "\n",
    "def main():\n",
    "    \n",
    "#     tf.set_random_seed(seed())\n",
    "    \n",
    "    # You can try adding some more learning rates\n",
    "    for learning_rate in [1E-4]:\n",
    "\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "        for use_two_fc in [False]:\n",
    "            for use_two_conv in [True]:\n",
    "                # Construct a hyperparameter string for each one (example: \"lr_1E-3,fc=2,conv=2)\n",
    "                \n",
    "                pcp2()\n",
    "                \n",
    "                hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv)\n",
    "                print('Starting run for %s' % hparam)\n",
    "\n",
    "                # Actually run with the new settings\n",
    "                mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iteration number 925, Accuracy is currently 0.5\n",
    "# Iteration number 930, Accuracy is currently 0.6000000238418579\n",
    "# Iteration number 935, Accuracy is currently 0.5699999928474426\n",
    "# Iteration number 940, Accuracy is currently 0.550000011920929\n",
    "# Iteration number 945, Accuracy is currently 0.5400000214576721\n",
    "# Iteration number 950, Accuracy is currently 0.5799999833106995\n",
    "# Iteration number 955, Accuracy is currently 0.6200000047683716\n",
    "# Iteration number 960, Accuracy is currently 0.5199999809265137\n",
    "# Iteration number 965, Accuracy is currently 0.5400000214576721\n",
    "# Iteration number 970, Accuracy is currently 0.6700000166893005\n",
    "# Iteration number 975, Accuracy is currently 0.5899999737739563\n",
    "# Iteration number 980, Accuracy is currently 0.46000000834465027\n",
    "# Iteration number 985, Accuracy is currently 0.5699999928474426\n",
    "# Iteration number 990, Accuracy is currently 0.5899999737739563\n",
    "# Iteration number 995, Accuracy is currently 0.550000011920929\n",
    "# Iteration number 1000, Accuracy is currently 0.5799999833106995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "target_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "EPOCHS = 10\n",
    "PRINT_STEP = 1\n",
    "\n",
    "data = np.array([[1, 2, 3, 4, 5], [ 2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\n",
    "target = np.array([[6], [7], [8]])\n",
    "\n",
    "x_ = tf.placeholder(tf.float32, [None, data.shape[1]])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "\n",
    "\n",
    "cell = rnn.BasicLSTMCell(num_units=data.shape[1])\n",
    "\n",
    "outputs, states = rnn.static_rnn(cell, [x_], dtype=tf.float32)\n",
    "outputs = outputs[-1]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([data.shape[1], 1]))     \n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "y = tf.matmul(outputs, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(y - y_))\n",
    "train_op = tf.train.RMSPropOptimizer(0.005, 0.2).minimize(cost)\n",
    "\n",
    "# init = tf.initialize_all_variables().run()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(EPOCHS):\n",
    "        sess.run(train_op, feed_dict={x_:data, y_:target})\n",
    "        if i % PRINT_STEP == 0:\n",
    "            c = sess.run(cost, feed_dict={x_:data, y_:target})\n",
    "            print('training cost:', c)\n",
    "\n",
    "    response = sess.run(y, feed_dict={x_:data})\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
