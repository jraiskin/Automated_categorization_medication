{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "from tensorflow.python.framework import ops  # for custom actiavation function definition\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# activation function\n",
    "def custom_tanh(x):  # spiky\n",
    "    return np.tanh(x)\n",
    "np_custom_tanh = np.vectorize(custom_tanh)  # np_spiky\n",
    "\n",
    "\n",
    "# derivative of the activation function\n",
    "def d_custom_tanh(x):  # d_spiky\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "np_d_custom_tanh = np.vectorize(d_custom_tanh)  # np_d_spiky = np.vectorize(d_spiky)\n",
    "\n",
    "\n",
    "def tf_d_custom_tanh(x, name=None, stateful=False):  # tf_d_spiky\n",
    "    \"\"\"\n",
    "    Converting a Numpy function to a Tensorflow function.\n",
    "    tf.py_func acts on lists of tensors and returns a list of tensors.\n",
    "    stateful, if the same input might produce a different outputs (stochastic).\n",
    "    \"\"\"\n",
    "    with ops.name_scope(name, \n",
    "                        default_name='d_custom_act', \n",
    "                        values=[x]) as name:\n",
    "        # np operates on float64 while tf operates on float32\n",
    "        result = tf.py_func(lambda x: np_d_custom_tanh(x).astype(np.float32),\n",
    "                            [x],\n",
    "                            [tf.float32],\n",
    "                            name=name,\n",
    "                            stateful=stateful)\n",
    "        return result[0]\n",
    "\n",
    "\n",
    "def py_func(func, input, type_out, stateful=True, name=None, grad=None):\n",
    "    \"\"\"\n",
    "    Modify the tf.py_func function to make it define the gradient at the same time\n",
    "    \"\"\"\n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "\n",
    "    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'PyFunc': rnd_name}):\n",
    "        return tf.py_func(func, input, type_out, stateful=stateful, name=name)\n",
    "\n",
    "\n",
    "def custom_tanh_grad(op, grad):\n",
    "    \"\"\"\n",
    "    py_func requires a function of a particular form, \n",
    "    taking an operation and a 'gradient' and returning the computed gradient.\n",
    "    \"\"\"\n",
    "    x = op.inputs[0]\n",
    "    n_gr = tf_d_custom_tanh(x)\n",
    "    return grad * n_gr\n",
    "\n",
    "\n",
    "def tf_custom_tanh(x, name=None):\n",
    "\n",
    "    with ops.name_scope(name, \n",
    "                        default_name='custom_act', \n",
    "                        values=[x]) as name:\n",
    "        result = py_func(lambda x: np_custom_tanh(x).astype(np.float32),\n",
    "                         [x],\n",
    "                         [tf.float32],\n",
    "                         name=name,\n",
    "                         grad=custom_tanh_grad)  # gradient call\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # tf.reset_default_graph()\n",
    "# with tf.Session() as sess:\n",
    "\n",
    "#     x = tf.constant([0.2,0.7,1.2,1.7])\n",
    "#     y = tf_custom_tanh(x)\n",
    "#     tf.global_variables_initializer().run()\n",
    "\n",
    "#     print(x.eval(), y.eval(), tf.gradients(y, [x])[0].eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The are 2919 observations\n",
      "Sampling from allowed 82 labels\n",
      "82 labels in the validation set, with\n",
      "1587 potential observation to draw from.\n",
      "365 observations sampled for validation\n",
      "1222 observations for training\n",
      "The ratio of validation to *training* is about 0.299\n"
     ]
    }
   ],
   "source": [
    "kwargs_neural_data_init = \\\n",
    "    {'mk_chars': True, \n",
    "               'model': 'neural', \n",
    "               'char_filter': 100, 'allowed_chars': None, \n",
    "               'mk_ngrams': False, 'ngram_width': 5, \n",
    "               'ngram_filter': 10, 'allowed_ngrams': None, \n",
    "               'keep_infreq_labels': False, 'label_count_thresh': 10, \n",
    "               'valid_ratio': 0.25, \n",
    "               'scale_func': unscale, 'to_permute': True, }\n",
    "\n",
    "x_feed_train, y_feed_train, x_feed_val, y_feed_val,\\\n",
    "    char_int, char_int_inv, label_int, label_int_inv, \\\n",
    "    statistics_dict =\\\n",
    "    data_load_preprocess(**kwargs_neural_data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory was deleted.\n"
     ]
    }
   ],
   "source": [
    "kwargs_simple_lstm = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'top_k': 5, \n",
    "    'seed': seed(), \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1E-2, \n",
    "    'dynamic_learn_rate': False, \n",
    "    'rnn_type': 'GRU',\n",
    "    'bidirection': False, \n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': True,\n",
    "    'hidden_state_size': 32, \n",
    "    'keep_prob': 0.7, \n",
    "    'l2_wieght_reg': 1E-3, \n",
    "    'target_rep': True, \n",
    "    'target_rep_weight': 0.1, \n",
    "    'epochs': 100,\n",
    "    'summary_step': 10, \n",
    "    'save_step': np.inf,\n",
    "    'to_save': False, \n",
    "    'verbose_summary': False\n",
    "})\n",
    "\n",
    "if kwargs_simple_lstm.save_step == np.inf and \\\n",
    "    kwargs_simple_lstm.to_save: \n",
    "    kwargs_simple_lstm.save_step = kwargs_simple_lstm.epochs\n",
    "    \n",
    "kwargs_simple_lstm = {**kwargs_simple_lstm, \n",
    "                      **statistics_dict}\n",
    "\n",
    "kwargs_simple_lstm = nice_dict({**kwargs_simple_lstm, \n",
    "                                **{'scale_func': kwargs_neural_data_init['scale_func'], \n",
    "                                   'keep_infreq_labels': kwargs_neural_data_init['keep_infreq_labels']}})\n",
    "\n",
    "if kwargs_simple_lstm.del_log: remove_dir_content(kwargs_simple_lstm.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "look_at_some_examples = False\n",
    "\"\"\"\n",
    "Collect examples from training and validation sets,\n",
    "group by label and print two examples for each \n",
    "(e.g. for each label, print 2 training and 2 validation examples).\n",
    "This was done due to a suspicion raised by similar evaulation metrics on the training and test.\n",
    "\"\"\"\n",
    "if look_at_some_examples:\n",
    "    label_to_text_val = {}  # collect validation examples\n",
    "    for obs,label in zip(x_feed_val, y_feed_val):\n",
    "        label_to_text_val.setdefault(label,[]).append(obs)\n",
    "\n",
    "    label_to_text_train = {}  # collect training examples\n",
    "    for obs,label in zip(x_feed_train, y_feed_train):\n",
    "        label_to_text_train.setdefault(label,[]).append(obs)\n",
    "\n",
    "    unique_keys = list(label_to_text_train.keys())\n",
    "    unique_keys.sort()\n",
    "\n",
    "    label_to_text_merge = {}  # collect both\n",
    "    for key in unique_keys:\n",
    "        label_to_text_merge[key] = {'training': label_to_text_train[key], \n",
    "                                    'validation': label_to_text_val[key]}\n",
    "\n",
    "    for key in unique_keys:\n",
    "        cur_dict = label_to_text_merge[key]\n",
    "        print('Key:{}, training:'.format(key))\n",
    "        print(''.join([char for char in cur_dict['training'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['training'][1] if char != '<pad-char>']))\n",
    "        print('validation:')\n",
    "        print(''.join([char for char in cur_dict['validation'][0] if char != '<pad-char>']))\n",
    "        print(''.join([char for char in cur_dict['validation'][1] if char != '<pad-char>']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "# training data\n",
    "X_train, _, Y_train = index_transorm_xy(x=x_feed_train, \n",
    "                                        y=y_feed_train, \n",
    "                                        char_int=char_int, \n",
    "                                        label_int=label_int, \n",
    "                                        **kwargs_simple_lstm)\n",
    "\n",
    "# validation data\n",
    "X_val, _, Y_val = index_transorm_xy(x=x_feed_val, \n",
    "                                    y=y_feed_val, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    **kwargs_simple_lstm)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_simple_lstm.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Lstm_model(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 *args, \n",
    "                 hparam_str, \n",
    "                 seq_len, \n",
    "                 n_class, \n",
    "                 n_char, \n",
    "                 char_embed_dim, \n",
    "                 one_hot, \n",
    "                 hidden_state_size, \n",
    "                 keep_prob, \n",
    "                 learn_rate, \n",
    "                 dynamic_learn_rate, \n",
    "                 rnn_type, \n",
    "                 bidirection, \n",
    "                 top_k, \n",
    "                 epochs, \n",
    "                 log_dir, \n",
    "                 embed_vis_path, \n",
    "                 summary_step, \n",
    "                 save_step, \n",
    "                 seed, \n",
    "                 l2_wieght_reg, \n",
    "                 target_rep, \n",
    "                 target_rep_weight, \n",
    "                 verbose_summary, \n",
    "                 feed_dict_train, \n",
    "                 feed_dict_test, \n",
    "                 **kwargs):\n",
    "        \n",
    "        self.hparam_str = hparam_str\n",
    "        self.seq_len = seq_len \n",
    "        self.n_class = n_class \n",
    "        self.n_char = n_char\n",
    "        self.char_embed_dim = char_embed_dim\n",
    "        self.one_hot = one_hot\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.learn_rate = learn_rate\n",
    "        self.dynamic_learn_rate = dynamic_learn_rate\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirection = bidirection\n",
    "        self.top_k = top_k\n",
    "        self.epochs = epochs\n",
    "        self.log_dir = log_dir\n",
    "        self.embed_vis_path = embed_vis_path\n",
    "        self.summary_step = summary_step \n",
    "        self.save_step = save_step\n",
    "        self.seed = seed\n",
    "        self.l2_wieght_reg = l2_wieght_reg\n",
    "        self.target_rep = target_rep\n",
    "        self.verbose_summary = verbose_summary\n",
    "        self.target_rep_weight = target_rep_weight if self.target_rep else 0.0\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "        # clear tf graph and set seeds\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        \n",
    "        # Setup placeholders, and reshape the data\n",
    "        self.x_ = tf.placeholder(tf.int32, [None, self.seq_len], \n",
    "                            name='Examples')\n",
    "        self.y_ = tf.placeholder(tf.int32, [None, self.n_class], \n",
    "                            name='Lables')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], \n",
    "                            name='Keep_probability')\n",
    "\n",
    "        self.feed_dict_train = {self.x_: feed_dict_train['x'], \n",
    "                                self.y_: feed_dict_train['y'], \n",
    "                                self.keep_prob: keep_prob}\n",
    "\n",
    "        self.feed_dict_train_eval = {**self.feed_dict_train, \n",
    "                                     **{self.keep_prob: 1.0}}\n",
    "\n",
    "        self.feed_dict_test = {self.x_: feed_dict_test['x'], \n",
    "                               self.y_: feed_dict_test['y'], \n",
    "                               self.keep_prob: 1.0}\n",
    "\n",
    "        self.embedding_matrix = self.embed_matrix()\n",
    "\n",
    "        self.outputs = self.lstm_unit(input=self.x_)\n",
    "        with tf.name_scope('logits_seq'):\n",
    "            if self.bidirection: logit_in_size = 2 * self.hidden_state_size\n",
    "            else: logit_in_size = self.hidden_state_size\n",
    "            self.logits = [self.logit(input=out, \n",
    "                                      size_in=logit_in_size, \n",
    "                                      size_out=self.n_class) \n",
    "                           for out in self.outputs]\n",
    "\n",
    "        with tf.name_scope('Cost_function'):\n",
    "            # cross entropy loss with target replication and\n",
    "            # regularization terms based on the weights' L2 norm\n",
    "            with tf.name_scope('target_replication_loss'):\n",
    "                self.cost_targetrep = tf.reduce_mean(\n",
    "                    [tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=log, labels=self.y_) \n",
    "                     for log in self.logits])\n",
    "            with tf.name_scope('cross_entropy'):\n",
    "                self.cost_crossent = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=self.logits[-1], labels=self.y_))\n",
    "            with tf.name_scope('L2_norm_reg'):\n",
    "                self.cost_l2reg = tf.reduce_mean([tf.nn.l2_loss(weight) \n",
    "                                                  for weight in tf.trainable_variables()])\n",
    "            with tf.name_scope('total_cost'):\n",
    "                self.cost = self.target_rep_weight * self.cost_targetrep + \\\n",
    "                    (1 - self.target_rep_weight) * self.cost_crossent + \\\n",
    "                    self.l2_wieght_reg * self.cost_l2reg\n",
    "            # add summaries\n",
    "            tf.summary.scalar('Total_cost_train', \n",
    "                              self.cost, collections=['train'])\n",
    "            tf.summary.scalar('Total_cost_test', \n",
    "                              self.cost, collections=['test'])\n",
    "            \n",
    "        with tf.name_scope('Cost_function_additional_metrics'):\n",
    "            tf.summary.scalar('Target_rep_cost_train', \n",
    "                              self.cost_targetrep, collections=['train'])\n",
    "            tf.summary.scalar('Target_rep_cost_test', \n",
    "                              self.cost_targetrep, collections=['test'])\n",
    "            tf.summary.scalar('Cross_entropy_train', \n",
    "                              self.cost_crossent, collections=['train'])\n",
    "            tf.summary.scalar('Cross_entropy_test', \n",
    "                              self.cost_crossent, collections=['test'])\n",
    "            tf.summary.scalar('L2_norm_train', \n",
    "                              self.cost_l2reg, collections=['train'])\n",
    "            tf.summary.scalar('L2_norm_test', \n",
    "                              self.cost_l2reg, collections=['test'])            \n",
    "            \n",
    "        with tf.name_scope('Train'):\n",
    "            if self.dynamic_learn_rate:\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(self.learn_rate)\n",
    "            else:\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learn_rate)\n",
    "            self.train_step = self.optimizer.minimize(self.cost)\n",
    "\n",
    "        with tf.name_scope('Accuracy'):  # takes the last element of logits\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.logits[-1], 1), tf.argmax(self.y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('Accuracy_train', self.accuracy, collections=['train'])\n",
    "            tf.summary.scalar('Accuracy_test', self.accuracy, collections=['test'])\n",
    "        \n",
    "        with tf.name_scope('Mean_Reciprocal_Rank'):  # takes the last element of logits\n",
    "            self.recip_rank = tf.reduce_mean(\n",
    "                self.get_reciprocal_rank(self.logits[-1], \n",
    "                                         self.y_, \n",
    "                                         True))\n",
    "            tf.summary.scalar('Mean_Reciprocal_Rank_train', \n",
    "                              self.recip_rank, collections=['train'])\n",
    "            tf.summary.scalar('Mean_Reciprocal_Rank_test', \n",
    "                              self.recip_rank, collections=['test'])        \n",
    "        \n",
    "        with tf.name_scope('In_top_{}'.format(self.top_k)):  # takes the last element of logits\n",
    "            self.y_targets = tf.argmax(self.y_, 1)\n",
    "            self.top_k_res = tf.reduce_mean(tf.cast(\n",
    "                tf.nn.in_top_k(self.logits[-1], self.y_targets, self.top_k), \n",
    "                tf.float32))\n",
    "            tf.summary.scalar('In_top_{}_train'.format(self.top_k), self.top_k_res, collections=['train'])\n",
    "            tf.summary.scalar('In_top_{}_test'.format(self.top_k), self.top_k_res, collections=['test'])\n",
    "\n",
    "        # summaries per collection and saver object\n",
    "        self.summ_train = tf.summary.merge_all('train')\n",
    "        self.summ_test = tf.summary.merge_all('test')\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        # init vars and setup writer\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init_op)\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + self.hparam_str)\n",
    "        self.writer.add_graph(self.sess.graph)\n",
    "        \n",
    "        # Add embedding tensorboard visualization. Need tensorflow version\n",
    "        self.config = projector.ProjectorConfig()\n",
    "        self.embed = self.config.embeddings.add()\n",
    "        self.embed.tensor_name = self.embedding_matrix.name\n",
    "        self.embed.metadata_path = os.path.join(self.embed_vis_path)\n",
    "        projector.visualize_embeddings(self.writer, self.config)\n",
    "        \n",
    "        \n",
    "    def embed_matrix(self, stddev=0.1, name='embeddings'):\n",
    "        # index_size would be the size of the character set\n",
    "        with tf.name_scope(name):\n",
    "            if not self.one_hot:\n",
    "                embedding_matrix = tf.get_variable(\n",
    "                    'embedding_matrix', \n",
    "                    initializer=tf.truncated_normal([self.n_char, self.char_embed_dim], \n",
    "                                                    stddev=stddev, \n",
    "                                                    seed=self.seed), \n",
    "                    trainable=True)\n",
    "            else:\n",
    "                # creating a one-hot for each character corresponds to the identity matrix\n",
    "                embedding_matrix = tf.constant(value=np.identity(self.n_char), \n",
    "                                               name='embedding_matrix', \n",
    "                                               dtype=tf.float32)\n",
    "                self.char_embed_dim = self.n_char\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('embedding_matrix', embedding_matrix, collections=['train'])\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            return self.embedding_matrix\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, \n",
    "                  input, \n",
    "                  name='LSTM'):\n",
    "        # check, then set the right name\n",
    "        assert self.rnn_type in ['LSTM', 'GRU'], \\\n",
    "            'rnn_type has to be either LSTM or GRU'\n",
    "        name = 'LSTM' if self.rnn_type == 'LSTM' else 'GRU'\n",
    "        if self.bidirection: name += '_bidir'\n",
    "        with tf.name_scope(name):\n",
    "            input = tf.nn.embedding_lookup(self.embedding_matrix, input)\n",
    "            # reshaping\n",
    "            # Permuting batch_size and n_steps\n",
    "            input = tf.transpose(input, [1, 0, 2])\n",
    "            # Reshaping to (n_steps*batch_size, n_input)\n",
    "            input = tf.reshape(input, [-1, self.char_embed_dim])\n",
    "            # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "            rnn_inputs = tf.split(input, self.seq_len, 0)\n",
    "            \n",
    "            # setting the correct RNN cell type (LSTM of GRU)\n",
    "            rnn_cell = rnn.BasicLSTMCell if self.rnn_type == 'LSTM' \\\n",
    "                else rnn.GRUCell\n",
    "            # setting the args (forget_bias applies only to LSTM)\n",
    "            rnn_cell_args = {'num_units': self.hidden_state_size, \n",
    "                             'activation': tf_custom_tanh}\n",
    "            if 'LSTMCell' in str(rnn_cell.__call__ ):\n",
    "                rnn_cell_args['forget_bias'] = 1.0\n",
    "            rnn_cell(**rnn_cell_args)\n",
    "            \n",
    "            cell_fw = rnn_cell(**rnn_cell_args)\n",
    "            cell_fw = rnn.DropoutWrapper(cell_fw, \n",
    "                                         output_keep_prob=self.keep_prob, \n",
    "                                         seed=self.seed)\n",
    "            \n",
    "            if self.bidirection:\n",
    "                # add another cell for backwards direction and a dropout wrapper\n",
    "                cell_bw = rnn_cell(**rnn_cell_args)\n",
    "                cell_bw = rnn.DropoutWrapper(cell_bw, \n",
    "                                             output_keep_prob=self.keep_prob, \n",
    "                                             seed=self.seed)\n",
    "                outputs, _, _ = rnn.static_bidirectional_rnn(\n",
    "                    cell_fw, cell_bw, rnn_inputs, dtype=tf.float32, scope=name)\n",
    "            else:\n",
    "                outputs, _ = rnn.static_rnn(cell_fw, rnn_inputs, dtype=tf.float32, scope=name)\n",
    "            \n",
    "            if not self.target_rep:  # take only last output (list for structure consistency)\n",
    "                outputs = [outputs[-1]]\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('outputs', outputs, collections=['train'])\n",
    "            return outputs\n",
    "\n",
    "\n",
    "    def logit(self, \n",
    "              input, \n",
    "              size_in, \n",
    "              size_out, \n",
    "              stddev=0.1, \n",
    "              name='logit'):\n",
    "\n",
    "        with tf.name_scope(name):\n",
    "            w = tf.Variable(tf.truncated_normal([size_in, size_out], \n",
    "                                                stddev=stddev, \n",
    "                                                seed=self.seed), \n",
    "                            name='W')\n",
    "            b = tf.Variable(tf.constant(0.1, \n",
    "                                        shape=[size_out]), \n",
    "                            name='B')\n",
    "            logits = tf.matmul(input, w) + b\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('weights', w, collections=['train'])\n",
    "                tf.summary.histogram('biases', b, collections=['train'])\n",
    "                tf.summary.histogram('logits', logits, collections=['train'])\n",
    "            return logits\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        print('Starting to train model {:s}'.format(self.hparam_str))\n",
    "        for i in range(1, self.epochs+1):\n",
    "            # update learning rate, if it is dynamic\n",
    "            if self.dynamic_learn_rate: self.update_lr(epoch=i)\n",
    "            # train step\n",
    "            self.sess.run(self.train_step, feed_dict=self.feed_dict_train)\n",
    "            if i % self.summary_step == 0:\n",
    "                # train summary\n",
    "                # use self.feed_dict_train_eval for evaluation (keep probability set to 1.0)\n",
    "                [train_accuracy, train_cost,_ , _, _, _, train_top_k, s] = \\\n",
    "                    self.sess.run([self.accuracy, \n",
    "                                   self.cost, self.cost_targetrep, self.cost_crossent, self.cost_l2reg, \n",
    "                                   self.recip_rank, \n",
    "                                   self.top_k_res, \n",
    "                                   self.summ_train],\n",
    "                                  feed_dict=self.feed_dict_train_eval)\n",
    "                self.writer.add_summary(s, i)\n",
    "                print('{:.3f} of observations in the top is {}'.format(train_top_k, self.top_k))\n",
    "                # test summary\n",
    "                [test_accuracy, test_cost,_ , _, _, _, test_top_k, s] = \\\n",
    "                    self.sess.run([self.accuracy, \n",
    "                                   self.cost, self.cost_targetrep, self.cost_crossent, self.cost_l2reg, \n",
    "                                   self.recip_rank, \n",
    "                                   self.top_k_res, \n",
    "                                   self.summ_test],\n",
    "                                  feed_dict=self.feed_dict_test)\n",
    "                self.writer.add_summary(s, i)\n",
    "                \n",
    "                print('Epoch number {}, '.format(i) +\n",
    "                      'training accuracy is {:.5f} and '.format(train_accuracy) + \n",
    "                      'test accuracy is {:.5f}, '.format(test_accuracy))\n",
    "                print('training cost is {:.5f} and '.format(train_cost) + \n",
    "                      'test cost is {:.5f} and '.format(test_cost))\n",
    "                \n",
    "            if i % self.save_step == 0:\n",
    "                print('Saving step {}'.format(i))\n",
    "                self.saver.save(self.sess, os.path.join(self.log_dir, \n",
    "                                                        self.hparam_str, \n",
    "                                                        'model.ckpt'), i)\n",
    "            \n",
    "        print('Training the model is done! ({:s})'.format(self.hparam_str))\n",
    "    \n",
    "    \n",
    "    def tf_get_rank_order(self, input, reciprocal):\n",
    "        \"\"\"\n",
    "        Returns a tensor of the rank of the input tensor's elements.\n",
    "        rank(highest element) = 1.\n",
    "        \"\"\"\n",
    "        assert isinstance(reciprocal, bool), 'reciprocal has to be bool'\n",
    "        size = tf.size(input)\n",
    "        indices_of_ranks = tf.nn.top_k(-input, k=size)[1]\n",
    "        indices_of_ranks = size - tf.nn.top_k(-indices_of_ranks, k=size)[1]\n",
    "        if reciprocal:\n",
    "            indices_of_ranks = tf.cast(indices_of_ranks, tf.float32)\n",
    "            indices_of_ranks = tf.map_fn(\n",
    "                lambda x: tf.reciprocal(x), indices_of_ranks, \n",
    "                dtype=tf.float32)\n",
    "            return indices_of_ranks\n",
    "        else:\n",
    "            return indices_of_ranks\n",
    "    \n",
    "    \n",
    "    def get_reciprocal_rank(self, logits, targets, reciprocal=True):\n",
    "        \"\"\"\n",
    "        Returns a tensor containing the (reciprocal) ranks\n",
    "        of the logits tensor (wrt the targets tensor).\n",
    "        The targets tensor should be a 'one hot' vector \n",
    "        (otherwise apply one_hot on targets, such that index_mask is a one_hot).\n",
    "        \"\"\"\n",
    "        function_to_map = lambda x: self.tf_get_rank_order(x, reciprocal=reciprocal)\n",
    "        ordered_array_dtype = tf.float32 if reciprocal is not None else tf.int32\n",
    "        ordered_array = tf.map_fn(function_to_map, logits, \n",
    "                                  dtype=ordered_array_dtype)\n",
    "\n",
    "        size = int(logits.shape[1])\n",
    "        index_mask = tf.reshape(\n",
    "                targets, [-1,size])\n",
    "        if reciprocal:\n",
    "            index_mask = tf.cast(index_mask, tf.float32)\n",
    "\n",
    "        return tf.reduce_sum(ordered_array * index_mask,1)\n",
    "    \n",
    "    \n",
    "    def restore(self, cp_path, feed_dict = None):\n",
    "        \n",
    "        print('Loading variables from {:s}'.format(cp_path))\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(cp_path)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\"no checkpoint found\")\n",
    "\n",
    "#         if feed_dict:\n",
    "#             self.feed(feed_dict=feed_dict)\n",
    "        print('Loading successful')\n",
    "    \n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "    \n",
    "    def update_lr(self, epoch):\n",
    "        self.learn_rate = 1.0 / np.sqrt(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "linear is expecting 2D arguments: [TensorShape([Dimension(None), Dimension(64)]), TensorShape(None)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1cec468e5cca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#                   **{**kwargs_simple_lstm,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#                      **{'epochs': 40}}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                   \u001b[0;34m**\u001b[0m\u001b[0mkwargs_simple_lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                  )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2695ae285e1c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hparam_str, seq_len, n_class, n_char, char_embed_dim, one_hot, hidden_state_size, keep_prob, learn_rate, dynamic_learn_rate, rnn_type, bidirection, top_k, epochs, log_dir, embed_vis_path, summary_step, save_step, seed, l2_wieght_reg, target_rep, target_rep_weight, verbose_summary, feed_dict_train, feed_dict_test, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_unit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logits_seq'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirection\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogit_in_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2695ae285e1c>\u001b[0m in \u001b[0;36mlstm_unit\u001b[0;34m(self, input, name)\u001b[0m\n\u001b[1;32m    240\u001b[0m                     cell_fw, cell_bw, rnn_inputs, dtype=tf.float32, scope=name)\n\u001b[1;32m    241\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_rep\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# take only last output (list for structure consistency)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yarden/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m    195\u001b[0m             state_size=cell.state_size)\n\u001b[1;32m    196\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yarden/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvarscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yarden/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    522\u001b[0m         self._input_keep_prob < 1):\n\u001b[1;32m    523\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_keep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m     if (not isinstance(self._output_keep_prob, float) or\n\u001b[1;32m    526\u001b[0m         self._output_keep_prob < 1):\n",
      "\u001b[0;32m/home/yarden/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m     90\u001b[0m         r, u = array_ops.split(\n\u001b[1;32m     91\u001b[0m             value=_linear(\n\u001b[0;32m---> 92\u001b[0;31m                 [inputs, state], 2 * self._num_units, True, 1.0, scope=scope),\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mnum_or_size_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             axis=1)\n",
      "\u001b[0;32m/home/yarden/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m_linear\u001b[0;34m(args, output_size, bias, bias_start, scope)\u001b[0m\n\u001b[1;32m    732\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"linear is expecting 2D arguments: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m       raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n",
      "\u001b[0;31mValueError\u001b[0m: linear is expecting 2D arguments: [TensorShape([Dimension(None), Dimension(64)]), TensorShape(None)]"
     ]
    }
   ],
   "source": [
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "hparam_str = make_hparam_string(**kwargs_simple_lstm)\n",
    "\n",
    "lstm = Lstm_model(hparam_str=hparam_str, \n",
    "                  embed_vis_path=embed_vis_path, \n",
    "                  feed_dict_train=kwargs_feed_dict_train, \n",
    "                  feed_dict_test=kwargs_feed_dict_test, \n",
    "#                   **{**kwargs_simple_lstm, \n",
    "#                      **{'epochs': 40}}\n",
    "                  **kwargs_simple_lstm\n",
    "                 )\n",
    "\n",
    "lstm.train()\n",
    "lstm.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting to train model scale_func=unscale,GRU,bidirection=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=32,l2_wieght_reg=1.0E-03,target_rep_weight=0.1/\n",
    "# 0.232 of observations in the top is 5\n",
    "# Epoch number 10, training accuracy is 0.08674 and test accuracy is 0.09589, \n",
    "# training cost is 4.16813 and test cost is 4.12558 and \n",
    "# 0.269 of observations in the top is 5\n",
    "# Epoch number 20, training accuracy is 0.07365 and test accuracy is 0.07397, \n",
    "# training cost is 3.92431 and test cost is 3.88113 and \n",
    "# 0.300 of observations in the top is 5\n",
    "# Epoch number 30, training accuracy is 0.11620 and test accuracy is 0.11507, \n",
    "# training cost is 3.80271 and test cost is 3.80097 and \n",
    "# 0.366 of observations in the top is 5\n",
    "# Epoch number 40, training accuracy is 0.14730 and test accuracy is 0.16164, \n",
    "# training cost is 3.52861 and test cost is 3.50138 and \n",
    "# 0.447 of observations in the top is 5\n",
    "# Epoch number 50, training accuracy is 0.16858 and test accuracy is 0.18082, \n",
    "# training cost is 3.30158 and test cost is 3.29581 and \n",
    "# 0.458 of observations in the top is 5\n",
    "# Epoch number 60, training accuracy is 0.18740 and test accuracy is 0.18356, \n",
    "# training cost is 3.17156 and test cost is 3.20611 and \n",
    "# 0.532 of observations in the top is 5\n",
    "# Epoch number 70, training accuracy is 0.22504 and test accuracy is 0.22740, \n",
    "# training cost is 2.99391 and test cost is 3.05044 and \n",
    "# 0.593 of observations in the top is 5\n",
    "# Epoch number 80, training accuracy is 0.26350 and test accuracy is 0.25479, \n",
    "# training cost is 2.80913 and test cost is 2.90349 and \n",
    "# 0.637 of observations in the top is 5\n",
    "# Epoch number 90, training accuracy is 0.30606 and test accuracy is 0.28493, \n",
    "# training cost is 2.63785 and test cost is 2.75494 and \n",
    "# 0.714 of observations in the top is 5\n",
    "# Epoch number 100, training accuracy is 0.35516 and test accuracy is 0.33151, \n",
    "# training cost is 2.46441 and test cost is 2.61622 and \n",
    "# Training the model is done! (scale_func=unscale,GRU,bidirection=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=32,l2_wieght_reg=1.0E-03,target_rep_weight=0.1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.sess.run(lstm.outputs, feed_dict=lstm.feed_dict_train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "lstm_models = {}\n",
    "for learn_rate in list(np.logspace(-2, -3, 2)):\n",
    "    for keep_prob in [0.8]:\n",
    "        for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "            for hidden_state_size in [64, 128]:\n",
    "                for l2_wieght_reg in list(np.logspace(-3, -4, 2)):\n",
    "                    for target_rep_weight in [0.1, 0.3]:\n",
    "                        # collect new hyperparameters as args\n",
    "                        current_kw_simple_lstm = {\n",
    "                            **kwargs_simple_lstm, \n",
    "                            **{'learn_rate': learn_rate, \n",
    "                               'keep_prob': keep_prob, \n",
    "                               'one_hot': one_hot, \n",
    "                               'char_embed_dim': char_embed_dim, \n",
    "                               'hidden_state_size': hidden_state_size, \n",
    "                               'l2_wieght_reg': l2_wieght_reg, \n",
    "                               'target_rep_weight': target_rep_weight\n",
    "        #                        'bidirection': bidirection, \n",
    "        #                        'target_rep': target_rep\n",
    "                              }}\n",
    "                        hparam_str = make_hparam_string(**current_kw_simple_lstm)\n",
    "                        var = 'lstm_{}'.format(hparam_str)\n",
    "                        lstm_models[var] = Lstm_model(feed_dict_train=kwargs_feed_dict_train, \n",
    "                                                      feed_dict_test=kwargs_feed_dict_test, \n",
    "                                                      hparam_str=hparam_str, \n",
    "                                                      embed_vis_path=embed_vis_path, \n",
    "                                                      **current_kw_simple_lstm)\n",
    "                        lstm_models[var].train()\n",
    "                        lstm_models[var].close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting data directly from a tensorboard log dir\n",
    "from tensorflow.python.summary import event_multiplexer\n",
    "# specify path (for parent log dir)\n",
    "log_parent_dir = './logdir_exper_4_3/'\n",
    "ea = event_multiplexer.EventMultiplexer().AddRunsFromDirectory(log_parent_dir)\n",
    "ea.Reload()  # load\n",
    "\n",
    "child_dir = next(os.walk(log_parent_dir))[1]\n",
    "print(ea.Scalars(child_dir[0], 'accuracy/accuracy_test'))  # specify run, scalar_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
