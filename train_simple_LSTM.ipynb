{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector  # embeddings visualizer\n",
    "\n",
    "# from collections import OrderedDict\n",
    "# from collections import Counter\n",
    "# from math import ceil\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The are 2919 observations\n",
      "Sampling from allowed 82 labels\n",
      "82 labels in the validation set, with\n",
      "1587 potential observation to draw from.\n",
      "365 observations sampled for validation\n",
      "1222 observations for training\n",
      "The ratio of validation to *training* is about 0.299\n"
     ]
    }
   ],
   "source": [
    "kwargs_neural_data_init = \\\n",
    "    {'mk_chars': True, \n",
    "               'model': 'neural', \n",
    "               'char_filter': 100, 'allowed_chars': None, \n",
    "               'mk_ngrams': False, 'ngram_width': 5, \n",
    "               'ngram_filter': 10, 'allowed_ngrams': None, \n",
    "               'keep_infreq_labels': False, 'label_count_thresh': 10, \n",
    "               'valid_ratio': 0.25, \n",
    "               'scale_func': unscale, 'to_permute': True, }\n",
    "\n",
    "x_feed_train, y_feed_train, x_feed_val, y_feed_val,\\\n",
    "    char_int, char_int_inv, label_int, label_int_inv, \\\n",
    "    statistics_dict =\\\n",
    "    data_load_preprocess(**kwargs_neural_data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory was deleted.\n"
     ]
    }
   ],
   "source": [
    "kwargs_simple_lstm = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'logdir/', \n",
    "    'del_log': True, \n",
    "    # preprocessing and data\n",
    "    'top_k': 5, \n",
    "    'seed': seed(), \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1E-1,  # 1E-4\n",
    "    'dynamic_learn_rate': False, \n",
    "    'rnn_type': 'GRU',\n",
    "    'bidirection': False, \n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': False,\n",
    "    'hidden_state_size': 32, \n",
    "    'keep_prob': 0.7, \n",
    "    'l2_wieght_reg': 1E-4, \n",
    "    'target_rep': True, \n",
    "    'target_rep_weight': 0.1, \n",
    "    'epochs': 200,\n",
    "    'summary_step': 10, \n",
    "    'save_step': np.inf,\n",
    "    'verbose_summary': False\n",
    "})\n",
    "\n",
    "if kwargs_simple_lstm.save_step == np.inf: \n",
    "    kwargs_simple_lstm.save_step = kwargs_simple_lstm.epochs\n",
    "    \n",
    "kwargs_simple_lstm = {**kwargs_simple_lstm, \n",
    "                      **statistics_dict}\n",
    "\n",
    "kwargs_simple_lstm = nice_dict({**kwargs_simple_lstm, \n",
    "                                **{'scale_func': kwargs_neural_data_init['scale_func'], \n",
    "                                   'keep_infreq_labels': kwargs_neural_data_init['keep_infreq_labels']}})\n",
    "\n",
    "if kwargs_simple_lstm.del_log: remove_dir_content(kwargs_simple_lstm.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "# training data\n",
    "X_train, _, Y_train = index_transorm_xy(x=x_feed_train, \n",
    "                                        y=y_feed_train, \n",
    "                                        char_int=char_int, \n",
    "                                        label_int=label_int, \n",
    "                                        **kwargs_simple_lstm)\n",
    "\n",
    "# validation data\n",
    "X_val, _, Y_val = index_transorm_xy(x=x_feed_val, \n",
    "                                    y=y_feed_val, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    **kwargs_simple_lstm)\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_simple_lstm.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Lstm_model(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 *args, \n",
    "                 hparam_str, \n",
    "#                  n_train, \n",
    "#                  n_test, \n",
    "                 seq_len, \n",
    "                 n_class, \n",
    "                 n_char, \n",
    "                 char_embed_dim, \n",
    "                 one_hot, \n",
    "                 hidden_state_size, \n",
    "                 keep_prob, \n",
    "                 learn_rate, \n",
    "                 dynamic_learn_rate, \n",
    "                 rnn_type, \n",
    "                 bidirection, \n",
    "                 top_k, \n",
    "                 epochs, \n",
    "                 log_dir, \n",
    "                 embed_vis_path, \n",
    "                 summary_step, \n",
    "                 save_step, \n",
    "                 seed, \n",
    "                 l2_wieght_reg, \n",
    "                 target_rep, \n",
    "                 target_rep_weight, \n",
    "                 verbose_summary, \n",
    "                 feed_dict_train, \n",
    "                 feed_dict_test, \n",
    "                 **kwargs):\n",
    "        \n",
    "        self.hparam_str = hparam_str\n",
    "#         self.n_train = n_train\n",
    "#         self.n_test = n_test\n",
    "        self.seq_len = seq_len \n",
    "        self.n_class = n_class \n",
    "        self.n_char = n_char\n",
    "        self.char_embed_dim = char_embed_dim\n",
    "        self.one_hot = one_hot\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "#         self.keep_prob = keep_prob\n",
    "        self.learn_rate = learn_rate\n",
    "        self.dynamic_learn_rate = dynamic_learn_rate\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirection = bidirection\n",
    "        self.top_k = top_k\n",
    "        self.epochs = epochs\n",
    "        self.log_dir = log_dir\n",
    "        self.embed_vis_path = embed_vis_path\n",
    "        self.summary_step = summary_step \n",
    "        self.save_step = save_step\n",
    "        self.seed = seed\n",
    "        self.l2_wieght_reg = l2_wieght_reg\n",
    "        self.target_rep = target_rep\n",
    "        self.verbose_summary = verbose_summary\n",
    "        self.target_rep_weight = target_rep_weight if self.target_rep else 0.0\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "        # clear tf graph and set seeds\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        \n",
    "        # Setup placeholders, and reshape the data\n",
    "        self.x_ = tf.placeholder(tf.int32, [None, self.seq_len], \n",
    "                            name='Examples')\n",
    "        self.y_ = tf.placeholder(tf.int32, [None, self.n_class], \n",
    "                            name='Lables')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], \n",
    "                            name='Keep_probability')\n",
    "\n",
    "        self.feed_dict_train = {self.x_: feed_dict_train['x'], \n",
    "                                self.y_: feed_dict_train['y'], \n",
    "                                self.keep_prob: keep_prob}\n",
    "\n",
    "        self.feed_dict_train_eval = {**self.feed_dict_train, \n",
    "                                     **{self.keep_prob: 1.0}}\n",
    "\n",
    "        self.feed_dict_test = {self.x_: feed_dict_test['x'], \n",
    "                               self.y_: feed_dict_test['y'], \n",
    "                               self.keep_prob: 1.0}\n",
    "\n",
    "        self.embedding_matrix = self.embed_matrix()\n",
    "\n",
    "        self.outputs = self.lstm_unit(input=self.x_)\n",
    "        with tf.name_scope('logits_seq'):\n",
    "            if self.bidirection: logit_in_size = 2 * self.hidden_state_size\n",
    "            else: logit_in_size = self.hidden_state_size\n",
    "            self.logits = [self.logit(input=out, \n",
    "                                      size_in=logit_in_size, \n",
    "                                      size_out=self.n_class) \n",
    "                           for out in self.outputs]\n",
    "\n",
    "        with tf.name_scope('Cost_function'):\n",
    "            # cross entropy loss with target replication and\n",
    "            # regularization terms based on the weights' L2 norm\n",
    "            with tf.name_scope('target_replication_loss'):\n",
    "                self.cost_targetrep = tf.reduce_mean(\n",
    "                    [tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=log, labels=self.y_) \n",
    "                     for log in self.logits])\n",
    "            with tf.name_scope('cross_entropy'):\n",
    "                self.cost_crossent = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=self.logits[-1], labels=self.y_))\n",
    "            with tf.name_scope('L2_norm_reg'):\n",
    "                self.cost_l2reg = tf.reduce_mean([tf.nn.l2_loss(weight) \n",
    "                                                  for weight in tf.trainable_variables()])\n",
    "            with tf.name_scope('total_cost'):\n",
    "                self.cost = self.target_rep_weight * self.cost_targetrep + \\\n",
    "                    (1 - self.target_rep_weight) * self.cost_crossent + \\\n",
    "                    self.l2_wieght_reg * self.cost_l2reg\n",
    "            # add summaries\n",
    "            tf.summary.scalar('Total_cost_train', \n",
    "                              self.cost, collections=['train'])\n",
    "            tf.summary.scalar('Total_cost_test', \n",
    "                              self.cost, collections=['test'])\n",
    "            \n",
    "        with tf.name_scope('Cost_function_additional_metrics'):\n",
    "            tf.summary.scalar('Target_rep_cost_train', \n",
    "                              self.cost_targetrep, collections=['train'])\n",
    "            tf.summary.scalar('Target_rep_cost_test', \n",
    "                              self.cost_targetrep, collections=['test'])\n",
    "            tf.summary.scalar('Cross_entropy_train', \n",
    "                              self.cost_crossent, collections=['train'])\n",
    "            tf.summary.scalar('Cross_entropy_test', \n",
    "                              self.cost_crossent, collections=['test'])\n",
    "            tf.summary.scalar('L2_norm_train', \n",
    "                              self.cost_l2reg, collections=['train'])\n",
    "            tf.summary.scalar('L2_norm_test', \n",
    "                              self.cost_l2reg, collections=['test'])            \n",
    "            \n",
    "        with tf.name_scope('Train'):\n",
    "            if self.dynamic_learn_rate:\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(self.learn_rate)\n",
    "            else:\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learn_rate)\n",
    "            self.train_step = self.optimizer.minimize(self.cost)\n",
    "\n",
    "        with tf.name_scope('Accuracy'):  # modified to take last element of logits\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.logits[-1], 1), tf.argmax(self.y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('Accuracy_train', self.accuracy, collections=['train'])\n",
    "            tf.summary.scalar('Accuracy_test', self.accuracy, collections=['test'])\n",
    "\n",
    "        with tf.name_scope('In_top_{}'.format(self.top_k)):  # modified to take last element of logits\n",
    "            self.y_targets = tf.argmax(self.y_, 1)\n",
    "            self.top_k_res = tf.reduce_mean(tf.cast(\n",
    "                tf.nn.in_top_k(self.logits[-1], self.y_targets, self.top_k), \n",
    "                tf.float32))\n",
    "            tf.summary.scalar('In_top_{}_train'.format(self.top_k), self.top_k_res, collections=['train'])\n",
    "            tf.summary.scalar('In_top_{}_test'.format(self.top_k), self.top_k_res, collections=['test'])\n",
    "\n",
    "        # summaries per collection and saver object\n",
    "        self.summ_train = tf.summary.merge_all('train')\n",
    "        self.summ_test = tf.summary.merge_all('test')\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        # init vars and setup writer\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init_op)\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + self.hparam_str)\n",
    "        self.writer.add_graph(self.sess.graph)\n",
    "        \n",
    "        # Add embedding tensorboard visualization. Need tensorflow version\n",
    "        self.config = projector.ProjectorConfig()\n",
    "        self.embed = self.config.embeddings.add()\n",
    "        self.embed.tensor_name = self.embedding_matrix.name\n",
    "        self.embed.metadata_path = os.path.join(self.embed_vis_path)\n",
    "        projector.visualize_embeddings(self.writer, self.config)\n",
    "        \n",
    "        \n",
    "    def embed_matrix(self, stddev=0.1, name='embeddings'):\n",
    "        # index_size would be the size of the character set\n",
    "        with tf.name_scope(name):\n",
    "            if not self.one_hot:\n",
    "                embedding_matrix = tf.get_variable(\n",
    "                    'embedding_matrix', \n",
    "                    initializer=tf.truncated_normal([self.n_char, self.char_embed_dim], \n",
    "                                                    stddev=stddev, \n",
    "                                                    seed=self.seed), \n",
    "                    trainable=True)\n",
    "            else:\n",
    "                # creating a one-hot for each character corresponds to the identity matrix\n",
    "                embedding_matrix = tf.constant(value=np.identity(self.n_char), \n",
    "                                               name='embedding_matrix', \n",
    "                                               dtype=tf.float32)\n",
    "                self.char_embed_dim = self.n_char\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('embedding_matrix', embedding_matrix, collections=['train'])\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            return self.embedding_matrix\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, \n",
    "                  input, \n",
    "                  name='LSTM'):\n",
    "        # check, then set the right name\n",
    "        assert self.rnn_type in ['LSTM', 'GRU'], \\\n",
    "            'rnn_type has to be either LSTM or GRU'\n",
    "        name = 'LSTM' if self.rnn_type == 'LSTM' else 'GRU'\n",
    "        if self.bidirection: name += '_bidir'\n",
    "        with tf.name_scope(name):\n",
    "            input = tf.nn.embedding_lookup(self.embedding_matrix, input)\n",
    "            # reshaping\n",
    "            # Permuting batch_size and n_steps\n",
    "            input = tf.transpose(input, [1, 0, 2])\n",
    "            # Reshaping to (n_steps*batch_size, n_input)\n",
    "            input = tf.reshape(input, [-1, self.char_embed_dim])\n",
    "            # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "            rnn_inputs = tf.split(input, self.seq_len, 0)\n",
    "            \n",
    "            ###########################################################\n",
    "            \n",
    "            # setting the correct RNN cell type (LSTM of GRU)\n",
    "            rnn_cell = rnn.BasicLSTMCell if self.rnn_type == 'LSTM' \\\n",
    "                else rnn.GRUCell\n",
    "            # setting the args (forget_bias applies only to LSTM)\n",
    "            rnn_cell_args = {'num_units': self.hidden_state_size}\n",
    "            if 'LSTMCell' in str(rnn_cell.__call__ ):\n",
    "                rnn_cell_args['forget_bias'] = 1.0\n",
    "\n",
    "            rnn_cell(**rnn_cell_args)\n",
    "            \n",
    "            ###########################################################\n",
    "\n",
    "            cell_fw = rnn_cell(**rnn_cell_args)\n",
    "            cell_fw = rnn.DropoutWrapper(cell_fw, \n",
    "                                         output_keep_prob=self.keep_prob, \n",
    "                                         seed=self.seed)\n",
    "            \n",
    "            if self.bidirection:\n",
    "                # add another cell for backwards direction and a dropout wrapper\n",
    "                cell_bw = rnn_cell(**rnn_cell_args)\n",
    "                cell_bw = rnn.DropoutWrapper(cell_bw, \n",
    "                                             output_keep_prob=self.keep_prob, \n",
    "                                             seed=self.seed)\n",
    "                outputs, _, _ = rnn.static_bidirectional_rnn(\n",
    "                    cell_fw, cell_bw, rnn_inputs, dtype=tf.float32, scope=name)\n",
    "            else:\n",
    "                outputs, _ = rnn.static_rnn(cell_fw, rnn_inputs, dtype=tf.float32, scope=name)\n",
    "            \n",
    "            if not self.target_rep:  # take only last output (list for structure consistency)\n",
    "                outputs = [outputs[-1]]\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('outputs', outputs, collections=['train'])\n",
    "            return outputs\n",
    "\n",
    "\n",
    "    def logit(self, \n",
    "              input, \n",
    "              size_in, \n",
    "              size_out, \n",
    "              stddev=0.1, \n",
    "              name='logit'):\n",
    "\n",
    "        with tf.name_scope(name):\n",
    "            w = tf.Variable(tf.truncated_normal([size_in, size_out], \n",
    "                                                stddev=stddev, \n",
    "                                                seed=self.seed), \n",
    "                            name='W')\n",
    "            b = tf.Variable(tf.constant(0.1, \n",
    "                                        shape=[size_out]), \n",
    "                            name='B')\n",
    "            logits = tf.matmul(input, w) + b\n",
    "            if self.verbose_summary:\n",
    "                tf.summary.histogram('weights', w, collections=['train'])\n",
    "                tf.summary.histogram('biases', b, collections=['train'])\n",
    "                tf.summary.histogram('logits', logits, collections=['train'])\n",
    "            return logits\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        print('Starting to train model {:s}'.format(self.hparam_str))\n",
    "        for i in range(1, self.epochs+1):\n",
    "            # update learning rate, if it is dynamic\n",
    "            if self.dynamic_learn_rate: self.update_lr(epoch=i)\n",
    "            # train step\n",
    "            self.sess.run(self.train_step, feed_dict=self.feed_dict_train)\n",
    "            if i % self.summary_step == 0:\n",
    "                # train summary\n",
    "                # use self.feed_dict_train_eval for evaluation (keep probability set to 1.0)\n",
    "                [train_accuracy, train_cost, _, _, _, train_top_k, s] = \\\n",
    "                    self.sess.run([self.accuracy, \n",
    "                                   self.cost, self.cost_targetrep, self.cost_crossent, self.cost_l2reg, \n",
    "                                   self.top_k_res, \n",
    "                                   self.summ_train],\n",
    "                                  feed_dict=self.feed_dict_train_eval)\n",
    "                self.writer.add_summary(s, i)\n",
    "                print('{:.3f} of observations in the top is {}'.format(train_top_k, self.top_k))\n",
    "                # test summary\n",
    "                [test_accuracy, test_cost, _, _, _, test_top_k, s] = \\\n",
    "                    self.sess.run([self.accuracy, \n",
    "                                   self.cost, self.cost_targetrep, self.cost_crossent, self.cost_l2reg, \n",
    "                                   self.top_k_res, \n",
    "                                   self.summ_test],\n",
    "                                  feed_dict=self.feed_dict_test)\n",
    "                self.writer.add_summary(s, i)\n",
    "                \n",
    "                print('Epoch number {}, '.format(i) +\n",
    "                      'training accuracy is {:.5f} and '.format(train_accuracy) + \n",
    "                      'test accuracy is {:.5f}, '.format(test_accuracy))\n",
    "                print('training cost is {:.5f} and '.format(train_cost) + \n",
    "                      'test cost is {:.5f} and '.format(test_cost))\n",
    "                \n",
    "            if i % self.save_step == 0:\n",
    "                print('Saving step {}'.format(i))\n",
    "                self.saver.save(self.sess, os.path.join(self.log_dir, \n",
    "                                                        self.hparam_str, \n",
    "                                                        'model.ckpt'), i)\n",
    "            \n",
    "        print('Training the model is done! ({:s})'.format(self.hparam_str))\n",
    "    \n",
    "    \n",
    "    def restore(self, cp_path, feed_dict = None):\n",
    "        \n",
    "        print('Loading variables from {:s}'.format(cp_path))\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(cp_path)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\"no checkpoint found\")\n",
    "\n",
    "#         if feed_dict:\n",
    "#             self.feed(feed_dict=feed_dict)\n",
    "        print('Loading successful')\n",
    "    \n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "    \n",
    "    def update_lr(self, epoch):\n",
    "        self.learn_rate = 1.0 / np.sqrt(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train model scale_func=unscale,GRU,bidirection=F,keep_infreq_labels=F,learn_rate=1.0E-01,keep_prob=0.7,char_embed_dim=4,hidden_state_size=32,l2_wieght_reg=1.0E-04,target_rep_weight=0.1/\n",
      "0.246 of observations in the top is 5\n",
      "Epoch number 10, training accuracy is 0.08674 and test accuracy is 0.09589, \n",
      "training cost is 4.02852 and test cost is 4.00730 and \n",
      "0.402 of observations in the top is 5\n",
      "Epoch number 20, training accuracy is 0.17185 and test accuracy is 0.19178, \n",
      "training cost is 3.44052 and test cost is 3.46011 and \n",
      "0.660 of observations in the top is 5\n",
      "Epoch number 30, training accuracy is 0.39853 and test accuracy is 0.38630, \n",
      "training cost is 2.42671 and test cost is 2.51974 and \n",
      "0.834 of observations in the top is 5\n",
      "Epoch number 40, training accuracy is 0.59002 and test accuracy is 0.57808, \n",
      "training cost is 1.58290 and test cost is 1.70795 and \n",
      "0.960 of observations in the top is 5\n",
      "Epoch number 50, training accuracy is 0.81097 and test accuracy is 0.72603, \n",
      "training cost is 0.83264 and test cost is 1.14407 and \n",
      "0.973 of observations in the top is 5\n",
      "Epoch number 60, training accuracy is 0.85270 and test accuracy is 0.77808, \n",
      "training cost is 0.61035 and test cost is 0.96130 and \n",
      "0.995 of observations in the top is 5\n",
      "Epoch number 70, training accuracy is 0.94354 and test accuracy is 0.84658, \n",
      "training cost is 0.34185 and test cost is 0.72937 and \n",
      "0.999 of observations in the top is 5\n",
      "Epoch number 80, training accuracy is 0.97627 and test accuracy is 0.87945, \n",
      "training cost is 0.19753 and test cost is 0.73258 and \n",
      "0.998 of observations in the top is 5\n",
      "Epoch number 90, training accuracy is 0.98527 and test accuracy is 0.88767, \n",
      "training cost is 0.15939 and test cost is 0.65547 and \n",
      "0.999 of observations in the top is 5\n",
      "Epoch number 100, training accuracy is 0.98200 and test accuracy is 0.89863, \n",
      "training cost is 0.15253 and test cost is 0.68080 and \n",
      "1.000 of observations in the top is 5\n",
      "Epoch number 110, training accuracy is 0.99018 and test accuracy is 0.90411, \n",
      "training cost is 0.12971 and test cost is 0.63330 and \n",
      "1.000 of observations in the top is 5\n",
      "Epoch number 120, training accuracy is 0.96154 and test accuracy is 0.88219, \n",
      "training cost is 0.20745 and test cost is 0.81562 and \n",
      "0.999 of observations in the top is 5\n",
      "Epoch number 130, training accuracy is 0.97300 and test accuracy is 0.85753, \n",
      "training cost is 0.18359 and test cost is 0.73011 and \n",
      "0.993 of observations in the top is 5\n",
      "Epoch number 140, training accuracy is 0.93944 and test accuracy is 0.87123, \n",
      "training cost is 0.29250 and test cost is 0.76781 and \n",
      "0.975 of observations in the top is 5\n",
      "Epoch number 150, training accuracy is 0.87234 and test accuracy is 0.79726, \n",
      "training cost is 0.57303 and test cost is 1.06118 and \n",
      "0.298 of observations in the top is 5\n",
      "Epoch number 160, training accuracy is 0.15712 and test accuracy is 0.15068, \n",
      "training cost is 7.33789 and test cost is 7.42620 and \n",
      "0.205 of observations in the top is 5\n",
      "Epoch number 170, training accuracy is 0.06383 and test accuracy is 0.06027, \n",
      "training cost is 6.58000 and test cost is 6.78774 and \n",
      "0.191 of observations in the top is 5\n",
      "Epoch number 180, training accuracy is 0.05646 and test accuracy is 0.09041, \n",
      "training cost is 5.70186 and test cost is 5.49129 and \n",
      "0.292 of observations in the top is 5\n",
      "Epoch number 190, training accuracy is 0.10147 and test accuracy is 0.11233, \n",
      "training cost is 4.53673 and test cost is 4.47150 and \n",
      "0.451 of observations in the top is 5\n",
      "Epoch number 200, training accuracy is 0.18822 and test accuracy is 0.19178, \n",
      "training cost is 3.56877 and test cost is 3.64228 and \n",
      "Saving step 200\n",
      "Training the model is done! (scale_func=unscale,GRU,bidirection=F,keep_infreq_labels=F,learn_rate=1.0E-01,keep_prob=0.7,char_embed_dim=4,hidden_state_size=32,l2_wieght_reg=1.0E-04,target_rep_weight=0.1/)\n"
     ]
    }
   ],
   "source": [
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "hparam_str = make_hparam_string(**kwargs_simple_lstm)\n",
    "\n",
    "lstm = Lstm_model(hparam_str=hparam_str, \n",
    "                  embed_vis_path=embed_vis_path, \n",
    "                  feed_dict_train=kwargs_feed_dict_train, \n",
    "                  feed_dict_test=kwargs_feed_dict_test, \n",
    "#                   **{**kwargs_simple_lstm, \n",
    "#                      **{'epochs': 40}}\n",
    "                  **kwargs_simple_lstm\n",
    "                 )\n",
    "\n",
    "lstm.train()\n",
    "lstm.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_cell = rnn.GRUCell if True \\\n",
    "    else rnn.BasicLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_cell = rnn.GRUCell\n",
    "# rnn_cell = rnn.BasicLSTMCell\n",
    "\n",
    "rnn_cell_args = {'num_units': 5}\n",
    "if 'LSTMCell' in str(rnn_cell.__call__ ):\n",
    "    rnn_cell_args['forget_bias'] = 1.0\n",
    "\n",
    "rnn_cell(**rnn_cell_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name='LSTM'\n",
    "name = 'LSTM' if False else 'RNN'\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# topk= tf.nn.top_k(lstm.logits, \n",
    "#                   k=lstm.n_class, \n",
    "#                   sorted=True)\n",
    "# topk.indices\n",
    "\n",
    "# mod_y_targets = lstm.n_class - 1 - lstm.y_targets\n",
    "# mod_y_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.restore(cp_path=os.path.join(lstm.log_dir, hparam_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm.sess.run(lstm.outputs, feed_dict=lstm.feed_dict_train_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trying out a LOT of hyper-parameters configurations\n",
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "lstm_models = {}\n",
    "for dynamic_learn_rate, learn_rate in [(True, 0.1)] + list(\n",
    "    zip([False] * 2 ,list(np.logspace(-1, -2, 2)))):\n",
    "    for keep_prob in [0.5, 0.7, 1.0]:\n",
    "        for one_hot, char_embed_dim in [(True, 4)] + list(zip([False] * 1 , [4])):\n",
    "            for hidden_state_size in [8, 32]:\n",
    "                # collect new hyperparameters as args\n",
    "                current_kw_simple_lstm = {\n",
    "                    **kwargs_simple_lstm, \n",
    "                    **{'dynamic_learn_rate': dynamic_learn_rate, \n",
    "                       'learn_rate': learn_rate, \n",
    "                       'keep_prob': keep_prob, \n",
    "                       'one_hot': one_hot, \n",
    "                       'char_embed_dim': char_embed_dim, \n",
    "                       'hidden_state_size': hidden_state_size\n",
    "#                        'bidirection': bidirection, \n",
    "#                        'target_rep': target_rep\n",
    "                      }}\n",
    "                # clear tf graph\n",
    "    #                     tf.reset_default_graph()\n",
    "                # create hyperparameter string\n",
    "                hparam_str = make_hparam_string(**current_kw_simple_lstm)\n",
    "                var = 'lstm_{}'.format(hparam_str)\n",
    "                lstm_models[var] = Lstm_model(feed_dict_train=kwargs_feed_dict_train, \n",
    "                                              feed_dict_test=kwargs_feed_dict_test, \n",
    "                                              hparam_str=hparam_str, \n",
    "                                              embed_vis_path=embed_vis_path, \n",
    "                                              **current_kw_simple_lstm)\n",
    "                lstm_models[var].train()\n",
    "                lstm_models[var].close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting data directly from a tensorboard log dir\n",
    "from tensorflow.python.summary import event_multiplexer\n",
    "# specify path (for parent log dir)\n",
    "log_parent_dir = './logdir_exper_4_3/'\n",
    "ea = event_multiplexer.EventMultiplexer().AddRunsFromDirectory(log_parent_dir)\n",
    "ea.Reload()  # load\n",
    "\n",
    "child_dir = next(os.walk(log_parent_dir))[1]\n",
    "print(ea.Scalars(child_dir[0], 'accuracy/accuracy_test'))  # specify run, scalar_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
