{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "from utils.Rnn_model import Rnn_model\n",
    "# import kwargs dicts\n",
    "from utils.kwargs_file import kwargs_neural_data_init  # , kwargs_rnn\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import random\n",
    "random.seed(seed())\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--use_suggestions'], dest='use_suggestions', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='Should the algorithm use label suggestions (False would mean using only the given labeled data)', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enable passing some keyword arguments from command line.\n",
    "This does not affect the Jupyter notebook.\n",
    "\"\"\"\n",
    "# try:\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--use_suggestions', action='store', dest='use_suggestions',\n",
    "                    help='Should the algorithm use label suggestions '+\\\n",
    "                        '(False would mean using only the given labeled data)', \n",
    "                    type=str, \n",
    "                    default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "if args.use_suggestions is not None:\n",
    "    kwargs_neural_data_init.use_suggestions = str(args.use_suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the correct kwargs file, based on suggestion args\n",
    "suggestion_convert_dict = \\\n",
    "    {'0.7': '07', \n",
    "     '0.8': '08', \n",
    "     '0.9': '09', \n",
    "     'F': '10', \n",
    "     'False': '10'}\n",
    "\n",
    "suggestion_str = \\\n",
    "    suggestion_convert_dict[kwargs_neural_data_init.use_suggestions]\n",
    "\n",
    "import_kwargs_str = \\\n",
    "    'from utils.kwargs_file_{} import kwargs_rnn_GRU, kwargs_rnn_LSTM'.format(suggestion_str)\n",
    "\n",
    "exec(import_kwargs_str)  # execute import statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label suggestion data with similarity threshold of 0.8\n",
      "The are 3565 observations\n",
      "Sampling from allowed 96 labels\n",
      "96 labels in the validation set, with\n",
      "2111 potential observation to draw from.\n",
      "480 observations sampled for validation\n",
      "1631 observations for training\n",
      "The ratio of validation to *training* is about 0.294\n"
     ]
    }
   ],
   "source": [
    "x_feed_train, y_feed_train, x_feed_val, y_feed_val,\\\n",
    "    char_int, char_int_inv, label_int, label_int_inv, \\\n",
    "    statistics_dict =\\\n",
    "    data_load_preprocess(**kwargs_neural_data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal validation to trainig ration is 0.100\n",
      "Maximal validation to trainig ration is 0.333\n"
     ]
    }
   ],
   "source": [
    "#### check if there are any strange splits between training and validation\n",
    "#### e.g. labels (training and validation) don't match exactly\n",
    "#### or not enough (or too many) observation were set aside for validation\n",
    "y_train_counter = Counter(y_feed_train)\n",
    "y_train_labs = list(y_train_counter.keys())\n",
    "y_train_labs.sort()\n",
    "\n",
    "y_val_counter = Counter(y_feed_val)\n",
    "y_val_labs = list(y_val_counter.keys())\n",
    "y_val_labs.sort()\n",
    "\n",
    "assert y_train_labs==y_train_labs, 'Labels in training and validation set do not match!'\n",
    "\n",
    "validation_training_ratios = []\n",
    "for label in y_train_labs:\n",
    "    val_num = y_val_counter[label]\n",
    "    train_num = y_train_counter[label]\n",
    "    ratio = float(val_num / train_num)\n",
    "    validation_training_ratios.append(ratio)\n",
    "#     print('{:.3f}'.format(ratio), label, val_num, train_num)\n",
    "\n",
    "print('Minimal validation to trainig ration is {:.3f}'.format(min(validation_training_ratios)))\n",
    "print('Maximal validation to trainig ration is {:.3f}'.format(max(validation_training_ratios)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns np.arrays to feed into tf model\n",
    "# training data\n",
    "X_train, _, Y_train = index_transorm_xy(x=x_feed_train, \n",
    "                                        y=y_feed_train, \n",
    "                                        char_int=char_int, \n",
    "                                        label_int=label_int, \n",
    "                                        n_class=statistics_dict['n_class'])\n",
    "\n",
    "# validation data\n",
    "X_val, _, Y_val = index_transorm_xy(x=x_feed_val, \n",
    "                                    y=y_feed_val, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    n_class=statistics_dict['n_class'])\n",
    "\n",
    "# write a metadata file for embeddings visualizer and create path string\n",
    "embed_vis_path = write_embeddings_metadata(log_dir=kwargs_rnn_GRU.log_dir, \n",
    "                                           dictionary=char_int, \n",
    "                                           file_name='metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs_feed_dict_train = {'x': X_train, 'y': Y_train}\n",
    "kwargs_feed_dict_test = {'x': X_val, 'y': Y_val}\n",
    "\n",
    "kwargs_rnn_GRU = nice_dict({**kwargs_rnn_GRU, **statistics_dict})\n",
    "kwargs_rnn_LSTM = nice_dict({**kwargs_rnn_LSTM, **statistics_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### GRU model\n",
    "hparam_str = make_hparam_string(**kwargs_rnn_GRU)\n",
    "\n",
    "rnn_GRU_model = Rnn_model(hparam_str=hparam_str, \n",
    "                      embed_vis_path=embed_vis_path, \n",
    "                      feed_dict_train=kwargs_feed_dict_train, \n",
    "                      feed_dict_test=kwargs_feed_dict_test, \n",
    "                **kwargs_rnn_GRU\n",
    "                      )\n",
    "\n",
    "rnn_GRU_model.train()\n",
    "rnn_GRU_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train model LSTM,bidir=F,noisy_tanh,learn_p=F,noise_alpha=0.9,noise_half_normal=T,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.5,one_hot,hidden_state_size=128,l2_weight_reg=1.0E-03,target_rep_weight=0.5/\n"
     ]
    }
   ],
   "source": [
    "##### LSTM model\n",
    "hparam_str = make_hparam_string(**kwargs_rnn_LSTM)\n",
    "\n",
    "rnn_LSTM_model = Rnn_model(hparam_str=hparam_str, \n",
    "                      embed_vis_path=embed_vis_path, \n",
    "                      feed_dict_train=kwargs_feed_dict_train, \n",
    "                      feed_dict_test=kwargs_feed_dict_test, \n",
    "                **kwargs_rnn_LSTM\n",
    "                      )\n",
    "\n",
    "rnn_LSTM_model.train()\n",
    "rnn_LSTM_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
