{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score  # gt, pred\n",
    "\n",
    "from utils.utils import user_opt_gen, nice_dict, seed, pcp1, pcp2, pcp3, pcp4\n",
    "\n",
    "# from collections import Counter\n",
    "# from math import isnan\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a charachter:count dict\n",
    "def char_freq_map(*, input_data, filter_by_chars = None, **kwargs):\n",
    "    char_dict = {}\n",
    "    unknown = '<unk-char>'\n",
    "    # check if dataframe or a single obs\n",
    "    if isinstance(input_data, pd.core.series.Series):\n",
    "        # getting line\n",
    "        for line in input_data:\n",
    "            # splitting into characters\n",
    "            chars = list(line)\n",
    "            for char in chars:\n",
    "                if filter_by_chars == None or char in filter_by_chars:\n",
    "                    char_dict[char] = char_dict.get(char, 0) + 1\n",
    "                else:\n",
    "                    char_dict[unknown] = char_dict.get(unknown, 0) + 1\n",
    "    elif isinstance(input_data, str):\n",
    "        # splitting into characters\n",
    "            chars = list(input_data)\n",
    "            for char in chars:\n",
    "                if filter_by_chars == None or char in filter_by_chars:\n",
    "                    char_dict[char] = char_dict.get(char, 0) + 1\n",
    "                else:\n",
    "                    char_dict[unknown] = char_dict.get(unknown, 0) + 1\n",
    "    return nice_dict(char_dict)\n",
    "\n",
    "\n",
    "# create a ngram:count dict\n",
    "def ngram_freq_map(*, input_data, width, filter_by_keys = None, **kwargs):\n",
    "    ngram_dict = {}\n",
    "    # check if dataframe or a single obs\n",
    "    if isinstance(input_data, pd.core.series.Series):\n",
    "        # getting line\n",
    "        for line in input_data:\n",
    "            ngram_dict = update_ngram_dict(line, width, ngram_dict, filter_by_keys)\n",
    "    elif isinstance(input_data, str):\n",
    "        ngram_dict = update_ngram_dict(input_data, width, ngram_dict, filter_by_keys)\n",
    "    return nice_dict(ngram_dict)\n",
    "\n",
    "\n",
    "# create a sliding window and update a dict with counts (default 0)\n",
    "def update_ngram_dict(line, width, ngram_dict, filter_by_keys, **kwargs):\n",
    "    ngrams = sliding_window(line, width)\n",
    "    unknown = '<unk-ngram>'\n",
    "    for ngram in ngrams:\n",
    "        if filter_by_keys == None or ngram in filter_by_keys:\n",
    "            ngram_dict[ngram] = ngram_dict.get(ngram, 0) + 1\n",
    "        else:\n",
    "            ngram_dict[unknown] = ngram_dict.get(unknown, 0) + 1\n",
    "    return ngram_dict\n",
    "\n",
    "\n",
    "def filter_dict_by_val_atleast(input_dict, value):\n",
    "    return nice_dict({k:input_dict[k] for k in input_dict if input_dict[k] >= value})\n",
    "\n",
    "\n",
    "# returns a list with a sliding window\n",
    "# over the string with given width\n",
    "def sliding_window(input_str, width):\n",
    "    assert len(input_str) >= width, 'Cannot slide with width larger than the string!'\n",
    "    return [input_str[i:i + width] for i in range(len(input_str) - width + 1)]\n",
    "\n",
    "\n",
    "# create a joint dict for every observation in the input_data\n",
    "# based on 'ngram_freq_map' and 'char_freq_map'\n",
    "# enables to only select one feature type and filtering\n",
    "def lin_clf_features(*, input_data,\n",
    "                     mk_ngrams=None, width, ngram_filter, \n",
    "                     mk_chars=None, char_filter, \n",
    "                     **kwargs):\n",
    "    assert (mk_ngrams or mk_chars), 'Please select either to create n-grams or character features.'\n",
    "\n",
    "    if mk_ngrams:\n",
    "        # filter ngrams to only those that appear at least 'ngram_filter' times in the input\n",
    "        if isinstance(ngram_filter, int):\n",
    "            print('N-grams filter is applied')\n",
    "            filter_keys_ngrams = list(\n",
    "                filter_dict_by_val_atleast(\n",
    "                    input_dict=ngram_freq_map(input_data=input_data, \n",
    "                                              width=width), \n",
    "                    value=ngram_filter)\n",
    "                .keys())\n",
    "            # apply ngram_freq_map, after figuring out which keys to keep\n",
    "            X_features_ngrams = [ngram_freq_map(input_data=obs, \n",
    "                                                width=width, \n",
    "                                                filter_by_keys=filter_keys_ngrams) for obs in input_data]\n",
    "        # if no filter, just apply the function (for all keys)\n",
    "        else:\n",
    "            print('N-grams filter is NOT applied')\n",
    "            X_features_ngrams = [ngram_freq_map(input_data=obs, \n",
    "                                                width=width)\n",
    "                     for obs in input_data]\n",
    "    else:\n",
    "        X_features_ngrams = [{} for ind in range(len(input_data))]\n",
    "        \n",
    "    if mk_chars:\n",
    "        # filter by character, appear at least 'char_filter' times in the input\n",
    "        if isinstance(char_filter, int):\n",
    "            print('Character filter is applied')\n",
    "            filter_keys_chars = list(\n",
    "                filter_dict_by_val_atleast(\n",
    "                    input_dict=char_freq_map(input_data=input_data), \n",
    "                    value=char_filter)\n",
    "                .keys())\n",
    "            # apply ngram_freq_map, after figuring out which keys to keep\n",
    "            X_features_chars = [char_freq_map(input_data = obs, \n",
    "                                              filter_by_chars=filter_keys_chars) \n",
    "                                for obs in input_data]\n",
    "        else:\n",
    "            print('Character filter is NOT applied')\n",
    "            X_features_chars = [char_freq_map(input_data = obs) \n",
    "                                for obs in input_data]\n",
    "    else:\n",
    "        X_features_chars = [{} for ind in range(len(input_data))]\n",
    "    \n",
    "    # merge two dicts\n",
    "    return [nice_dict({** X_features_ngrams[ind] ,**X_features_chars[ind]}) \n",
    "              for ind in range(len(input_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams filter is applied\n",
      "Character filter is applied\n",
      "X (sparse) matrix, of size 2028 by 1988 has been created.\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=2178, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "0.214990138067\n",
      "['B05B' 'A10BA02' 'B01AB04' 'V06DB' 'V06DB' 'A10BA02' 'V06DB' 'B05B'\n",
      " 'B01AB04' 'V06DB']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    user_opt = user_opt_gen()\n",
    "\n",
    "    main_data = pd.read_csv(user_opt['data_path'], \n",
    "                             sep=';', \n",
    "                             header=0, \n",
    "                             encoding='cp850')\n",
    "\n",
    "    # only observations with ATC labels\n",
    "    main_data_labeled = main_data.loc[[isinstance(k, str) for k in main_data['ATC']],:]\n",
    "\n",
    "#     atc_conversion_data = pd.read_csv(user_opt['atc_conversion_data_path'], \n",
    "#                                       sep=';', \n",
    "#                                       header=0, \n",
    "#                                       encoding='cp850')\n",
    "    \n",
    "    # smaller n for testing purposes\n",
    "    # n = 1000\n",
    "    n = len(main_data_labeled)\n",
    "\n",
    "    x = main_data_labeled['FREETXT'][:n]\n",
    "    y = main_data_labeled['ATC'][:n]\n",
    "    \n",
    "    # create a sparse matrix (X) to hold features \n",
    "    kwargs_lin_clf = nice_dict({'input_data': x, \n",
    "                                'width': 5, \n",
    "                                'mk_ngrams':True, 'ngram_filter': 10, \n",
    "                                'mk_chars':True, 'char_filter': 100})\n",
    "\n",
    "    X_features = lin_clf_features(**kwargs_lin_clf)\n",
    "    \n",
    "    # vectorizer transforms dict into sparse matrix\n",
    "    v = DictVectorizer(sparse=True)\n",
    "\n",
    "    # create a sparse X matrix with character and n-grams features\n",
    "    X = v.fit_transform(X_features)\n",
    "\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "    # v.get_feature_names()\n",
    "    # v.restrict()\n",
    "\n",
    "    # print(y.shape)  # pd object\n",
    "    print('X (sparse) matrix, of size {} by {} has been created.'\n",
    "          .format(X.get_shape()[0], X.get_shape()[1]))  # vectorized\n",
    "    \n",
    "    kwargs_svm = nice_dict({'C': 1.0,  # penalty term\n",
    "                        'decision_function_shape': 'ovr',  # one-vs-rest (‘ovr’) / one-vs-one (‘ovo’) \n",
    "                        'random_state': seed(), \n",
    "                        'kernel': 'rbf', \n",
    "                        'gamma': 'auto' ,  # kernel coef for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘auto’ -> 1/n_features\n",
    "                        'probability': True,  # enable probability estimates \n",
    "                        'shrinking': True,  # use the shrinking heuristic \n",
    "                        'max_iter': -1  # -1 mean no limitation \n",
    "                        })\n",
    "\n",
    "    svm_clf = svm.SVC(**kwargs_svm)\n",
    "\n",
    "    print(svm_clf)\n",
    "    \n",
    "    svm_clf.fit(X, y)\n",
    "\n",
    "    pred = svm_clf.predict(X)\n",
    "\n",
    "    # http://scikit-learn.org/stable/modules/svm.html\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    \n",
    "    print(accuracy_score(y, pred))\n",
    "    print(pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# array to hold log probabilities (takes a bit longer to calc)\n",
    "pred_prob = svm_clf.predict_log_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns an array with indecies of top k elements in the input_array\n",
    "def top_k_ind(input_array, k):\n",
    "    return input_array.argsort()[-k:][::-1]\n",
    "\n",
    "\n",
    "# returns an array with probabilities of top k elements (log likelihood) in the input_array\n",
    "def top_k_prob(input_array, k):\n",
    "    return np.exp(input_array[top_k_ind(input_array, k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indecies of top entries are [103 248 379]\n",
      "Corresponding probabilities are [ 0.15474735  0.01789337  0.01780771]\n",
      "Corresponding labels are ['B05B' 'H02AB07' 'L04AD01']\n",
      "[('B05B', 0.15474734918011418), ('H02AB07', 0.017893367158681218), ('L04AD01', 0.017807706617362012)]\n"
     ]
    }
   ],
   "source": [
    "# proto-type for outputting a ranked list of predictions with SVM\n",
    "test_obs = pred_prob[0,:]\n",
    "test_k = 3\n",
    "# indecies\n",
    "top_ind = top_k_ind(test_obs, test_k)\n",
    "print('Indecies of top entries are {}'.format(top_ind))\n",
    "# probabilities\n",
    "print('Corresponding probabilities are {}'.format(\n",
    "    top_k_prob(test_obs, test_k))\n",
    "     )\n",
    "\n",
    "# labels\n",
    "print('Corresponding labels are {}'.format(\n",
    "    svm_clf.classes_[top_ind])\n",
    "     )\n",
    "\n",
    "print(\n",
    "list(zip(svm_clf.classes_[top_ind], top_k_prob(test_obs, test_k)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
