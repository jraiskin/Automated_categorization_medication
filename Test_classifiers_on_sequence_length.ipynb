{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.utils_nn import *\n",
    "from utils.utils_baseline_svm import *\n",
    "from utils.Rnn_model import Rnn_model\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed())\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import os\n",
    "\n",
    "# import random\n",
    "# random.seed(seed())\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup hyper paramteres and classifiers configurations\n",
    "# SVM ========================\n",
    "kwargs_lin_clf = nice_dict({'mk_chars': True, \n",
    "                            'model': 'linear', \n",
    "                            'char_filter': 100, 'allowed_chars': None, \n",
    "                            'mk_ngrams': True, 'ngram_width': 5, \n",
    "                            'ngram_filter': 10, 'allowed_ngrams': None, \n",
    "                            'keep_infreq_labels': False, 'label_count_thresh': 10, \n",
    "                            'valid_ratio': 0.25, \n",
    "                            'scale_func': unscale, 'to_permute': True, })\n",
    "\n",
    "kwargs_svm = nice_dict({'C': 0.1,  # penalty term\n",
    "                        'decision_function_shape': 'ovr',  # one-vs-rest (‘ovr’) / one-vs-one (‘ovo’) \n",
    "                        'random_state': seed(), \n",
    "                        'kernel': 'linear', \n",
    "                        'gamma': 'auto' ,  # kernel coef for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘auto’ -> 1/n_features\n",
    "                        'probability': True,  # enable probability estimates \n",
    "                        'shrinking': True,  # use the shrinking heuristic \n",
    "                        'max_iter': -1  # -1 mean no limitation \n",
    "                        })\n",
    "\n",
    "# RNN ========================\n",
    "kwargs_neural_data_init = \\\n",
    "    {'mk_chars': True, \n",
    "               'model': 'neural', \n",
    "               'char_filter': 100, 'allowed_chars': None, \n",
    "               'mk_ngrams': False, 'ngram_width': 5, \n",
    "               'ngram_filter': 10, 'allowed_ngrams': None, \n",
    "               'keep_infreq_labels': False, 'label_count_thresh': 10, \n",
    "               'valid_ratio': 0.25, \n",
    "               'scale_func': unscale, 'to_permute': True, }\n",
    "\n",
    "kwargs_simple_rnn = nice_dict({\n",
    "    # log\n",
    "    'log_dir': 'experiments/logdir_exper_4_20_GRU/', \n",
    "    'del_log': False, \n",
    "    # preprocessing and data\n",
    "    'top_k': 5, \n",
    "    'seed': seed(), \n",
    "    # learning hyper-params\n",
    "    'learn_rate': 1E-2, \n",
    "    'dynamic_learn_rate': False, \n",
    "    'rnn_type': 'GRU',\n",
    "    'bidirection': False, \n",
    "    'char_embed_dim': 4, \n",
    "    'one_hot': True,\n",
    "    'hidden_state_size': 128, \n",
    "    'keep_prob': 0.7, \n",
    "    # noisy activation hyper params\n",
    "    'activation_function': 'noisy_tanh',  # tf.tanh / 'noisy_tanh'\n",
    "    'learn_p_delta_scale': False,   # noise scale param in noisy activation\n",
    "    'noise_act_alpha': 0.9,  # mixing in the linear activation\n",
    "    'noise_act_half_normal': False,\n",
    "    # regularization constants\n",
    "    'l2_weight_reg': 1E-3, \n",
    "    'target_rep': True, \n",
    "    'target_rep_weight': 0.3, \n",
    "    # training settings\n",
    "    'epochs': 20,\n",
    "    'summary_step': 10, \n",
    "    'save_step': np.inf,\n",
    "    'to_save': False, \n",
    "    'verbose_summary': False\n",
    "})\n",
    "\n",
    "# GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=0.9,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=128,l2_wieght_reg=1.0E-03,target_rep_weight=0.3\n",
    "rnn_dir_name = 'GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=0.9,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=128,l2_wieght_reg=1.0E-03,target_rep_weight=0.3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The are 2919 observations\n",
      "Sampling from allowed 82 labels\n",
      "82 labels in the validation set, with\n",
      "1587 potential observation to draw from.\n",
      "365 observations sampled for validation\n",
      "1222 observations for training\n",
      "The ratio of validation to *training* is about 0.299\n",
      "X_train_svm (sparse) matrix, of size 1222 by 1886 has been created.\n",
      "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=2178, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=2178, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM ========================\n",
    "# vectorizer transforms dict into sparse matrix\n",
    "v = DictVectorizer(sparse=True)\n",
    "\n",
    "# load data\n",
    "x_train_svm, x_val_svm, \\\n",
    "    y_train_svm, y_val_svm, \\\n",
    "    allowed_ngrams = \\\n",
    "    data_load_preprocess(**{**kwargs_lin_clf, \n",
    "                            **{'linear_counters': False}})\n",
    "\n",
    "# create and fit classifier\n",
    "# create a sparse X matrix with character and n-grams features\n",
    "X_train_svm = v.fit_transform([Counter(elem) for elem in x_train_svm])\n",
    "X_val_svm = v.transform([Counter(elem) for elem in x_val_svm])\n",
    "\n",
    "print('X_train_svm (sparse) matrix, of size {} by {} has been created.'\n",
    "      .format(X_train_svm.get_shape()[0], \n",
    "              X_train_svm.get_shape()[1]))  # vectorized\n",
    "\n",
    "svm_clf = svm.SVC(**kwargs_svm)\n",
    "\n",
    "# print(svm_clf)\n",
    "\n",
    "svm_clf.fit(X_train_svm, y_train_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# array to hold log probabilities (takes a bit longer to calc)\n",
    "pred_prob = svm_clf.predict_log_proba(X_val_svm)\n",
    "# makes y into array with the same shape as the log prob\n",
    "y_val_svm_dense = y_to_dense(y=y_val_svm, \n",
    "                         classes=svm_clf.classes_)\n",
    "\n",
    "print('Accuracy on validation set is {:.3f}'.\n",
    "      format(svm_clf.score(X_val_svm, \n",
    "                           y_val_svm)))\n",
    "\n",
    "print('Mean Reciprocal Rank is {:.3f}'\n",
    "      .format(mean_reciprocal_rank(y_val_svm_dense, \n",
    "                                   pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mrr = mean_reciprocal_rank(\n",
    "    y_val_svm_dense, \n",
    "    pred_prob)\n",
    "\n",
    "acc1 = in_top_k(\n",
    "    y_val_svm_dense, \n",
    "    pred_prob, \n",
    "    1)\n",
    "\n",
    "top5 = in_top_k(\n",
    "    y_val_svm_dense, \n",
    "    pred_prob, \n",
    "    5)\n",
    "\n",
    "print(mrr, top5, acc1, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_first_k_chars(input=x_train_svm, k=7, \n",
    "                   model='linear', \n",
    "                   ngram_width=kwargs_lin_clf.ngram_width, \n",
    "                   mk_ngrams=kwargs_lin_clf.mk_ngrams, \n",
    "                   allowed_ngrams=allowed_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The are 2919 observations\n",
      "Sampling from allowed 82 labels\n",
      "82 labels in the validation set, with\n",
      "1587 potential observation to draw from.\n",
      "365 observations sampled for validation\n",
      "1222 observations for training\n",
      "The ratio of validation to *training* is about 0.299\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "# load data (only validation data, since we're not training RNN now)\n",
    "_, _, x_feed_val_rnn, y_feed_val_rnn,\\\n",
    "    char_int, char_int_inv, label_int, label_int_inv, \\\n",
    "    statistics_dict =\\\n",
    "    data_load_preprocess(**kwargs_neural_data_init)\n",
    "\n",
    "kwargs_simple_rnn = {**kwargs_simple_rnn, \n",
    "                      **statistics_dict}\n",
    "\n",
    "kwargs_simple_rnn = nice_dict({**kwargs_simple_rnn, \n",
    "                               **{'scale_func': kwargs_neural_data_init['scale_func'], \n",
    "                                  'keep_infreq_labels': kwargs_neural_data_init['keep_infreq_labels']}})\n",
    "\n",
    "# validation data\n",
    "X_val_rnn, _, Y_val_rnn = index_transorm_xy(x=x_feed_val_rnn, \n",
    "                                    y=y_feed_val_rnn, \n",
    "                                    char_int=char_int, \n",
    "                                    label_int=label_int, \n",
    "                                    **kwargs_simple_rnn)\n",
    "\n",
    "kwargs_feed_dict_test = {'x': X_val_rnn, \n",
    "                         'y': Y_val_rnn}\n",
    "\n",
    "# create classifier and load weights\n",
    "Rnn_clf = Rnn_model(\n",
    "    feed_dict_test=kwargs_feed_dict_test, \n",
    "    **{**kwargs_simple_rnn, \n",
    "       **{}}  # 'epochs': 100\n",
    ")\n",
    "\n",
    "\n",
    "# Rnn_clf.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparam_str = make_hparam_string(**kwargs_simple_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=0.9,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=128,l2_weight_reg=1.0E-03,target_rep_weight=0.3/\n",
      "GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=0.9,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=128,l2_wieght_reg=1.0E-03,target_rep_weight=0.3/\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(hparam_str)\n",
    "print(rnn_dir_name)\n",
    "print(hparam_str == rnn_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading variables from experiments/logdir_exper_4_20_GRU/GRU,bidir=F,noisy_tanh,learn_p=F,noise_alpha=0.9,noise_half_normal=F,keep_infreq_labels=F,learn_rate=1.0E-02,keep_prob=0.7,one_hot,hidden_state_size=128,l2_wieght_reg=1.0E-03,target_rep_weight=0.3/\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "no checkpoint found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bcfc118f2d4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Rnn_clf.restore(cp_path=os.path.join(kwargs_simple_rnn.log_dir, hparam_str))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRnn_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs_simple_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_dir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Rnn_clf.restore(cp_path=rnn_dir_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yarden/git/Automated_categorization_medication/utils/Rnn_model.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, cp_path, feed_dict)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no checkpoint found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading successful'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: no checkpoint found"
     ]
    }
   ],
   "source": [
    "# Rnn_clf.restore(cp_path=os.path.join(kwargs_simple_rnn.log_dir, hparam_str))\n",
    "Rnn_clf.restore(cp_path=os.path.join(kwargs_simple_rnn.log_dir, rnn_dir_name))\n",
    "# Rnn_clf.restore(cp_path=rnn_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_new.update_test_dict(kwargs_feed_dict_train)\n",
    "[accuracy, cost, recip_rank, top_k] = rnn_new.run_eval()\n",
    "print('accuracy is {:.5f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_first_k_chars(input=X_train, k=2, \n",
    "                       model='neural', \n",
    "                       char_int=char_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate first_k_characters sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect evaulation metrics for sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
